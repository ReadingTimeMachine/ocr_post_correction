{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ya2rjz5flA0f"
   },
   "source": [
    "This runs our model with byt5-ocr specific corrections: https://huggingface.co/yelpfeast/byt5-base-english-ocr-correction \n",
    "\n",
    "Make sure you are in the PyTorchOCR environment on HAL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCqaqXXMmqF_"
   },
   "outputs": [],
   "source": [
    "main_dir = '/home/jnaiman/'\n",
    "output_dir = main_dir + 'models/byt5_inline_cite_ref_ocr/' # math/cite/refs -- just left in as raw\n",
    "aligned_dataset_dir = '/home/jnaiman/wwt_image_extraction/alignments/'\n",
    "\n",
    "# which model do we want to start from pre-trained?\n",
    "#model_pretrained = 'google/byt5-small' # orig\n",
    "model_pretrained = 'yelpfeast/byt5-base-english-ocr-correction' # for OCR correction specifically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxxtYvsbncTB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(aligned_dataset_dir+'train_masked_n500000_20230503.csv')\n",
    "eval_df = pd.read_csv(aligned_dataset_dir+'val_masked_n10000_20230503.csv')\n",
    "test_df = pd.read_csv(aligned_dataset_dir+'test_masked_n10000_20230503.csv')\n",
    "\n",
    "only_words = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wotefPaCoxuA",
    "outputId": "92286aae-79e6-4a20-a4f6-60bcca924a83"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers==4.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bfOyWk8lkUr"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ir14BJRimVE8"
   },
   "source": [
    "Order here is important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6THPU5s7l5Ey"
   },
   "outputs": [],
   "source": [
    "# !pip install pybind11 \n",
    "# !pip install fastwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g154RiKGlb_h"
   },
   "outputs": [],
   "source": [
    "from transformers import HfArgumentParser, TensorFlowBenchmark, TensorFlowBenchmarkArguments\n",
    "#import pandas as pd\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "from transformers import TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Trainer\n",
    "from transformers import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WX29dDbfls7l"
   },
   "outputs": [],
   "source": [
    "#import fastwer\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Giern-Vlfsr"
   },
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append(main_dir + 'libraries/')\n",
    "from utils_ocr_mini import get_fill_in_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJUsyo_UlZko",
    "outputId": "b52e62fd-3c1c-4bdb-a1b7-f6dc04f2833c"
   },
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "cuda.empty_cache()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOeops60nnoU"
   },
   "outputs": [],
   "source": [
    "def add_formatted_columns(datain):\n",
    "    source = []\n",
    "    target = []\n",
    "    source_aligned = []\n",
    "    target_aligned = []\n",
    "    for i in range(len(datain)):\n",
    "        d = datain.iloc[i]\n",
    "        s = np.array(list(d['aligned sentences source'])) # aligned source, with ^ symbols\n",
    "        t = np.array(list(d['aligned sentences target'])) # aligned target, with @ symbols\n",
    "        a = np.array(list(get_fill_in_types(d['aligned sentences target types'])))\n",
    "        if len(s) == len(t):\n",
    "            ss = \"\".join(s[np.where( (a == ' ') | (a == 'W') | (a == 'w'))[0]].tolist())\n",
    "            tt = \"\".join(t[np.where( (a == ' ') | (a == 'W') | (a == 'w'))[0]].tolist())\n",
    "        else:\n",
    "            print('have issue, testing')\n",
    "            if t[0] == ' ' and s[0] != ' ':\n",
    "                t = np.array(list(d['aligned sentences target']))[1:] # aligned target, with @ symbols\n",
    "                a = np.array(list(get_fill_in_types(d['aligned sentences target types'])))[1:]\n",
    "                if len(s) == len(t):\n",
    "                    ss = \"\".join(s[np.where( (a == ' ') | (a == 'W') | (a == 'w'))[0]].tolist())\n",
    "                    tt = \"\".join(t[np.where( (a == ' ') | (a == 'W') | (a == 'w'))[0]].tolist())\n",
    "                else:\n",
    "                    print('not aligned, best guess')\n",
    "                    import sys; sys.exit()\n",
    "\n",
    "        source_aligned.append(ss.replace('^','@')) # align with original \n",
    "        target_aligned.append(tt)\n",
    "        source.append(ss.replace('^',''))\n",
    "        target.append(tt.replace('@',''))\n",
    "\n",
    "    datain['words source aligned'] = source_aligned\n",
    "    datain['words target aligned'] = target_aligned\n",
    "    datain['words source'] = source\n",
    "    datain['words target'] = target\n",
    "    return datain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "QItVt00_lZiZ",
    "outputId": "f807c704-9aaf-452a-a148-04f08de34893"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQA5YZDdlZgM"
   },
   "outputs": [],
   "source": [
    "if only_words:\n",
    "    train_df = add_formatted_columns(train_df)\n",
    "    eval_df = add_formatted_columns(eval_df)\n",
    "    test_df = add_formatted_columns(test_df)\n",
    "    # rename sentences we want\n",
    "    train_df = train_df.rename(columns={\"words source\": \"input_text\", \n",
    "                        \"words target\": \"target_text\"})\n",
    "    eval_df = eval_df.rename(columns={\"words source\": \"input_text\", \n",
    "                        \"words target\": \"target_text\"})\n",
    "    test_df = test_df.rename(columns={\"words source\": \"input_text\", \n",
    "                        \"words target\": \"target_text\"})\n",
    "else:\n",
    "    # rename sentences we want\n",
    "    train_df = train_df.rename(columns={\"sentences source\": \"input_text\", \n",
    "                        \"sentences target\": \"target_text\"})\n",
    "    eval_df = eval_df.rename(columns={\"sentences source\": \"input_text\", \n",
    "                        \"sentences target\": \"target_text\"})\n",
    "    test_df = test_df.rename(columns={\"sentences source\": \"input_text\", \n",
    "                        \"sentences target\": \"target_text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jd1PXsovlZd2"
   },
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    #\"model_name_or_path\": 'google/byt5-small',\n",
    "    #\"max_len\": 4096,\n",
    "    #\"max_length\": 4096,\n",
    "    \"output_dir\": output_dir,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"warmup_steps\": 250,\n",
    "    \"logging_steps\": 100,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": 1000,\n",
    "    \"num_train_epochs\": 4,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"fp16\": False,\n",
    "    #\"use_cache\": False,\n",
    "    \"max_steps\": 100000,\n",
    "    'save_steps':1000,\n",
    "    'save_strategy':'steps',\n",
    "    'load_best_model_at_end': True#,\n",
    "    # 'metric_for_best_model':'eval_loss',\n",
    "    # 'greater_is_better':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxQxNcMjoi8O"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWfk71QslZby"
   },
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(\n",
    "        (TrainingArguments))\n",
    "training_args = parser.parse_dict(args_dict)\n",
    "# set_seed(training_args.seed)\n",
    "args = training_args[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UWnONbvlZZt"
   },
   "outputs": [],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_pretrained,\n",
    "    cache_dir=output_dir, \n",
    "    max_length=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vbky6fbWlZXe"
   },
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_pretrained,\n",
    "    cache_dir=output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVetcdNVlZVI"
   },
   "outputs": [],
   "source": [
    "# overwriting the default max_length of 20 \n",
    "tokenizer.model_max_length=4096\n",
    "model.config.max_length=4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVv94v7BlZS6"
   },
   "outputs": [],
   "source": [
    "class GPReviewDataset(Dataset):\n",
    "    def __init__(self, Text, Label):\n",
    "        self.Text = Text\n",
    "        self.Label = Label\n",
    "        # self.tokenizer = tokenizer\n",
    "        # self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.Text)\n",
    "    def __getitem__(self, item):\n",
    "        Text = str(self.Text[item])\n",
    "        Label = self.Label[item]\n",
    "        inputs = tokenizer(Text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "        outputs = tokenizer(Label, padding=\"max_length\", truncation=True, max_length=512)\n",
    "        return {\n",
    "          \"input_ids\":inputs.input_ids,\n",
    "          \"attention_mask\" : inputs.attention_mask,\n",
    "          \"labels\" : outputs.input_ids,\n",
    "          \"decoder_attention_mask\" : outputs.attention_mask,\n",
    "          # \"labels\" : lbz\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mix1mmuWlZQs"
   },
   "outputs": [],
   "source": [
    "ds_train = GPReviewDataset(\n",
    "  Text=train_df.input_text.to_numpy(),\n",
    "  Label=train_df.target_text.to_numpy()\n",
    "  # tokenizer=tokenizer,\n",
    "  # max_len=max_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxekcQahlZOn"
   },
   "outputs": [],
   "source": [
    "ds_test = GPReviewDataset(\n",
    "  Text=eval_df.input_text.to_numpy(),\n",
    "  Label=eval_df.target_text.to_numpy()\n",
    "  # tokenizer=tokenizer,\n",
    "  # max_len=max_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFQ-LlwNlZMd"
   },
   "outputs": [],
   "source": [
    "train_dataset = ds_train\n",
    "valid_dataset = ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKJTtQS9lZKh"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    # callbacks = [EarlyStoppingCallback(early_stopping_patience=10)]\n",
    "    # compute_metrics=compute_metrics\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tpQpGEMU2hjf",
    "outputId": "8221f736-d6ec-4502-a566-41f775106f69"
   },
   "outputs": [],
   "source": [
    "#!ls gdrive/MyDrive/TPDL\\ 2023\\ Colab\\ Notebooks/byt5_models/byt5_inline_cite_ref_ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "B1SmApob2d1_",
    "outputId": "379f8e10-6149-44f5-ddd3-cb8a6c85f52c"
   },
   "outputs": [],
   "source": [
    "#output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "kggLRL_42vAL",
    "outputId": "8241612e-e878-485c-cc89-51c6692da677"
   },
   "outputs": [],
   "source": [
    "#f''+output_dir + 'checkpoint-15000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "v0mnJg8HlZIc",
    "outputId": "c45bb6b2-6ba5-4fce-89be-46cbf1157bd1"
   },
   "outputs": [],
   "source": [
    "trainer.args.save_total_limit = 10\n",
    "trainer.args.logging_steps = 100 # down from 100\n",
    "trainer.args.save_steps=500 # down from 10000\n",
    "#trainer.train() # put in checkpoint if need be here to load \n",
    "trainer.train(output_dir + 'checkpoint-500/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwBjpXo-lY-r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GeTmNSxK1yWf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:.conda-PyTorchOCR]",
   "language": "python",
   "name": "conda-env-.conda-PyTorchOCR-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
