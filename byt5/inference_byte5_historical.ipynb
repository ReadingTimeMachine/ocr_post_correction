{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b6ea89-6c9f-4dce-bda6-f6407bf577df",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/Users/jnaiman/Downloads/tmp/byt5_inline_cite_ref_large/' \n",
    "snapshot = 'checkpoint-87000' # set to None to take last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22ef81c6-4502-40d2-80c8-414709efc9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, ByT5Tokenizer, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb76ab06-8690-4f40-b571-30946278779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google/byt5-small\",\n",
    "    cache_dir=output_dir, \n",
    "    max_length=4096\n",
    ")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"google/byt5-small\",\n",
    "    cache_dir=output_dir,\n",
    ")\n",
    "\n",
    "# overwriting the default max_length of 20 \n",
    "tokenizer.model_max_length=4096\n",
    "model.config.max_length=4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d84a0a59-a04e-42c0-91d1-b1ed5133e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if snapshot == None:\n",
    "    snapshots = glob(output_dir+'checkpoint*')\n",
    "    order = []\n",
    "    for s in snapshots:\n",
    "        order.append(s.split('-')[-1])\n",
    "    argsort = np.argsort(np.array(order).astype('int'))\n",
    "    snapshot = np.array(snapshots)[argsort][-1]\n",
    "else:\n",
    "    snapshot = output_dir + snapshot\n",
    "\n",
    "ckpoint = snapshot.split('-')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3545b32-f605-41da-b8e3-a2cfa0c81a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jnaiman/Downloads/tmp/byt5_inline_cite_ref_large/checkpoint-87000'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "893c056a-02a3-4be2-bbf9-cd93c6f9dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with ours\n",
    "model = T5ForConditionalGeneration.from_pretrained(snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdfbe04c-c621-4643-8a58-065a13267796",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '140 has n = 0.67 and v, = 6.3 X 1014 sec\", but arises from a volume V = 4 X 10?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "625a373d-5133-4597-a03a-993cf51e380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, padding=\"longest\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef37d469-c373-4392-890e-80f66b9c43c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699e0e7d-5539-4767-a2cf-7e9f899a35f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba7478b-efd9-4784-8fd1-a8b8af66abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_checkpoint = 'google/byt5-small'\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "# tokenizer = ByT5Tokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google/byt5-small\",\n",
    "    cache_dir=output_dir, \n",
    "    max_length=4096\n",
    ")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"google/byt5-small\",\n",
    "    cache_dir=output_dir,\n",
    ")\n",
    "\n",
    "# overwriting the default max_length of 20 \n",
    "tokenizer.model_max_length=4096\n",
    "model.config.max_length=4096\n",
    "\n",
    "\n",
    "texts = [\"Life is like a box of chocolates.\", \"Today is Monday.\"]\n",
    "\n",
    "for text in texts:\n",
    "  inputs = tokenizer(text, padding=\"longest\", return_tensors=\"pt\")\n",
    "  output = model.generate(**inputs)\n",
    "  print(tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
