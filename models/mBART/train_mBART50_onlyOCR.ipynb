{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This runs our model with mBART-50 specific corrections: https://huggingface.co/facebook/mbart-large-50"
      ],
      "metadata": {
        "id": "Ya2rjz5flA0f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnTDdWhrkhSL",
        "outputId": "505ff85d-3356-4970-c58e-ff1ed867101d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_dir = 'gdrive/MyDrive/TPDL 2023 Colab Notebooks/'\n",
        "\n",
        "# where to output models\n",
        "output_dir = main_dir + 'mBART_models/ocrOnly_large/' # math/cite/refs -- just left in as raw\n",
        "\n",
        "# where is data stored?\n",
        "aligned_dataset_dir = main_dir + 'data/alignments/'\n",
        "\n",
        "# which model do we want to start from pre-trained?\n",
        "#model_pretrained = 'google/byt5-small' # orig\n",
        "#model_pretrained = 'yelpfeast/byt5-base-english-ocr-correction' # for OCR correction specifically\n",
        "###model_pretrained = 'facebook/mbart-large-50' # mBART-50"
      ],
      "metadata": {
        "id": "kCqaqXXMmqF_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv(aligned_dataset_dir+'train_masked_n500000_20230503.csv')\n",
        "eval_df = pd.read_csv(aligned_dataset_dir+'val_masked_n10000_20230503.csv')\n",
        "test_df = pd.read_csv(aligned_dataset_dir+'test_masked_n10000_20230503.csv')\n",
        "\n",
        "only_words = True"
      ],
      "metadata": {
        "id": "oxxtYvsbncTB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece]==4.28.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        },
        "id": "wotefPaCoxuA",
        "outputId": "6f08186d-21bc-4c31-d35c-5a6ff1e93129"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers[sentencepiece]==4.28.0\n",
            "  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]==4.28.0) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers[sentencepiece]==4.28.0)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]==4.28.0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]==4.28.0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]==4.28.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]==4.28.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]==4.28.0) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers[sentencepiece]==4.28.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]==4.28.0) (4.65.0)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece]==4.28.0)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf<=3.20.2 (from transformers[sentencepiece]==4.28.0)\n",
            "  Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers[sentencepiece]==4.28.0) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers[sentencepiece]==4.28.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]==4.28.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]==4.28.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]==4.28.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]==4.28.0) (3.4)\n",
            "Installing collected packages: tokenizers, sentencepiece, protobuf, huggingface-hub, transformers\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.14.1 protobuf-3.20.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.28.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers"
      ],
      "metadata": {
        "id": "2bfOyWk8lkUr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Order here is important!"
      ],
      "metadata": {
        "id": "ir14BJRimVE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pybind11 \n",
        "# !pip install fastwer"
      ],
      "metadata": {
        "id": "6THPU5s7l5Ey"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import HfArgumentParser, TensorFlowBenchmark, TensorFlowBenchmarkArguments\n",
        "#import pandas as pd\n",
        "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
        "from transformers import TrainingArguments\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import Trainer\n",
        "from transformers import EarlyStoppingCallback"
      ],
      "metadata": {
        "id": "g154RiKGlb_h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##import fastwer\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "WX29dDbfls7l"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sys import path\n",
        "path.append(main_dir + 'libraries/')\n",
        "from utils_ocr_mini import get_fill_in_types"
      ],
      "metadata": {
        "id": "3Giern-Vlfsr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "cuda.empty_cache()\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJUsyo_UlZko",
        "outputId": "0d5d670a-aac0-4c96-e556-f64bfadd58e6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_formatted_columns(datain):\n",
        "    source = []\n",
        "    target = []\n",
        "    source_aligned = []\n",
        "    target_aligned = []\n",
        "    for i in range(len(datain)):\n",
        "        d = datain.iloc[i]\n",
        "        s = np.array(list(d['aligned sentences source'])) # aligned source, with ^ symbols\n",
        "        t = np.array(list(d['aligned sentences target'])) # aligned target, with @ symbols\n",
        "        a = np.array(list(get_fill_in_types(d['aligned sentences target types'])))\n",
        "        if len(s) == len(t):\n",
        "            ss = \"\".join(s[np.where( (a == ' ') | (a == 'W') | (a == 'w'))[0]].tolist())\n",
        "            tt = \"\".join(t[np.where( (a == ' ') | (a == 'W') | (a == 'w'))[0]].tolist())\n",
        "        else:\n",
        "            print('have issue, testing')\n",
        "            if t[0] == ' ' and s[0] != ' ':\n",
        "                t = np.array(list(d['aligned sentences target']))[1:] # aligned target, with @ symbols\n",
        "                a = np.array(list(get_fill_in_types(d['aligned sentences target types'])))[1:]\n",
        "                if len(s) == len(t):\n",
        "                    ss = \"\".join(s[np.where( (a == ' ') | (a == 'W') | (a == 'w'))[0]].tolist())\n",
        "                    tt = \"\".join(t[np.where( (a == ' ') | (a == 'W') | (a == 'w'))[0]].tolist())\n",
        "                else:\n",
        "                    print('not aligned, best guess')\n",
        "                    import sys; sys.exit()\n",
        "\n",
        "        source_aligned.append(ss.replace('^','@')) # align with original \n",
        "        target_aligned.append(tt)\n",
        "        source.append(ss.replace('^',''))\n",
        "        target.append(tt.replace('@',''))\n",
        "\n",
        "    datain['words source aligned'] = source_aligned\n",
        "    datain['words target aligned'] = target_aligned\n",
        "    datain['words source'] = source\n",
        "    datain['words target'] = target\n",
        "    return datain"
      ],
      "metadata": {
        "id": "nOeops60nnoU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "QItVt00_lZiZ",
        "outputId": "de9645bb-12e7-4a3b-fdf5-aa2462fc3c6a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            aligned sentences source  \\\n",
              "0   To a good approxiuiati^^^ the radial velocity...   \n",
              "1  Where all the units must be in ces and the ^L^...   \n",
              "2  ^At sulliciently high temperatures the ^^^^^^^...   \n",
              "3   The resulting spectral resolution was ^^^^^^8...   \n",
              "4   One approach is ^o search for a CAV signal fo...   \n",
              "\n",
              "                            aligned sentences target  \\\n",
              "0   To a good approxim@ation, the radial velocity...   \n",
              "1  Where all the units must be in cgs and the $L_...   \n",
              "2   At sufficiently high temperatures the $^4\\mat...   \n",
              "3   The resulting spectral resolution was $\\sim 8...   \n",
              "4   One approach is to search for a GW@ signal fo...   \n",
              "\n",
              "                                    sentences source  \\\n",
              "0   To a good approxiuiati the radial velocity of...   \n",
              "1  Where all the units must be in ces and the Leu...   \n",
              "2  At sulliciently high temperatures the !1 and =...   \n",
              "3   The resulting spectral resolution was 8 kIlz ...   \n",
              "4   One approach is o search for a CAV signal fol...   \n",
              "\n",
              "                                    sentences target  \\\n",
              "0   To a good approximation, the radial velocity ...   \n",
              "1  Where all the units must be in cgs and the $L_...   \n",
              "2   At sufficiently high temperatures the $^4\\mat...   \n",
              "3   The resulting spectral resolution was $\\sim 8...   \n",
              "4   One approach is to search for a GW signal fol...   \n",
              "\n",
              "                      aligned sentences source types  \\\n",
              "0   WW W WWWW WWWWWWWWWWWW^^^ WWW WWWWWW WWWWWWWW...   \n",
              "1  WWWWW WWW WWW WWWWW WWWW WW WW WWW WWW WWW ^I^...   \n",
              "2  ^WW WWWWWWWWWWWW WWWW WWWWWWWWWWWW WWW ^^^^^^^...   \n",
              "3   WWW WWWWWWWWW WWWWWWWW WWWWWWWWWW WWW ^^^^^II...   \n",
              "4   WWW WWWWWWWW WW ^W WWWWWW WWW W WWW WWWWWW WW...   \n",
              "\n",
              "                      aligned sentences target types  \\\n",
              "0   WW W WWWW WWWWWWWW@WWWWWW WWW WWWWWW WWWWWWWW...   \n",
              "1  WWWWW WWW WWW WWWWW WWWW WW WW WWW WWW WWW III...   \n",
              "2   WW WWWWWWWWWWWW WWWW WWWWWWWWWWWW WWW IIIIIII...   \n",
              "3   WWW WWWWWWWWW WWWWWWWW WWWWWWWWWW WWW IIIIIII...   \n",
              "4   WWW WWWWWWWW WW WW WWWWWW WWW W WW@ WWWWWW WW...   \n",
              "\n",
              "                              sentences source types  \\\n",
              "0   WW W WWWW WWWWWWWWWWWW WWW WWWWWW WWWWWWWW WW...   \n",
              "1  WWWWW WWW WWW WWWWW WWWW WW WW WWW WWW WWW III...   \n",
              "2  WW WWWWWWWWWWWW WWWW WWWWWWWWWWWW WWW II WWW I...   \n",
              "3   WWW WWWWWWWWW WWWWWWWW WWWWWWWWWW WWW II WWWW...   \n",
              "4   WWW WWWWWWWW WW W WWWWWW WWW W WWW WWWWWW WWW...   \n",
              "\n",
              "                              sentences target types  \n",
              "0   WW W WWWW WWWWWWWWWWWWWW WWW WWWWWW WWWWWWWW ...  \n",
              "1  WWWWW WWW WWW WWWWW WWWW WW WW WWW WWW WWW III...  \n",
              "2   WW WWWWWWWWWWWW WWWW WWWWWWWWWWWW WWW IIIIIII...  \n",
              "3   WWW WWWWWWWWW WWWWWWWW WWWWWWWWWW WWW IIIIIII...  \n",
              "4   WWW WWWWWWWW WW WW WWWWWW WWW W WW WWWWWW WWW...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-225767d2-3101-45c7-ad4f-1cc96a17d226\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aligned sentences source</th>\n",
              "      <th>aligned sentences target</th>\n",
              "      <th>sentences source</th>\n",
              "      <th>sentences target</th>\n",
              "      <th>aligned sentences source types</th>\n",
              "      <th>aligned sentences target types</th>\n",
              "      <th>sentences source types</th>\n",
              "      <th>sentences target types</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>To a good approxiuiati^^^ the radial velocity...</td>\n",
              "      <td>To a good approxim@ation, the radial velocity...</td>\n",
              "      <td>To a good approxiuiati the radial velocity of...</td>\n",
              "      <td>To a good approximation, the radial velocity ...</td>\n",
              "      <td>WW W WWWW WWWWWWWWWWWW^^^ WWW WWWWWW WWWWWWWW...</td>\n",
              "      <td>WW W WWWW WWWWWWWW@WWWWWW WWW WWWWWW WWWWWWWW...</td>\n",
              "      <td>WW W WWWW WWWWWWWWWWWW WWW WWWWWW WWWWWWWW WW...</td>\n",
              "      <td>WW W WWWW WWWWWWWWWWWWWW WWW WWWWWW WWWWWWWW ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Where all the units must be in ces and the ^L^...</td>\n",
              "      <td>Where all the units must be in cgs and the $L_...</td>\n",
              "      <td>Where all the units must be in ces and the Leu...</td>\n",
              "      <td>Where all the units must be in cgs and the $L_...</td>\n",
              "      <td>WWWWW WWW WWW WWWWW WWWW WW WW WWW WWW WWW ^I^...</td>\n",
              "      <td>WWWWW WWW WWW WWWWW WWWW WW WW WWW WWW WWW III...</td>\n",
              "      <td>WWWWW WWW WWW WWWWW WWWW WW WW WWW WWW WWW III...</td>\n",
              "      <td>WWWWW WWW WWW WWWWW WWWW WW WW WWW WWW WWW III...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>^At sulliciently high temperatures the ^^^^^^^...</td>\n",
              "      <td>At sufficiently high temperatures the $^4\\mat...</td>\n",
              "      <td>At sulliciently high temperatures the !1 and =...</td>\n",
              "      <td>At sufficiently high temperatures the $^4\\mat...</td>\n",
              "      <td>^WW WWWWWWWWWWWW WWWW WWWWWWWWWWWW WWW ^^^^^^^...</td>\n",
              "      <td>WW WWWWWWWWWWWW WWWW WWWWWWWWWWWW WWW IIIIIII...</td>\n",
              "      <td>WW WWWWWWWWWWWW WWWW WWWWWWWWWWWW WWW II WWW I...</td>\n",
              "      <td>WW WWWWWWWWWWWW WWWW WWWWWWWWWWWW WWW IIIIIII...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The resulting spectral resolution was ^^^^^^8...</td>\n",
              "      <td>The resulting spectral resolution was $\\sim 8...</td>\n",
              "      <td>The resulting spectral resolution was 8 kIlz ...</td>\n",
              "      <td>The resulting spectral resolution was $\\sim 8...</td>\n",
              "      <td>WWW WWWWWWWWW WWWWWWWW WWWWWWWWWW WWW ^^^^^II...</td>\n",
              "      <td>WWW WWWWWWWWW WWWWWWWW WWWWWWWWWW WWW IIIIIII...</td>\n",
              "      <td>WWW WWWWWWWWW WWWWWWWW WWWWWWWWWW WWW II WWWW...</td>\n",
              "      <td>WWW WWWWWWWWW WWWWWWWW WWWWWWWWWW WWW IIIIIII...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>One approach is ^o search for a CAV signal fo...</td>\n",
              "      <td>One approach is to search for a GW@ signal fo...</td>\n",
              "      <td>One approach is o search for a CAV signal fol...</td>\n",
              "      <td>One approach is to search for a GW signal fol...</td>\n",
              "      <td>WWW WWWWWWWW WW ^W WWWWWW WWW W WWW WWWWWW WW...</td>\n",
              "      <td>WWW WWWWWWWW WW WW WWWWWW WWW W WW@ WWWWWW WW...</td>\n",
              "      <td>WWW WWWWWWWW WW W WWWWWW WWW W WWW WWWWWW WWW...</td>\n",
              "      <td>WWW WWWWWWWW WW WW WWWWWW WWW W WW WWWWWW WWW...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-225767d2-3101-45c7-ad4f-1cc96a17d226')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-225767d2-3101-45c7-ad4f-1cc96a17d226 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-225767d2-3101-45c7-ad4f-1cc96a17d226');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if only_words:\n",
        "    train_df = add_formatted_columns(train_df)\n",
        "    eval_df = add_formatted_columns(eval_df)\n",
        "    test_df = add_formatted_columns(test_df)\n",
        "    # rename sentences we want\n",
        "    train_df = train_df.rename(columns={\"words source\": \"input_text\", \n",
        "                        \"words target\": \"target_text\"})\n",
        "    eval_df = eval_df.rename(columns={\"words source\": \"input_text\", \n",
        "                        \"words target\": \"target_text\"})\n",
        "    test_df = test_df.rename(columns={\"words source\": \"input_text\", \n",
        "                        \"words target\": \"target_text\"})\n",
        "else:\n",
        "    # rename sentences we want\n",
        "    train_df = train_df.rename(columns={\"sentences source\": \"input_text\", \n",
        "                        \"sentences target\": \"target_text\"})\n",
        "    eval_df = eval_df.rename(columns={\"sentences source\": \"input_text\", \n",
        "                        \"sentences target\": \"target_text\"})\n",
        "    test_df = test_df.rename(columns={\"sentences source\": \"input_text\", \n",
        "                        \"sentences target\": \"target_text\"})"
      ],
      "metadata": {
        "id": "oQA5YZDdlZgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0ebefd3-f6f6-41c2-d0c1-27c8022205da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "have issue, testing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args_dict = {\n",
        "    #\"model_name_or_path\": 'google/byt5-small',\n",
        "    #\"max_len\": 4096,\n",
        "    #\"max_length\": 4096,\n",
        "    \"output_dir\": output_dir,\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"per_device_train_batch_size\": 4,\n",
        "    \"per_device_eval_batch_size\": 4,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"warmup_steps\": 250,\n",
        "    \"logging_steps\": 100,\n",
        "    \"evaluation_strategy\": \"steps\",\n",
        "    \"eval_steps\": 1000,\n",
        "    \"num_train_epochs\": 4,\n",
        "    \"do_train\": True,\n",
        "    \"do_eval\": True,\n",
        "    \"fp16\": False,\n",
        "    #\"use_cache\": False,\n",
        "    \"max_steps\": 100000,\n",
        "    'save_steps':1000,\n",
        "    'save_strategy':'steps',\n",
        "    'load_best_model_at_end': True#,\n",
        "    # 'metric_for_best_model':'eval_loss',\n",
        "    # 'greater_is_better':False\n",
        "}"
      ],
      "metadata": {
        "id": "Jd1PXsovlZd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade accelerate"
      ],
      "metadata": {
        "id": "FxQxNcMjoi8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = HfArgumentParser(\n",
        "        (TrainingArguments))\n",
        "training_args = parser.parse_dict(args_dict)\n",
        "# set_seed(training_args.seed)\n",
        "args = training_args[0]"
      ],
      "metadata": {
        "id": "MWfk71QslZby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast"
      ],
      "metadata": {
        "id": "GvNihf_CtUC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install sentencepiece"
      ],
      "metadata": {
        "id": "RCGKYeZctdea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "U2hDbx3BtmfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained model and tokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\n",
        "#     model_pretrained,\n",
        "#     cache_dir=output_dir, \n",
        "#     max_length=4096\n",
        "# )\n",
        "# mbart specific\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"en_XX\")"
      ],
      "metadata": {
        "id": "-UWnONbvlZZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = T5ForConditionalGeneration.from_pretrained(\n",
        "#     model_pretrained,\n",
        "#     cache_dir=output_dir,\n",
        "# )\n",
        "# specific for mBART\n",
        "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")"
      ],
      "metadata": {
        "id": "Vbky6fbWlZXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overwriting the default max_length of 20 \n",
        "tokenizer.model_max_length=4096\n",
        "model.config.max_length=4096"
      ],
      "metadata": {
        "id": "lVetcdNVlZVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPReviewDataset(Dataset):\n",
        "    def __init__(self, Text, Label):\n",
        "        self.Text = Text\n",
        "        self.Label = Label\n",
        "        # self.tokenizer = tokenizer\n",
        "        # self.max_len = max_len\n",
        "    def __len__(self):\n",
        "        return len(self.Text)\n",
        "    def __getitem__(self, item):\n",
        "        Text = str(self.Text[item])\n",
        "        Label = self.Label[item]\n",
        "        inputs = tokenizer(Text, padding=\"max_length\", truncation=True, max_length=512)\n",
        "        outputs = tokenizer(Label, padding=\"max_length\", truncation=True, max_length=512)\n",
        "        return {\n",
        "          \"input_ids\":inputs.input_ids,\n",
        "          \"attention_mask\" : inputs.attention_mask,\n",
        "          \"labels\" : outputs.input_ids,\n",
        "          \"decoder_attention_mask\" : outputs.attention_mask,\n",
        "          # \"labels\" : lbz\n",
        "        }"
      ],
      "metadata": {
        "id": "lVv94v7BlZS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = GPReviewDataset(\n",
        "  Text=train_df.input_text.to_numpy(),\n",
        "  Label=train_df.target_text.to_numpy()\n",
        "  # tokenizer=tokenizer,\n",
        "  # max_len=max_len\n",
        ")"
      ],
      "metadata": {
        "id": "mix1mmuWlZQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_test = GPReviewDataset(\n",
        "  Text=eval_df.input_text.to_numpy(),\n",
        "  Label=eval_df.target_text.to_numpy()\n",
        "  # tokenizer=tokenizer,\n",
        "  # max_len=max_len\n",
        ")"
      ],
      "metadata": {
        "id": "XxekcQahlZOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ds_train\n",
        "valid_dataset = ds_test"
      ],
      "metadata": {
        "id": "XFQ-LlwNlZMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    # callbacks = [EarlyStoppingCallback(early_stopping_patience=10)]\n",
        "    # compute_metrics=compute_metrics\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "vKJTtQS9lZKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.args.save_total_limit = 10\n",
        "trainer.args.logging_steps = 100 # down from 100\n",
        "trainer.args.save_steps=500 # down from 10000\n",
        "#trainer.train() # put in checkpoint if need be here to load \n",
        "trainer.train(output_dir + 'checkpoint-5500') # put in checkpoint if need be here to load "
      ],
      "metadata": {
        "id": "v0mnJg8HlZIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jwBjpXo-lY-r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}