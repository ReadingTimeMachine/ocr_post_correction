{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Local -------\n",
    "#output_folder = '/Users/jnaiman/Downloads/tmp/ocrpost/data/morgan/' # local\n",
    "#model_save_dir = '/Users/jnaiman/Downloads/tmp/ocrpost/data/morgan/models/' # local\n",
    "#file_dir = '/Users/jnaiman/Dropbox/wwt_image_extraction/OCRPostCorrection/alignments/'\n",
    "#lib_dir = '../scienceDigitization/text_mining_ocr_and_pdf/ocr_spell_check/'\n",
    "#post_correction_repo_dir = '../'\n",
    "\n",
    "\n",
    "# -------- HAL -----------\n",
    "output_folder = '/home/jnaiman/data/morgan/' # HAL\n",
    "model_save_dir = '/home/jnaiman/data/morgan/models/' # HAL\n",
    "eval_save_dir = '/home/jnaiman/data/morgan/eval/' # HAL\n",
    "file_dir = '/home/jnaiman/wwt_image_extraction/alignments/'\n",
    "lib_dir = '/home/jnaiman/libraries/'\n",
    "post_correction_repo_dir = '/home/jnaiman/repos/'\n",
    "\n",
    "\n",
    "ender = '_small_words' # small has 100,000 for training, 5000 for dev\n",
    "# model save dir\n",
    "model_file = 'new_torch_file_new_pages_small_words.pt'\n",
    "test_file = 'test_masked_n5000_20230510.csv'\n",
    "\n",
    "\n",
    "# its not 100% clear if we need this... setting a flag, but looks like we DO need it for memory issues\n",
    "use_train_dev_size = True\n",
    "train_size = 1000000\n",
    "dev_size = 1000000\n",
    "window_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##**window size is not consistent!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char2i = pickle.load(open(output_folder + \"data/char2i_new_pages.pkl\", \"rb\"))\n",
    "# i2char = pickle.load(open(output_folder + \"data/i2char_new_pages.pkl\", \"rb\"))\n",
    "char2i = pickle.load(open(output_folder + \"data/char2i_new_pages\"+ender+\".pkl\", \"rb\"))\n",
    "i2char = pickle.load(open(output_folder + \"data/i2char_new_pages\"+ender+\".pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #source = [list(\"abcdefghijkl\"), list(\"mnopqrstwxyz\")]\n",
    "# #target = [list(\"ABCDEFGHIJKL\"), list(\"MNOPQRSTWXYZ\")] #update to astronomy vocab\n",
    "# source = []#yy\n",
    "# for s in model.source_index:\n",
    "#     sentence = \"\"\n",
    "#     for c in s:\n",
    "#         #nu = source_index[c]\n",
    "#         #sentence.append(c)\n",
    "#         #sentence.append(num)\n",
    "#         sentence += c\n",
    "#     source.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from nltk.lm import Vocabulary\n",
    "import pickle\n",
    "import sys\n",
    "import torch\n",
    "import importlib\n",
    "from timeit import default_timer as t\n",
    "sys.path.append(post_correction_repo_dir)\n",
    "#from metrics import levenshtein\n",
    "import post_ocr_correction # https://github.com/jarobyte91/post_ocr_correction/tree/master\n",
    "from pytorch_beam_search import seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#!pip install Levenshtein\n",
    "from Levenshtein import distance\n",
    "import Levenshtein \n",
    "\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append(lib_dir)\n",
    "from importlib import reload\n",
    "if 'home' in lib_dir: # on HAL\n",
    "    import utils_ocr_mini\n",
    "    reload(utils_ocr_mini)\n",
    "    from utils_ocr_mini import align_texts_fast, get_fill_in_types\n",
    "else: # on home\n",
    "    import utils\n",
    "    reload(utils)\n",
    "    from utils import align_texts_fast, get_fill_in_types    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_beam_search.seq2seq import Transformer, beam_search\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cuda', True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what device are we on?\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#model.to(device)\n",
    "#predictions.to(device)\n",
    "use_cuda = True\n",
    "if device == 'cpu': use_cuda = False\n",
    "device, use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module): \n",
    "    \"\"\"\n",
    "    A generic sequence-to-sequence model. All other sequence-to-sequence models should extend this class \n",
    "    with a __init__ and forward methods, in the same way as in normal PyTorch.\n",
    "    \"\"\"\n",
    "    def print_architecture(self):\n",
    "        \"\"\"\n",
    "        Displays the information about the model in standard output. \n",
    "        \"\"\"\n",
    "        for k in self.architecture.keys():\n",
    "            print(f\"{k.replace('_', ' ').capitalize()}: {self.architecture[k]}\")\n",
    "        print(f\"Trainable parameters: {sum([p.numel() for p in self.parameters()]):,}\")\n",
    "        print()\n",
    "\n",
    "    def fit(self,#train_loader, \n",
    "            X_train, \n",
    "            Y_train, \n",
    "            X_dev = None, \n",
    "            Y_dev = None, \n",
    "            batch_size = 100, \n",
    "            epochs = 5, \n",
    "            learning_rate = 10**-4, \n",
    "            weight_decay = 0, \n",
    "            progress_bar = 0, \n",
    "            save_path = None):\n",
    "        print(\"fit begins\")\n",
    "        best_dev_loss=float('inf')\n",
    "        best_epoch=float('inf')\n",
    "        \"\"\"\n",
    "        A generic training method with Adam and Cross Entropy.\n",
    "\n",
    "        Parameters\n",
    "        ----------    \n",
    "        X_train: LongTensor of shape (train_examples, train_input_length)\n",
    "            The input sequences of the training set.\n",
    "            \n",
    "        Y_train: LongTensor of shape (train_examples, train_output_length)\n",
    "            The output sequences of the training set.\n",
    "            \n",
    "        X_dev: LongTensor of shape (dev_examples, dev_input_length), optional\n",
    "            The input sequences for the development set.\n",
    "            \n",
    "        Y_train: LongTensor of shape (dev_examples, dev_output_length), optional\n",
    "            The output sequences for the development set.\n",
    "            \n",
    "        batch_size: int\n",
    "            The number of examples to process in each batch.\n",
    "\n",
    "        epochs: int\n",
    "            The number of epochs of the training process.\n",
    "            \n",
    "        learning_rate: float\n",
    "            The learning rate to use with Adam in the training process. \n",
    "            \n",
    "        weight_decay: float\n",
    "            The weight_decay parameter of Adam (L2 penalty), useful for regularizing models. For a deeper \n",
    "            documentation, go to https://pytorch.org/docs/stable/_modules/torch/optim/adam.html#Adam            \n",
    "\n",
    "        progress_bar: int\n",
    "            Shows a tqdm progress bar, useful for tracking progress with large tensors.\n",
    "            If equal to 0, no progress bar is shown. \n",
    "            If equal to 1, shows a bar with one step for every epoch.\n",
    "            If equal to 2, shows the bar when equal to 1 and also shows a bar with one step per batch for every epoch.\n",
    "            If equal to 3, shows the bars when equal to 2 and also shows a bar to track the progress of the evaluation\n",
    "            in the development set.\n",
    "            \n",
    "        save_path: string, optional\n",
    "            Path to save the .pt file containing the model parameters when the training ends.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        performance: Pandas DataFrame\n",
    "            DataFrame with the following columns: epoch, train_loss, train_error_rate, (optionally dev_loss and \n",
    "            dev_error_rate), minutes, learning_rate, weight_decay, model, encoder_embedding_dimension, \n",
    "            decoder_embedding_dimension, encoder_hidden_units, encoder_layers, decoder_hidden_units, decoder_layers, \n",
    "            dropout, parameters and one row for each of the epochs, containing information about the training process.\n",
    "        \"\"\"\n",
    "        assert X_train.shape[0] == Y_train.shape[0]\n",
    "        assert (X_dev is None and Y_dev is None) or (X_dev is not None and Y_dev is not None) \n",
    "        if (X_dev is not None and Y_dev is not None):\n",
    "            assert X_dev.shape[0] == Y_dev.shape[0]\n",
    "            dev = True\n",
    "        else:\n",
    "            dev = False\n",
    "            \n",
    "\n",
    "        train_dataset = tud.TensorDataset(X_train, Y_train)\n",
    "        train_loader = tud.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)# make own class data loader to read in batches at a time\n",
    "#         class YourDataset(torch.utils.data.Dataset):\n",
    "#             def __init__(self) -> None:\n",
    "#                 self.source = torch.load(scratch + \"data/train_source_new.pt\")[:train_size].to(device)\n",
    "#                 self.target = torch.load(scratch + \"data/train_target_new.pt\")[:train_size].to(device)\n",
    "                \n",
    "#                 source_files = glob.glob(scratch + \"data/train_source_new0*.pt\")\n",
    "# #                 target_files = glob.glob(scratch + \"data/train_target_new0*.pt\")\n",
    "# #                 source_files.sort()\n",
    "# #                 target_files.sort()\n",
    "# #                 self.source_files = source_files\n",
    "# #                 self.target_files = target_files\n",
    "# #                 self.file_number = 0\n",
    "# #                 self.source = torch.load(source_files[self.file_number])[:train_size].to(device)\n",
    "# #                 self.target = torch.load(target_files[self.file_number])[:train_size].to(device)\n",
    "#                 self.file_length = len(self.source)\n",
    "                \n",
    "#             def __getitem__(self, idx) -> torch.Tensor:\n",
    "#                 if idx < self.file_length:\n",
    "#                     # load one sample by index, e.g like this:\n",
    "#                     source_sample = self.source[idx]\n",
    "#                     target_sample = self.target[idx]\n",
    "#                 else:\n",
    "#                     self.file_number += 1\n",
    "#                     self.source = torch.load(source_files[self.file_number])[:train_size].to(device)\n",
    "#                     self.target = torch.load(target_files[self.file_number])[:train_size].to(device)\n",
    "#                     self.file_length = len(self.source)\n",
    "#                     source_sample = self.source[idx]\n",
    "#                     target_sample = self.target[idx]\n",
    "            \n",
    "                    \n",
    "#                     #pd.len() \n",
    "\n",
    "#                 # do some preprocessing, convert to tensor and what not\n",
    "\n",
    "#                 return source_sample, target_sample\n",
    "\n",
    "#             def __len__(self):\n",
    "#                 return len(self.source)\n",
    "#         creating dataloader\n",
    "#         yourDataset = YourDataset()\n",
    "#         train_loader = torch.utils.data.DataLoader(yourDataset, batch_size= batch_size, num_workers=0, shuffle=True)\n",
    "    \n",
    "        criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "        performance = []\n",
    "        start = timer()\n",
    "        epochs_iterator = range(1, epochs + 1)\n",
    "        if progress_bar > 0:\n",
    "            epochs_iterator = tqdm(epochs_iterator)\n",
    "            print(\"Training started\")\n",
    "        print(\"X_train.shape:\", X_train.shape)\n",
    "        print(\"Y_train.shape:\", Y_train.shape)\n",
    "        if dev:\n",
    "            print(\"X_dev.shape:\", X_dev.shape)\n",
    "            print(\"Y_dev.shape:\", Y_dev.shape)\n",
    "        print(f\"Epochs: {epochs:,}\\nLearning rate: {learning_rate}\\nWeight decay: {weight_decay}\")\n",
    "        header_1 = \"Epoch | Train                \"\n",
    "        header_2 = \"      | Loss     | Error Rate\"\n",
    "        rule = \"-\" * 29\n",
    "        if dev:\n",
    "            header_1 += \" | Development          \"\n",
    "            header_2 += \" | Loss     | Error Rate\"\n",
    "            rule += \"-\" * 24\n",
    "        header_1 += \" | Minutes\"\n",
    "        header_2 += \" |\"\n",
    "        rule += \"-\" * 10\n",
    "        print(header_1, header_2, rule, sep = \"\\n\")\n",
    "        for e in epochs_iterator:\n",
    "            self.train()\n",
    "            losses = []\n",
    "            errors = []\n",
    "            sizes = []\n",
    "            train_iterator = train_loader\n",
    "            if progress_bar > 1:\n",
    "                train_iterator = tqdm(train_iterator)\n",
    "            for x, y in train_iterator:\n",
    "                # compute loss and backpropagate\n",
    "                probabilities = self.forward(x, y).transpose(1, 2)[:, :, :-1]\n",
    "                y = y[:, 1:]\n",
    "                loss = criterion(probabilities, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                # compute accuracy\n",
    "                predictions = probabilities.argmax(1)\n",
    "                batch_errors = (predictions != y)\n",
    "                # append the results\n",
    "                losses.append(loss.item())\n",
    "                errors.append(batch_errors.sum().item())\n",
    "                sizes.append(batch_errors.numel())\n",
    "            train_loss = sum(losses) / len(losses)\n",
    "            train_error_rate = 100 * sum(errors) / sum(sizes)\n",
    "            t = (timer() - start) / 60\n",
    "            status_string = f\"{e:>5} | {train_loss:>8.4f} | {train_error_rate:>10.3f}\"\n",
    "            status = {\"epoch\":e,\n",
    "                      \"train_loss\": train_loss,\n",
    "                      \"train_error_rate\": train_error_rate}\n",
    "            if dev:\n",
    "                dev_loss, dev_error_rate = self.evaluate(X_dev, \n",
    "                                                         Y_dev, \n",
    "                                                         batch_size = batch_size, \n",
    "                                                         progress_bar = progress_bar > 2, \n",
    "                                                         criterion = criterion)\n",
    "                status_string += f\" | {dev_loss:>8.4f} | {dev_error_rate:>10.3f}\"\n",
    "                status.update({\"dev_loss\": dev_loss, \"dev_error_rate\": dev_error_rate})\n",
    "            status.update({\"training_minutes\": t,\n",
    "                           \"learning_rate\": learning_rate,\n",
    "                           \"weight_decay\": weight_decay})\n",
    "            performance.append(status)\n",
    "            if save_path is not None: \n",
    "                print(\"dev =\", dev)\n",
    "                print(\"e =\", e)\n",
    "                print(\"dev loss =\", dev_loss)\n",
    "                print(\"best dev loss =\", best_dev_loss)\n",
    "                #if (not dev) or (e < 2) or (dev_loss < min([p[\"dev_loss\"] for p in performance[:-1]])):\n",
    "                if (not dev) or (e < 2) or (dev_loss < best_dev_loss):\n",
    "                    torch.save(self.state_dict(), save_path)\n",
    "                    print(status)\n",
    "                    best_dev_loss = dev_loss\n",
    "                    print(\"save path =\", save_path)\n",
    "            status_string += f\" | {t:>7.1f}\"\n",
    "            print(status_string)\n",
    "        print()\n",
    "        return pd.concat((pd.DataFrame(performance), \n",
    "                          pd.DataFrame([self.architecture for i in performance])), axis = 1)\\\n",
    "               .drop(columns = [\"source_index\", \"target_index\"])\n",
    "    \n",
    "            \n",
    "    def evaluate(self, \n",
    "                 X, \n",
    "                 Y, \n",
    "                 criterion = nn.CrossEntropyLoss(), \n",
    "                 batch_size = 128, \n",
    "                 progress_bar = False):\n",
    "        \"\"\"\n",
    "        Evaluates the model on a dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: LongTensor of shape (examples, input_length)\n",
    "            The input sequences of the dataset.\n",
    "            \n",
    "        Y: LongTensor of shape (examples, output_length)\n",
    "            The output sequences of the dataset.\n",
    "            \n",
    "        criterion: PyTorch module\n",
    "            The loss function to evalue the model on the dataset, has to be able to compare self.forward(X, Y) and Y\n",
    "            to produce a real number.\n",
    "            \n",
    "        batch_size: int\n",
    "            The batch size of the evaluation loop.\n",
    "            \n",
    "        progress_bar: bool\n",
    "            Shows a tqdm progress bar, useful for tracking progress with large tensors.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        loss: float\n",
    "            The average of criterion across the whole dataset.\n",
    "            \n",
    "        error_rate: float\n",
    "            The step-by-step accuracy of the model across the whole dataset. Useful as a sanity check, as it should\n",
    "            go to zero as the loss goes to zero.\n",
    "            \n",
    "        \"\"\"\n",
    "        dataset = tud.TensorDataset(X, Y)\n",
    "        loader = tud.DataLoader(dataset, batch_size = batch_size)\n",
    "        self.eval()\n",
    "        losses = []\n",
    "        errors = []\n",
    "        sizes = []\n",
    "        with torch.no_grad():\n",
    "            iterator = iter(loader)\n",
    "            if progress_bar:\n",
    "                iterator = tqdm(iterator)\n",
    "            for batch in iterator:\n",
    "                x, y = batch\n",
    "                # compute loss\n",
    "                probabilities = self.forward(x, y).transpose(1, 2)[:, :, :-1]\n",
    "                y = y[:, 1:]\n",
    "                loss = criterion(probabilities, y)\n",
    "                # compute accuracy\n",
    "                predictions = probabilities.argmax(1)\n",
    "                batch_errors = (predictions != y)\n",
    "                # append the results\n",
    "                losses.append(loss.item())\n",
    "                errors.append(batch_errors.sum().item())\n",
    "                sizes.append(batch_errors.numel())\n",
    "            loss = sum(losses) / len(losses)\n",
    "            error_rate = 100 * sum(errors) / sum(sizes)\n",
    "        return loss, error_rate \n",
    "    \n",
    "class LSTM(Seq2Seq):\n",
    "    def __init__(self, \n",
    "                 source_index, \n",
    "                 target_index, \n",
    "                 encoder_embedding_dimension = 32,\n",
    "                 decoder_embedding_dimension = 32,\n",
    "                 encoder_hidden_units = 128, \n",
    "                 encoder_layers = 2,\n",
    "                 decoder_hidden_units = 128,\n",
    "                 decoder_layers = 2,\n",
    "                 dropout = 0.0):\n",
    "        \"\"\"\n",
    "        A standard Seq2Seq LSTM model as in 'Learning Phrase Representations using RNN Encoder-Decoder \n",
    "        for Statistical Machine Translation' by Cho et al. (2014). \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        in_vocabulary: dictionary\n",
    "            Vocabulary with the index:token pairs for the inputs of the model.\n",
    "            \n",
    "        out_vocabulary: dictionary\n",
    "            Vocabulary with the token:index pairs for the outputs of the model.\n",
    "            \n",
    "        encoder_embedding_dimension: int\n",
    "            Dimension of the embeddings to feed into the encoder.\n",
    "            \n",
    "        decoder_embedding_dimension: int\n",
    "            Dimension of the embeddings to feed into the decoder.\n",
    "            \n",
    "        encoder_hidden_units: int\n",
    "            Hidden size of the encoder.\n",
    "            \n",
    "        encoder_layers: int\n",
    "            Hidden layers of the encoder.\n",
    "            \n",
    "        decoder_hidden_units: int\n",
    "            Hidden units of the decoder.\n",
    "            \n",
    "        decoder_layers: int\n",
    "            Hidden layers of the decoder.\n",
    "            \n",
    "        dropout: float between 0.0 and 1.0\n",
    "            Dropout rate to apply to whole model.\n",
    "        \"\"\"\n",
    "        self.source_index = source_index\n",
    "        self.target_index = target_index\n",
    "        super().__init__()\n",
    "        self.source_embeddings = nn.Embedding(len(source_index), encoder_embedding_dimension)\n",
    "        self.target_embeddings = nn.Embedding(len(target_index), decoder_embedding_dimension)\n",
    "        self.encoder_rnn = nn.LSTM(input_size = encoder_embedding_dimension, \n",
    "                                   hidden_size = encoder_hidden_units, \n",
    "                                   num_layers = encoder_layers,\n",
    "                                   dropout = dropout)\n",
    "        self.decoder_rnn = nn.LSTM(input_size = encoder_layers * encoder_hidden_units + decoder_embedding_dimension, \n",
    "                                   hidden_size = decoder_hidden_units, \n",
    "                                   num_layers = decoder_layers,\n",
    "                                   dropout = dropout)\n",
    "        self.output_layer = nn.Linear(decoder_hidden_units, len(target_index))\n",
    "        self.architecture = dict(model = \"Seq2Seq LSTM\",\n",
    "                                 source_index = source_index, \n",
    "                                 target_index = target_index, \n",
    "                                 encoder_embedding_dimension = encoder_embedding_dimension,\n",
    "                                 decoder_embedding_dimension = decoder_embedding_dimension,\n",
    "                                 encoder_hidden_units = encoder_hidden_units, \n",
    "                                 encoder_layers = encoder_layers,\n",
    "                                 decoder_hidden_units = decoder_hidden_units,\n",
    "                                 decoder_layers = decoder_layers,\n",
    "                                 dropout = dropout)\n",
    "        self.print_architecture()\n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"\n",
    "        Forward method of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: LongTensor of shape (batch_size, input_length)\n",
    "            Tensor of integers containing the inputs for the model.\n",
    "            \n",
    "        Y: LongTensor of shape (batch_size, output_length)\n",
    "            Tensor of integers containing the output produced so far.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        output: FloatTensor of shape (batch_size, output_length, len(out_vocabulary))\n",
    "            Tensor of floats containing the inputs for the final Softmax layer (usually integrated into the loss function).\n",
    "        \"\"\"\n",
    "        X = self.source_embeddings(X.T)\n",
    "        encoder, (encoder_last_hidden, encoder_last_memory) = self.encoder_rnn(X)\n",
    "        encoder_last_hidden = encoder_last_hidden.transpose(0, 1).flatten(start_dim = 1)\n",
    "        encoder_last_hidden = encoder_last_hidden.repeat((Y.shape[1], 1, 1))\n",
    "        Y = self.target_embeddings(Y.T)\n",
    "        Y = torch.cat((Y, encoder_last_hidden), axis = -1)\n",
    "        decoder, (decoder_last_hidden, decoder_last_memory) = self.decoder_rnn(Y)\n",
    "        output = self.output_layer(decoder.transpose(0, 1))\n",
    "        return output        \n",
    "    \n",
    "    \n",
    "class ReversingLSTM(Seq2Seq):\n",
    "    def __init__(self, \n",
    "                 source_index, \n",
    "                 target_index, \n",
    "                 encoder_embedding_dimension = 32,\n",
    "                 decoder_embedding_dimension = 32,\n",
    "                 encoder_hidden_units = 128, \n",
    "                 encoder_layers = 2,\n",
    "                 decoder_hidden_units = 128,\n",
    "                 decoder_layers = 2,\n",
    "                 dropout = 0.0):\n",
    "        \"\"\"\n",
    "        A standard Seq2Seq LSTM model that reverses the order of the input as in \n",
    "        'Sequence to sequence learning with Neural Networks' by Sutskever et al. (2014). \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        in_vocabulary: dictionary\n",
    "            Vocabulary with the index:token pairs for the inputs of the model.\n",
    "            \n",
    "        out_vocabulary: dictionary\n",
    "            Vocabulary with the token:index pairs for the outputs of the model.\n",
    "            \n",
    "        encoder_embedding_dimension: int\n",
    "            Dimension of the embeddings to feed into the encoder.\n",
    "            \n",
    "        decoder_embedding_dimension: int\n",
    "            Dimension of the embeddings to feed into the decoder.\n",
    "            \n",
    "        encoder_hidden_units: int\n",
    "            Hidden size of the encoder.\n",
    "            \n",
    "        encoder_layers: int\n",
    "            Hidden layers of the encoder.\n",
    "            \n",
    "        decoder_hidden_units: int\n",
    "            Hidden units of the decoder.\n",
    "            \n",
    "        decoder_layers: int\n",
    "            Hidden layers of the decoder.\n",
    "            \n",
    "        dropout: float between 0.0 and 1.0\n",
    "            Dropout rate to apply to whole model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.source_index = source_index\n",
    "        self.target_index = target_index\n",
    "        self.source_embeddings = nn.Embedding(len(source_index), encoder_embedding_dimension)\n",
    "        self.target_embeddings = nn.Embedding(len(target_index), decoder_embedding_dimension)\n",
    "        self.encoder_rnn = nn.LSTM(input_size = encoder_embedding_dimension, \n",
    "                                   hidden_size = encoder_hidden_units, \n",
    "                                   num_layers = encoder_layers,\n",
    "                                   dropout = dropout)\n",
    "        self.decoder_rnn = nn.LSTM(input_size = decoder_embedding_dimension, \n",
    "                                   hidden_size = decoder_hidden_units, \n",
    "                                   num_layers = decoder_layers,\n",
    "                                   dropout = dropout)\n",
    "        self.output_layer = nn.Linear(decoder_hidden_units, len(target_index))\n",
    "        self.enc2dec = nn.Linear(encoder_hidden_units * encoder_layers, decoder_hidden_units * decoder_layers)\n",
    "        self.architecture = dict(model = \"Seq2Seq Reversing LSTM\",\n",
    "                                 source_index = source_index, \n",
    "                                 target_index = target_index, \n",
    "                                 encoder_embedding_dimension = encoder_embedding_dimension,\n",
    "                                 decoder_embedding_dimension = decoder_embedding_dimension,\n",
    "                                 encoder_hidden_units = encoder_hidden_units, \n",
    "                                 encoder_layers = encoder_layers,\n",
    "                                 decoder_hidden_units = decoder_hidden_units,\n",
    "                                 decoder_layers = decoder_layers,\n",
    "                                 dropout = dropout)\n",
    "        self.print_architecture()\n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"\n",
    "        Forward method of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: LongTensor of shape (batch_size, input_length)\n",
    "            Tensor of integers containing the inputs for the model.\n",
    "            \n",
    "        Y: LongTensor of shape (batch_size, output_length)\n",
    "            Tensor of integers containing the output produced so far.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        output: FloatTensor of shape (batch_size, output_length, len(out_vocabulary))\n",
    "            Tensor of floats containing the inputs for the final Softmax layer (usually integrated into the loss function).\n",
    "        \"\"\"\n",
    "        X = self.source_embeddings(torch.flip(X.T, dims = (1, )))\n",
    "        encoder, (encoder_last_hidden, encoder_last_memory) = self.encoder_rnn(X)\n",
    "        encoder_last_hidden = encoder_last_hidden.transpose(0, 1).flatten(start_dim = 1)\n",
    "        enc2dec = self.enc2dec(encoder_last_hidden)\\\n",
    "        .reshape(-1, self.decoder_rnn.num_layers, self.decoder_rnn.hidden_size)\\\n",
    "        .transpose(0, 1)\\\n",
    "        .contiguous()\n",
    "        Y = self.target_embeddings(Y.T)\n",
    "        decoder, (decoder_last_hidden, decoder_last_memory) = self.decoder_rnn(Y, (enc2dec, torch.zeros_like(enc2dec)))\n",
    "        output = self.output_layer(decoder.transpose(0, 1))\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class Transformer(Seq2Seq):\n",
    "    def __init__(self, \n",
    "                 source_index, \n",
    "                 target_index,\n",
    "                 max_sequence_length = 32,\n",
    "                 embedding_dimension = 32,\n",
    "                 feedforward_dimension = 128,\n",
    "                 encoder_layers = 2,\n",
    "                 decoder_layers = 2,\n",
    "                 attention_heads = 2,\n",
    "                 activation = \"relu\",\n",
    "                 dropout = 0.0):\n",
    "        \"\"\"\n",
    "        The standard PyTorch implementation of a Transformer model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        in_vocabulary: dictionary\n",
    "            Vocabulary with the index:token pairs for the inputs of the model.\n",
    "            \n",
    "        out_vocabulary: dictionary\n",
    "            Vocabulary with the token:index pairs for the outputs of the model.\n",
    "            \n",
    "        max_sequence_length: int\n",
    "            Maximum sequence length accepted by the model, both for the encoder and the decoder.\n",
    "            \n",
    "        embedding_dimension: int\n",
    "            Dimension of the embeddings of the model.\n",
    "            \n",
    "        feedforward_dimension: int\n",
    "            Dimension of the feedforward network inside the self-attention layers of the model.\n",
    "            \n",
    "        encoder_layers: int\n",
    "            Hidden layers of the encoder.\n",
    "            \n",
    "        decoder_layers: int\n",
    "            Hidden layers of the decoder.\n",
    "            \n",
    "        attention_heads: int\n",
    "            Attention heads inside every self-attention layer of the model.\n",
    "            \n",
    "        activation: string\n",
    "            Activation function of the feedforward network inside the self-attention layers of the model. Can\n",
    "            be either 'relu' or 'gelu'.\n",
    "            \n",
    "        dropout: float between 0.0 and 1.0\n",
    "            Dropout rate to apply to whole model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.source_index = source_index\n",
    "        self.target_index = target_index\n",
    "        self.source_embeddings = nn.Embedding(len(source_index), embedding_dimension)\n",
    "        self.target_embeddings = nn.Embedding(len(target_index), embedding_dimension)\n",
    "        self.positional_embeddings = nn.Embedding(max_sequence_length, embedding_dimension)\n",
    "        self.transformer = nn.Transformer(d_model = embedding_dimension, \n",
    "                                          dim_feedforward = feedforward_dimension,\n",
    "                                          nhead = attention_heads, \n",
    "                                          num_encoder_layers = encoder_layers, \n",
    "                                          num_decoder_layers = decoder_layers,\n",
    "                                          activation = activation,\n",
    "                                          dropout = dropout)\n",
    "        self.output_layer = nn.Linear(embedding_dimension, len(target_index))\n",
    "        self.architecture = dict(model = \"Seq2Seq Transformer\",\n",
    "                                 source_index = source_index,\n",
    "                                 target_index = target_index,\n",
    "                                 max_sequence_length = max_sequence_length,\n",
    "                                 embedding_dimension = embedding_dimension,\n",
    "                                 feedforward_dimension = feedforward_dimension,\n",
    "                                 encoder_layers = encoder_layers,\n",
    "                                 decoder_layers = decoder_layers,\n",
    "                                 attention_heads = attention_heads,\n",
    "                                 activation = activation,\n",
    "                                 dropout = dropout)\n",
    "        self.print_architecture()\n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"\n",
    "        Forward method of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: LongTensor of shape (batch_size, input_length)\n",
    "            Tensor of integers containing the inputs for the model.\n",
    "            \n",
    "        Y: LongTensor of shape (batch_size, output_length)\n",
    "            Tensor of integers containing the output produced so far.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        output: FloatTensor of shape (batch_size, output_length, len(out_vocabulary))\n",
    "            Tensor of floats containing the inputs for the final Softmax layer (usually integrated in the loss function).\n",
    "        \"\"\"\n",
    "        assert X.shape[1] <= self.architecture[\"max_sequence_length\"]\n",
    "        assert Y.shape[1] <= self.architecture[\"max_sequence_length\"]\n",
    "        X = self.source_embeddings(X)\n",
    "        X_positional = torch.arange(X.shape[1], device = X.device).repeat((X.shape[0], 1))\n",
    "        X_positional = self.positional_embeddings(X_positional)\n",
    "        X = (X + X_positional).transpose(0, 1)\n",
    "        Y = self.target_embeddings(Y)\n",
    "        Y_positional = torch.arange(Y.shape[1], device = Y.device).repeat((Y.shape[0], 1))\n",
    "        Y_positional = self.positional_embeddings(Y_positional)\n",
    "        Y = (Y + Y_positional).transpose(0, 1)\n",
    "        mask = self.transformer.generate_square_subsequent_mask(Y.shape[0]).to(Y.device)\n",
    "        transformer_output = self.transformer.forward(src = X,\n",
    "                                                      tgt = Y, \n",
    "                                                      tgt_mask = mask)\n",
    "        transformer_output = transformer_output.transpose(0, 1)\n",
    "        return self.output_layer(transformer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Seq2Seq Transformer\n",
      "Source index: {'\\t': 3, '\\n': 4, ' ': 5, '!': 6, '\"': 7, '#': 8, '$': 9, '%': 10, '&': 11, \"'\": 12, '(': 13, ')': 14, '*': 15, '+': 16, ',': 17, '-': 18, '.': 19, '/': 20, '0': 21, '1': 22, '2': 23, '3': 24, '4': 25, '5': 26, '6': 27, '7': 28, '8': 29, '9': 30, ':': 31, ';': 32, '<': 33, '<UNK>': 34, '=': 35, '>': 36, '?': 37, '@': 38, 'A': 39, 'B': 40, 'C': 41, 'D': 42, 'E': 43, 'F': 44, 'G': 45, 'H': 46, 'I': 47, 'J': 48, 'K': 49, 'L': 50, 'M': 51, 'N': 52, 'O': 53, 'P': 54, 'Q': 55, 'R': 56, 'S': 57, 'T': 58, 'U': 59, 'V': 60, 'W': 61, 'X': 62, 'Y': 63, 'Z': 64, '[': 65, '\\\\': 66, ']': 67, '^': 68, '_': 69, '`': 70, 'a': 71, 'b': 72, 'c': 73, 'd': 74, 'e': 75, 'f': 76, 'g': 77, 'h': 78, 'i': 79, 'j': 80, 'k': 81, 'l': 82, 'm': 83, 'n': 84, 'o': 85, 'p': 86, 'q': 87, 'r': 88, 's': 89, 't': 90, 'u': 91, 'v': 92, 'w': 93, 'x': 94, 'y': 95, 'z': 96, '{': 97, '|': 98, '}': 99, '~': 100, '\\xa0': 101, '¡': 102, '¢': 103, '£': 104, '¥': 105, '§': 106, '©': 107, 'ª': 108, '«': 109, '\\xad': 110, '®': 111, '¯': 112, '°': 113, '±': 114, '³': 115, '´': 116, 'µ': 117, '¶': 118, '»': 119, '¼': 120, '½': 121, '¿': 122, 'À': 123, 'Á': 124, 'Â': 125, 'Ã': 126, 'Å': 127, 'É': 128, 'Ê': 129, 'Í': 130, 'Ð': 131, 'Ò': 132, 'Ó': 133, 'Õ': 134, 'Ö': 135, 'Ü': 136, 'Þ': 137, 'à': 138, 'á': 139, 'â': 140, 'ä': 141, 'ç': 142, 'è': 143, 'é': 144, 'ê': 145, 'ë': 146, 'í': 147, 'î': 148, 'ï': 149, 'ò': 150, 'ó': 151, 'ô': 152, 'ö': 153, 'ø': 154, 'ú': 155, 'ü': 156, 'ý': 157, 'ć': 158, 'Č': 159, 'č': 160, 'İ': 161, 'ł': 162, 'ń': 163, 'ő': 164, 'ř': 165, 'ş': 166, 'š': 167, 'ź': 168, 'Ż': 169, 'ż': 170, 'ž': 171, '̆': 172, '̇': 173, '΄': 174, 'Ά': 175, 'Έ': 176, 'Ί': 177, 'Ό': 178, 'Ώ': 179, 'Α': 180, 'Β': 181, 'Γ': 182, 'Δ': 183, 'Ε': 184, 'Ζ': 185, 'Η': 186, 'Θ': 187, 'Ι': 188, 'Κ': 189, 'Λ': 190, 'Μ': 191, 'Ν': 192, 'Ξ': 193, 'Ο': 194, 'Π': 195, 'Ρ': 196, 'Σ': 197, 'Τ': 198, 'Υ': 199, 'Φ': 200, 'Χ': 201, 'Ω': 202, 'ά': 203, 'έ': 204, 'ή': 205, 'ί': 206, 'α': 207, 'β': 208, 'γ': 209, 'δ': 210, 'ε': 211, 'ζ': 212, 'η': 213, 'θ': 214, 'ι': 215, 'κ': 216, 'λ': 217, 'μ': 218, 'ν': 219, 'ξ': 220, 'ο': 221, 'π': 222, 'ρ': 223, 'ς': 224, 'σ': 225, 'τ': 226, 'υ': 227, 'φ': 228, 'χ': 229, 'ψ': 230, 'ω': 231, 'ϊ': 232, 'ό': 233, 'ύ': 234, 'ώ': 235, 'ϐ': 236, 'ϱ': 237, 'ἀ': 238, 'ἁ': 239, 'ἂ': 240, 'ἃ': 241, 'ἄ': 242, 'ἆ': 243, 'Ἀ': 244, 'Ἅ': 245, 'ἐ': 246, 'ἓ': 247, 'Ἐ': 248, 'ἣ': 249, 'Ἠ': 250, 'Ἡ': 251, 'ἰ': 252, 'ἱ': 253, 'ἲ': 254, 'ἳ': 255, 'ἴ': 256, 'ἵ': 257, 'ἶ': 258, 'ἷ': 259, 'Ἰ': 260, 'Ἱ': 261, 'Ἴ': 262, 'ὁ': 263, 'ὃ': 264, 'ὅ': 265, 'Ὀ': 266, 'Ὁ': 267, 'Ὃ': 268, 'Ὅ': 269, 'ὐ': 270, 'ὑ': 271, 'ὓ': 272, 'ὖ': 273, 'ὠ': 274, 'Ὦ': 275, 'ὰ': 276, 'ὲ': 277, 'ὴ': 278, 'ὶ': 279, 'ὸ': 280, 'ᾱ': 281, 'ᾳ': 282, '᾿': 283, 'ῃ': 284, 'ῇ': 285, 'ῖ': 286, 'ῥ': 287, 'ῦ': 288, 'Ῥ': 289, '–': 290, '—': 291, '‘': 292, '’': 293, '“': 294, '”': 295, '€': 296, '™': 297, '↑': 298, '→': 299, '↓': 300, '↔': 301, '↕': 302, '↖': 303, '↗': 304, '↘': 305, '↙': 306, '↛': 307, '↜': 308, '↝': 309, '↠': 310, '↡': 311, '↢': 312, '↣': 313, '↤': 314, '↥': 315, '↦': 316, '↧': 317, '↨': 318, '↩': 319, '↪': 320, '↫': 321, '↭': 322, '↰': 323, '↱': 324, '↲': 325, '↳': 326, '↴': 327, '↵': 328, '↶': 329, '↷': 330, '↸': 331, '↺': 332, '↻': 333, '↼': 334, '↽': 335, '↾': 336, '↿': 337, '⇀': 338, '⇁': 339, '⇂': 340, '⇃': 341, '⇄': 342, '⇆': 343, '⇇': 344, '⇈': 345, '⇉': 346, '⇍': 347, '⇖': 348, '⇜': 349, '⇠': 350, '⇡': 351, '⇥': 352, '⇪': 353, '∀': 354, '∁': 355, '∂': 356, '∃': 357, '∆': 358, '∇': 359, '∉': 360, '∊': 361, '∋': 362, '∍': 363, '∎': 364, '∏': 365, '∐': 366, '∑': 367, '−': 368, '∓': 369, '∔': 370, '∕': 371, '∖': 372, '∙': 373, '∞': 374, '∟': 375, '∠': 376, '∡': 377, '∢': 378, '∣': 379, '∤': 380, '∥': 381, '∩': 382, '∪': 383, '∫': 384, '∱': 385, '∶': 386, '∷': 387, '∸': 388, '∺': 389, '∼': 390, '∾': 391, '∿': 392, '≀': 393, '≃': 394, '≈': 395, '≊': 396, '≋': 397, '≏': 398, '≒': 399, '≓': 400, '≖': 401, '≙': 402, '≚': 403, '≜': 404, '≝': 405, '≞': 406, '≟': 407, '≡': 408, '≣': 409, '≤': 410, '≥': 411, '≦': 412, '≧': 413, '≨': 414, '≩': 415, '≪': 416, '≯': 417, '≱': 418, '≲': 419, '≳': 420, '≴': 421, '≵': 422, '≶': 423, '≸': 424, '≹': 425, '≺': 426, '≻': 427, '≼': 428, '≽': 429, '≿': 430, '⊀': 431, '⊂': 432, '⊃': 433, '⊆': 434, '⊇': 435, '⊊': 436, '⊋': 437, '⊍': 438, '⊏': 439, '⊐': 440, '⊑': 441, '⊒': 442, '⊓': 443, '⊔': 444, '⊖': 445, '⊙': 446, '⊜': 447, '⊟': 448, '⊡': 449, '⊢': 450, '⊣': 451, '⊤': 452, '⊥': 453, '⊨': 454, '⊪': 455, '⊰': 456, '⊱': 457, '⊲': 458, '⊳': 459, '⊴': 460, '⊸': 461, '⊹': 462, '⊺': 463, '⊻': 464, '⊼': 465, '⊽': 466, '⊾': 467, '⋀': 468, '⋁': 469, '⋂': 470, '⋃': 471, '⋅': 472, '⋈': 473, '⋉': 474, '⋊': 475, '⋋': 476, '⋍': 477, '⋎': 478, '⋏': 479, '⋔': 480, '⋖': 481, '⋗': 482, '⋚': 483, '⋜': 484, '⋝': 485, '⋞': 486, '⋟': 487, '⋠': 488, '⋡': 489, '⋣': 490, '⋤': 491, '⋥': 492, '⋦': 493, '⋨': 494, '⋪': 495, '⋮': 496, '⋯': 497, '⋰': 498, '⋱': 499, '<PAD>': 0, '<START>': 1, '<END>': 2}\n",
      "Target index: {3: '\\t', 4: '\\n', 5: ' ', 6: '!', 7: '\"', 8: '#', 9: '$', 10: '%', 11: '&', 12: \"'\", 13: '(', 14: ')', 15: '*', 16: '+', 17: ',', 18: '-', 19: '.', 20: '/', 21: '0', 22: '1', 23: '2', 24: '3', 25: '4', 26: '5', 27: '6', 28: '7', 29: '8', 30: '9', 31: ':', 32: ';', 33: '<', 34: '<UNK>', 35: '=', 36: '>', 37: '?', 38: '@', 39: 'A', 40: 'B', 41: 'C', 42: 'D', 43: 'E', 44: 'F', 45: 'G', 46: 'H', 47: 'I', 48: 'J', 49: 'K', 50: 'L', 51: 'M', 52: 'N', 53: 'O', 54: 'P', 55: 'Q', 56: 'R', 57: 'S', 58: 'T', 59: 'U', 60: 'V', 61: 'W', 62: 'X', 63: 'Y', 64: 'Z', 65: '[', 66: '\\\\', 67: ']', 68: '^', 69: '_', 70: '`', 71: 'a', 72: 'b', 73: 'c', 74: 'd', 75: 'e', 76: 'f', 77: 'g', 78: 'h', 79: 'i', 80: 'j', 81: 'k', 82: 'l', 83: 'm', 84: 'n', 85: 'o', 86: 'p', 87: 'q', 88: 'r', 89: 's', 90: 't', 91: 'u', 92: 'v', 93: 'w', 94: 'x', 95: 'y', 96: 'z', 97: '{', 98: '|', 99: '}', 100: '~', 101: '\\xa0', 102: '¡', 103: '¢', 104: '£', 105: '¥', 106: '§', 107: '©', 108: 'ª', 109: '«', 110: '\\xad', 111: '®', 112: '¯', 113: '°', 114: '±', 115: '³', 116: '´', 117: 'µ', 118: '¶', 119: '»', 120: '¼', 121: '½', 122: '¿', 123: 'À', 124: 'Á', 125: 'Â', 126: 'Ã', 127: 'Å', 128: 'É', 129: 'Ê', 130: 'Í', 131: 'Ð', 132: 'Ò', 133: 'Ó', 134: 'Õ', 135: 'Ö', 136: 'Ü', 137: 'Þ', 138: 'à', 139: 'á', 140: 'â', 141: 'ä', 142: 'ç', 143: 'è', 144: 'é', 145: 'ê', 146: 'ë', 147: 'í', 148: 'î', 149: 'ï', 150: 'ò', 151: 'ó', 152: 'ô', 153: 'ö', 154: 'ø', 155: 'ú', 156: 'ü', 157: 'ý', 158: 'ć', 159: 'Č', 160: 'č', 161: 'İ', 162: 'ł', 163: 'ń', 164: 'ő', 165: 'ř', 166: 'ş', 167: 'š', 168: 'ź', 169: 'Ż', 170: 'ż', 171: 'ž', 172: '̆', 173: '̇', 174: '΄', 175: 'Ά', 176: 'Έ', 177: 'Ί', 178: 'Ό', 179: 'Ώ', 180: 'Α', 181: 'Β', 182: 'Γ', 183: 'Δ', 184: 'Ε', 185: 'Ζ', 186: 'Η', 187: 'Θ', 188: 'Ι', 189: 'Κ', 190: 'Λ', 191: 'Μ', 192: 'Ν', 193: 'Ξ', 194: 'Ο', 195: 'Π', 196: 'Ρ', 197: 'Σ', 198: 'Τ', 199: 'Υ', 200: 'Φ', 201: 'Χ', 202: 'Ω', 203: 'ά', 204: 'έ', 205: 'ή', 206: 'ί', 207: 'α', 208: 'β', 209: 'γ', 210: 'δ', 211: 'ε', 212: 'ζ', 213: 'η', 214: 'θ', 215: 'ι', 216: 'κ', 217: 'λ', 218: 'μ', 219: 'ν', 220: 'ξ', 221: 'ο', 222: 'π', 223: 'ρ', 224: 'ς', 225: 'σ', 226: 'τ', 227: 'υ', 228: 'φ', 229: 'χ', 230: 'ψ', 231: 'ω', 232: 'ϊ', 233: 'ό', 234: 'ύ', 235: 'ώ', 236: 'ϐ', 237: 'ϱ', 238: 'ἀ', 239: 'ἁ', 240: 'ἂ', 241: 'ἃ', 242: 'ἄ', 243: 'ἆ', 244: 'Ἀ', 245: 'Ἅ', 246: 'ἐ', 247: 'ἓ', 248: 'Ἐ', 249: 'ἣ', 250: 'Ἠ', 251: 'Ἡ', 252: 'ἰ', 253: 'ἱ', 254: 'ἲ', 255: 'ἳ', 256: 'ἴ', 257: 'ἵ', 258: 'ἶ', 259: 'ἷ', 260: 'Ἰ', 261: 'Ἱ', 262: 'Ἴ', 263: 'ὁ', 264: 'ὃ', 265: 'ὅ', 266: 'Ὀ', 267: 'Ὁ', 268: 'Ὃ', 269: 'Ὅ', 270: 'ὐ', 271: 'ὑ', 272: 'ὓ', 273: 'ὖ', 274: 'ὠ', 275: 'Ὦ', 276: 'ὰ', 277: 'ὲ', 278: 'ὴ', 279: 'ὶ', 280: 'ὸ', 281: 'ᾱ', 282: 'ᾳ', 283: '᾿', 284: 'ῃ', 285: 'ῇ', 286: 'ῖ', 287: 'ῥ', 288: 'ῦ', 289: 'Ῥ', 290: '–', 291: '—', 292: '‘', 293: '’', 294: '“', 295: '”', 296: '€', 297: '™', 298: '↑', 299: '→', 300: '↓', 301: '↔', 302: '↕', 303: '↖', 304: '↗', 305: '↘', 306: '↙', 307: '↛', 308: '↜', 309: '↝', 310: '↠', 311: '↡', 312: '↢', 313: '↣', 314: '↤', 315: '↥', 316: '↦', 317: '↧', 318: '↨', 319: '↩', 320: '↪', 321: '↫', 322: '↭', 323: '↰', 324: '↱', 325: '↲', 326: '↳', 327: '↴', 328: '↵', 329: '↶', 330: '↷', 331: '↸', 332: '↺', 333: '↻', 334: '↼', 335: '↽', 336: '↾', 337: '↿', 338: '⇀', 339: '⇁', 340: '⇂', 341: '⇃', 342: '⇄', 343: '⇆', 344: '⇇', 345: '⇈', 346: '⇉', 347: '⇍', 348: '⇖', 349: '⇜', 350: '⇠', 351: '⇡', 352: '⇥', 353: '⇪', 354: '∀', 355: '∁', 356: '∂', 357: '∃', 358: '∆', 359: '∇', 360: '∉', 361: '∊', 362: '∋', 363: '∍', 364: '∎', 365: '∏', 366: '∐', 367: '∑', 368: '−', 369: '∓', 370: '∔', 371: '∕', 372: '∖', 373: '∙', 374: '∞', 375: '∟', 376: '∠', 377: '∡', 378: '∢', 379: '∣', 380: '∤', 381: '∥', 382: '∩', 383: '∪', 384: '∫', 385: '∱', 386: '∶', 387: '∷', 388: '∸', 389: '∺', 390: '∼', 391: '∾', 392: '∿', 393: '≀', 394: '≃', 395: '≈', 396: '≊', 397: '≋', 398: '≏', 399: '≒', 400: '≓', 401: '≖', 402: '≙', 403: '≚', 404: '≜', 405: '≝', 406: '≞', 407: '≟', 408: '≡', 409: '≣', 410: '≤', 411: '≥', 412: '≦', 413: '≧', 414: '≨', 415: '≩', 416: '≪', 417: '≯', 418: '≱', 419: '≲', 420: '≳', 421: '≴', 422: '≵', 423: '≶', 424: '≸', 425: '≹', 426: '≺', 427: '≻', 428: '≼', 429: '≽', 430: '≿', 431: '⊀', 432: '⊂', 433: '⊃', 434: '⊆', 435: '⊇', 436: '⊊', 437: '⊋', 438: '⊍', 439: '⊏', 440: '⊐', 441: '⊑', 442: '⊒', 443: '⊓', 444: '⊔', 445: '⊖', 446: '⊙', 447: '⊜', 448: '⊟', 449: '⊡', 450: '⊢', 451: '⊣', 452: '⊤', 453: '⊥', 454: '⊨', 455: '⊪', 456: '⊰', 457: '⊱', 458: '⊲', 459: '⊳', 460: '⊴', 461: '⊸', 462: '⊹', 463: '⊺', 464: '⊻', 465: '⊼', 466: '⊽', 467: '⊾', 468: '⋀', 469: '⋁', 470: '⋂', 471: '⋃', 472: '⋅', 473: '⋈', 474: '⋉', 475: '⋊', 476: '⋋', 477: '⋍', 478: '⋎', 479: '⋏', 480: '⋔', 481: '⋖', 482: '⋗', 483: '⋚', 484: '⋜', 485: '⋝', 486: '⋞', 487: '⋟', 488: '⋠', 489: '⋡', 490: '⋣', 491: '⋤', 492: '⋥', 493: '⋦', 494: '⋨', 495: '⋪', 496: '⋮', 497: '⋯', 498: '⋰', 499: '⋱', 0: '<PAD>', 1: '<START>', 2: '<END>'}\n",
      "Max sequence length: 110\n",
      "Embedding dimension: 512\n",
      "Feedforward dimension: 2048\n",
      "Encoder layers: 4\n",
      "Decoder layers: 4\n",
      "Attention heads: 8\n",
      "Activation: relu\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 30,252,532\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import torch\n",
    "\n",
    "# model = Transformer(char2i, \n",
    "#                     i2char, \n",
    "#                     max_sequence_length = 110,\n",
    "#                     embedding_dimension = 512, #256,\n",
    "#                     feedforward_dimension = 2048, #1024,\n",
    "#                     attention_heads = 8,\n",
    "#                     encoder_layers = 4,\n",
    "#                     decoder_layers = 4)\n",
    "#                    #dropout = .5)\n",
    "# print(\"model created\")\n",
    "# model.to(device)\n",
    "\n",
    "# model = Transformer(char2i, \n",
    "#                     i2char, \n",
    "#                     max_sequence_length = 110,\n",
    "#                     embedding_dimension = 512, #256,\n",
    "#                     feedforward_dimension = 2048, #1024,\n",
    "#                     attention_heads = 8,\n",
    "#                     encoder_layers = 4,\n",
    "#                     decoder_layers = 4)\n",
    "\n",
    "\n",
    "model = Transformer(char2i, \n",
    "                    i2char, \n",
    "                    max_sequence_length = 110,\n",
    "                    embedding_dimension = 512,\n",
    "                    feedforward_dimension = 2048,\n",
    "                    attention_heads = 8,\n",
    "                    encoder_layers = 4,\n",
    "                    decoder_layers = 4)#,\n",
    "                   #dropout = .5)\n",
    "\n",
    "model.load_state_dict(torch.load(model_save_dir + model_file, map_location=torch.device('cpu')))\n",
    "#model = torch.load(\"../../data/torch_file60.pt\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pd.read_csv('./source_dataframe.csv')\n",
    "# print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(file_dir + test_file)\n",
    "\n",
    "#test_data = test_data.iloc[:ntest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source = []\n",
    "# target = []\n",
    "# source_aligned = []\n",
    "# target_aligned = []\n",
    "# for i in range(len(test_data)):\n",
    "#     d = test_data.iloc[i]\n",
    "#     s = np.array(list(d['aligned sentences source'])) # aligned source\n",
    "#     t = np.array(list(d['aligned sentences target'])) # aligned target\n",
    "#     a = np.array(list(get_fill_in_types(d['aligned sentences target types'])))\n",
    "#     ss = \"\".join(s[np.where( (a == ' ') | (a == 'W') | (a == 'w'))[0]].tolist())\n",
    "#     tt = \"\".join(t[np.where( (a == ' ') | (a == 'W') | (a == 'w'))[0]].tolist())\n",
    "#     source_aligned.append(ss)\n",
    "#     target_aligned.append(tt)\n",
    "#     source.append(ss.replace('@',''))\n",
    "#     target.append(tt.replace('^',''))\n",
    "    \n",
    "# test_data['words source aligned'] = source_aligned\n",
    "# test_data['words target aligned'] = target_aligned\n",
    "# test_data['words source'] = source\n",
    "# test_data['words target'] = target\n",
    "\n",
    "def add_formatted_columns(datain):\n",
    "    source = []\n",
    "    target = []\n",
    "    source_aligned = []\n",
    "    target_aligned = []\n",
    "    for i in range(len(datain)):\n",
    "        d = datain.iloc[i]\n",
    "        s = np.array(list(d['aligned sentences source'])) # aligned source, with ^ symbols\n",
    "        t = np.array(list(d['aligned sentences target'])) # aligned target, with @ symbols\n",
    "        a = np.array(list(get_fill_in_types(d['aligned sentences target types'])))\n",
    "        ss = \"\".join(s[np.where( (a == ' ') | (a == 'W') | (a == 'w'))[0]].tolist())\n",
    "        tt = \"\".join(t[np.where( (a == ' ') | (a == 'W') | (a == 'w'))[0]].tolist())\n",
    "        source_aligned.append(ss.replace('^','@')) # align with original \n",
    "        target_aligned.append(tt)\n",
    "        source.append(ss.replace('^',''))\n",
    "        target.append(tt.replace('@',''))\n",
    "\n",
    "    datain['words source aligned'] = source_aligned\n",
    "    datain['words target aligned'] = target_aligned\n",
    "    datain['words source'] = source\n",
    "    datain['words target'] = target\n",
    "    return datain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = (pd.isnull(test_data['words source'])) | (pd.isnull(test_data['words target']))\n",
    "\n",
    "# test_data = test_data[~mask]\n",
    "\n",
    "# # now align\n",
    "# gs_aligned = []; ocr_aligned = []\n",
    "# gs = []; ocr = []\n",
    "# for o,p in zip(test_data['words source'].values, test_data['words target'].values):\n",
    "#     eops = Levenshtein.editops(o, p)\n",
    "#     ocr_text_aligned, pdf_text_aligned = align_texts_fast(o,p,eops)\n",
    "#     gs_aligned.append(pdf_text_aligned)\n",
    "#     # futze with ocr aligned so it matches data input\n",
    "#     ocr_text_aligned = ocr_text_aligned.replace('^', '@')\n",
    "#     ocr_aligned.append(ocr_text_aligned)\n",
    "#     gs.append(pdf_text_aligned.replace('@',''))\n",
    "#     ocr.append(ocr_text_aligned.replace('@',''))\n",
    "    \n",
    "# test_data['gs_aligned'] = gs_aligned\n",
    "# test_data['ocr_aligned'] = ocr_aligned\n",
    "# test_data['gs'] = gs\n",
    "# test_data['ocr'] = ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = add_formatted_columns(test_data)\n",
    "\n",
    "mask = (pd.isnull(test_data['words source'])) | (pd.isnull(test_data['words target']))\n",
    "\n",
    "test_data = test_data[~mask]\n",
    "len(mask[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.rename(columns={'words source':\"ocr_to_input\", \n",
    "                            'words source aligned': \"ocr_aligned\",\n",
    "                            'words target aligned':\"gs_aligned\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.assign(source = lambda df: df.ocr_aligned.str.replace(\"@\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(reference, hypothesis, progress_bar = False):\n",
    "    #assert len(reference) == len(hypothesis)\n",
    "    text = zip(reference, hypothesis)\n",
    "    if progress_bar:\n",
    "        text = tqdm(text, total = len(reference))\n",
    "    d = [distance(r, h) for r, h in text]\n",
    "    output = pd.DataFrame({\"reference\":reference, \"hypothesis\":hypothesis})\\\n",
    "    .assign(distance = lambda df: d)\\\n",
    "    .assign(\n",
    "        cer = lambda df: df.apply(\n",
    "            lambda r: 100 * r[\"distance\"] / max(len(r[\"reference\"]), 1), \n",
    "            axis = 1\n",
    "        )\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arr = test_data[\"ocr_to_input\"].to_numpy()\n",
    "# gs_arr = test_data[\"gs_aligned\"].to_numpy()\n",
    "ocr_list = test_data[\"ocr_aligned\"].tolist()\n",
    "gs_list = test_data[\"gs_aligned\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def uniform(j, window_size):\n",
    "    return 1.0\n",
    "\n",
    "def triangle(j, window_size):\n",
    "    m = window_size//2\n",
    "    return m - 0.5 * abs(m - j)\n",
    "\n",
    "def bell(j, window_size):\n",
    "    m = window_size // 2\n",
    "    s = window_size // 2\n",
    "    return exp(-((m-j)/s)**2)\n",
    "\n",
    "def disjoint(\n",
    "    string,\n",
    "    model, \n",
    "    source_index,\n",
    "    target_index,\n",
    "    window_size = 50,\n",
    "    decoding_method = \"greedy_search\", \n",
    "    document_progress_bar = False, \n",
    "    document_batch_progress_bar = 0, \n",
    "    *arcorrect\n",
    "):\n",
    "    model.eval()\n",
    "    windows = [string[i:i+window_size] \n",
    "        for i in range(0, len(string), window_size)]\n",
    "    windows = [\"\".join([source_index.vocabulary.lookup(c) for c in s])\\\n",
    "        .replace(\"<UNK>\", \" \") for s in windows]\n",
    "    #X = source_index.text2tensor(windows, progress_bar = False).cuda()\n",
    "    X = source_index.text2tensor(windows, progress_bar = False).cpu()\n",
    "    if decoding_method == \"greedy_search\":\n",
    "        predictions, probs = seq2seq.greedy_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )\n",
    "    elif decoding_method == \"beam_search\":\n",
    "        predictions, probs = seq2seq.beam_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )   \n",
    "        predictions = predictions[:, 0, :]\n",
    "    output = target_index.tensor2text(predictions)\n",
    "    output = [re.sub(r\"<START>|<PAD>|<UNK>|<END>.*\", \"\", s) for s in output]\n",
    "    return \"\".join(output)\n",
    "\n",
    "def n_grams(\n",
    "    string,\n",
    "    model,\n",
    "    source_index,\n",
    "    target_index,\n",
    "    window_size = 50, \n",
    "    decoding_method = \"greedy_search\", \n",
    "    weighting = \"uniform\",\n",
    "    document_progress_bar = False, \n",
    "    document_batch_progress_bar = 0,      \n",
    "    main_batch_size = 1024,\n",
    "    *arcorrect\n",
    "):\n",
    "    model.eval()\n",
    "    if len(string) <= window_size:\n",
    "        windows = [string]\n",
    "    else:\n",
    "        windows = [string[i:i + window_size] \n",
    "            for i in range(len(string) - window_size + 1)]\n",
    "    windows = [\"\".join([source_index.vocabulary.lookup(c) for c in s])\\\n",
    "    .replace(\"<UNK>\", \" \") for s in windows]\n",
    "    #X = source_index.text2tensor(windows, progress_bar = False).cuda()\n",
    "    X = source_index.text2tensor(windows, progress_bar = False).cpu()\n",
    "    if decoding_method == \"greedy_search\":\n",
    "        predictions, probs = seq2seq.greedy_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = False, \n",
    "            *arcorrect\n",
    "        )\n",
    "    if decoding_method == \"beam_search\":\n",
    "        predictions, probs = seq2seq.beam_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )   \n",
    "        predictions = predictions[:, 0, :]   \n",
    "    output = target_index.tensor2text(predictions)\n",
    "    output = [re.sub(r\"<START>|<PAD>|<UNK>|<END>.*\", \"\", s) for s in output]\n",
    "    if weighting == \"uniform\":\n",
    "        weighting = uniform\n",
    "    elif weighting == \"triangle\":\n",
    "        weighting = triangle\n",
    "    elif weighting == \"bell\":\n",
    "        weighting = bell\n",
    "    votes = [\n",
    "        {k:0.0 for k in target_index.vocabulary} \n",
    "        for c in string\n",
    "    ]\n",
    "    for i, s in enumerate(output):\n",
    "        for j, (counter, char)\\\n",
    "        in enumerate(zip(votes[i:i + window_size], s)):\n",
    "            counter[char] += weighting(j, window_size)\n",
    "    output = [max(c.keys(), key = lambda x: c[x]) for c in votes]\n",
    "    output = \"\".join(output)\n",
    "    return votes, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source = [list(\"abcdefghijkl\"), list(\"mnopqrstwxyz\")]\n",
    "#target = [list(\"ABCDEFGHIJKL\"), list(\"MNOPQRSTWXYZ\")] #update to astronomy vocab\n",
    "source = []#yy\n",
    "for s in model.source_index:\n",
    "    sentence = \"\"\n",
    "    for c in s:\n",
    "        #nu = source_index[c]\n",
    "        #sentence.append(c)\n",
    "        #sentence.append(num)\n",
    "        sentence += c\n",
    "    source.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_source = []\n",
    "n = 20 # chunk length  \n",
    "#for s in arr:\n",
    "ntest2 = 100\n",
    "for x in range(min([48,ntest2])):#only process n paragraphs?\n",
    "    s = ocr_list[x]\n",
    "    #sentence_array = list(s) #convert string to an array of characters\n",
    "    chunks = [s[i:i+n] for i in range(0, len(s), n)]\n",
    "    for sentence_chunk in chunks:\n",
    "        new_source.append(list(sentence_chunk))#append array to new_source array\n",
    "\n",
    "    #new_source.append(list(\"stars cataclysmic  \"))#mark end of sentences with stars cataclysmic\n",
    "    new_source.append( list(\"advice charm touch  \"))#new end of sentence with 20 chars\n",
    "    #because the model only allows 20 characters at a time, each row is 20 chars chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_beam_search import seq2seq\n",
    "source_index = seq2seq.Index(source)\n",
    "target_index =  source_index\n",
    "X_new = source_index.text2tensor(new_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions, log_probabilities = beam_search(\n",
    "#     model,\n",
    "#     X_new.cuda())\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "if not use_cuda:\n",
    "    predictions, log_probabilities = beam_search(\n",
    "        model,\n",
    "        X_new.cpu())\n",
    "else:\n",
    "    predictions, log_probabilities = beam_search(\n",
    "        model,\n",
    "        X_new.cuda())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  5, 58, 78, 75,  5, 89, 91, 89, 90, 75, 92, 75, 74,  5, 90, 78, 71,\n",
       "         90,  5, 90],\n",
       "        [ 1,  5, 58, 78, 75,  5, 89, 91, 77, 77, 75, 89, 90,  5, 90, 78, 71, 90,\n",
       "          5, 90, 78],\n",
       "        [ 1,  5, 58, 78, 75,  5, 89, 91, 92, 75, 89, 90, 75, 74,  5, 90, 78, 71,\n",
       "         90,  5, 90],\n",
       "        [ 1,  5, 58, 78, 75,  5, 89, 90, 91, 74, 75, 89,  5, 90, 78, 71, 90,  5,\n",
       "         90, 78, 71],\n",
       "        [ 1,  5, 58, 78, 75,  5, 89, 90, 91, 74, 75, 89,  5, 90, 78, 71, 90,  5,\n",
       "         90, 78, 75]], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [target_index.tensor2text(p) for p in predictions]\n",
    "# output = []\n",
    "# for p in predictions:\n",
    "#     try:\n",
    "#         xo = target_index.tensor2text(p)\n",
    "#         output.append(xo)\n",
    "#     except:\n",
    "#         print('key error', np.unique(p))\n",
    "#         output.append('KEY ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  5, 58,  ..., 90,  5, 90],\n",
       "         [ 1,  5, 58,  ...,  5, 90, 78],\n",
       "         [ 1,  5, 58,  ..., 90,  5, 90],\n",
       "         [ 1,  5, 58,  ..., 90, 78, 71],\n",
       "         [ 1,  5, 58,  ..., 90, 78, 75]],\n",
       "\n",
       "        [[ 1,  5, 85,  ..., 85, 84,  5],\n",
       "         [ 1,  5, 90,  ...,  5, 90, 85],\n",
       "         [ 1,  5, 90,  ..., 83,  5, 90],\n",
       "         [ 1,  5, 85,  ..., 88, 75,  5],\n",
       "         [ 1,  5, 85,  ..., 90, 85,  5]],\n",
       "\n",
       "        [[ 1, 88, 75,  ...,  5, 73, 71],\n",
       "         [ 1, 88, 75,  ..., 85, 88, 88],\n",
       "         [ 1, 88, 75,  ..., 75, 73, 90],\n",
       "         [ 1, 88, 75,  ..., 73, 71, 88],\n",
       "         [ 1, 88, 75,  ..., 73, 71, 88]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1,  5, 52,  ..., 71, 90, 79],\n",
       "         [ 1,  5, 52,  ..., 90, 88, 71],\n",
       "         [ 1,  5, 52,  ...,  5, 71, 90],\n",
       "         [ 1,  5, 52,  ...,  5, 79, 89],\n",
       "         [ 1,  5, 52,  ...,  5, 71, 90]],\n",
       "\n",
       "        [[ 1, 84,  5,  ..., 38, 38, 38],\n",
       "         [ 1, 84, 89,  ..., 38, 38, 38],\n",
       "         [ 1, 84,  5,  ..., 38, 38, 38],\n",
       "         [ 1, 84, 89,  ..., 75, 82, 95],\n",
       "         [ 1, 84,  5,  ...,  2, 75, 82]],\n",
       "\n",
       "        [[ 1, 73, 78,  ...,  5,  5,  5],\n",
       "         [ 1, 73, 78,  ...,  5,  5,  5],\n",
       "         [ 1, 73, 78,  ...,  5,  5,  5],\n",
       "         [ 1, 73, 78,  ...,  5,  5, 83],\n",
       "         [ 1, 73, 78,  ...,  5,  5,  5]]], device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def uniform(j, window_size):\n",
    "    return 1.0\n",
    "\n",
    "def triangle(j, window_size):\n",
    "    m = window_size//2\n",
    "    return m - 0.5 * abs(m - j)\n",
    "\n",
    "def bell(j, window_size):\n",
    "    m = window_size // 2\n",
    "    s = window_size // 2\n",
    "    return exp(-((m-j)/s)**2)\n",
    "\n",
    "def disjoint(\n",
    "    string,\n",
    "    model, \n",
    "    source_index,\n",
    "    target_index,\n",
    "    window_size = 50,\n",
    "    decoding_method = \"greedy_search\", \n",
    "    document_progress_bar = False, \n",
    "    document_batch_progress_bar = 0, \n",
    "    use_cuda = True,\n",
    "    *arcorrect\n",
    "):\n",
    "    model.eval()\n",
    "    windows = [string[i:i+window_size] \n",
    "        for i in range(0, len(string), window_size)]\n",
    "    windows = [\"\".join([source_index.vocabulary.lookup(c) for c in s])\\\n",
    "        .replace(\"<UNK>\", \" \") for s in windows]\n",
    "    if use_cuda:\n",
    "        X = source_index.text2tensor(windows, progress_bar = False).cuda()\n",
    "    else:\n",
    "        X = source_index.text2tensor(windows, progress_bar = False).cpu()\n",
    "    if decoding_method == \"greedy_search\":\n",
    "        predictions, probs = seq2seq.greedy_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )\n",
    "    elif decoding_method == \"beam_search\":\n",
    "        predictions, probs = seq2seq.beam_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )   \n",
    "        predictions = predictions[:, 0, :]\n",
    "    output = target_index.tensor2text(predictions)\n",
    "    output = [re.sub(r\"<START>|<PAD>|<UNK>|<END>.*\", \"\", s) for s in output]\n",
    "    return \"\".join(output)\n",
    "\n",
    "def n_grams(\n",
    "    string,\n",
    "    model,\n",
    "    source_index,\n",
    "    target_index,\n",
    "    window_size = 50, \n",
    "    decoding_method = \"greedy_search\", \n",
    "    weighting = \"uniform\",\n",
    "    document_progress_bar = False, \n",
    "    document_batch_progress_bar = 0,      \n",
    "    main_batch_size = 1024,\n",
    "    use_cuda=True,\n",
    "    *arcorrect\n",
    "):\n",
    "    model.eval()\n",
    "    if len(string) <= window_size:\n",
    "        windows = [string]\n",
    "    else:\n",
    "        windows = [string[i:i + window_size] \n",
    "            for i in range(len(string) - window_size + 1)]\n",
    "    windows = [\"\".join([source_index.vocabulary.lookup(c) for c in s])\\\n",
    "    .replace(\"<UNK>\", \" \") for s in windows]\n",
    "    if use_cuda:\n",
    "        X = source_index.text2tensor(windows, progress_bar = False).cuda()\n",
    "    else:\n",
    "        X = source_index.text2tensor(windows, progress_bar = False).cpu()\n",
    "    if decoding_method == \"greedy_search\":\n",
    "        predictions, probs = seq2seq.greedy_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = False, \n",
    "            *arcorrect\n",
    "        )\n",
    "    if decoding_method == \"beam_search\":\n",
    "        predictions, probs = seq2seq.beam_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )   \n",
    "        predictions = predictions[:, 0, :]   \n",
    "    output = target_index.tensor2text(predictions)\n",
    "    output = [re.sub(r\"<START>|<PAD>|<UNK>|<END>.*\", \"\", s) for s in output]\n",
    "    if weighting == \"uniform\":\n",
    "        weighting = uniform\n",
    "    elif weighting == \"triangle\":\n",
    "        weighting = triangle\n",
    "    elif weighting == \"bell\":\n",
    "        weighting = bell\n",
    "    votes = [\n",
    "        {k:0.0 for k in target_index.vocabulary} \n",
    "        for c in string\n",
    "    ]\n",
    "    for i, s in enumerate(output):\n",
    "        for j, (counter, char)\\\n",
    "        in enumerate(zip(votes[i:i + window_size], s)):\n",
    "            counter[char] += weighting(j, window_size)\n",
    "    output = [max(c.keys(), key = lambda x: c[x]) for c in votes]\n",
    "    output = \"\".join(output)\n",
    "    return votes, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_evaluation(\n",
    "    raw, \n",
    "    gs, \n",
    "    model, \n",
    "    source_index,\n",
    "    target_index,\n",
    "    save_path = None, \n",
    "    window_size = 40, \n",
    "    document_progress_bar = False, \n",
    "    use_cuda = True\n",
    "):\n",
    "    print(\"evaluating all methods...\")\n",
    "    metrics = []\n",
    "    old = levenshtein(reference = gs, hypothesis = raw).cer.mean()\n",
    "    # disjoint\n",
    "    print(\"  disjoint window...\")\n",
    "    print(\"    greedy_search...\")\n",
    "    #start = t()\n",
    "    start = time.time()\n",
    "    corrections = [\n",
    "        disjoint(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            document_progress_bar = document_progress_bar, \n",
    "            window_size = window_size,\n",
    "            use_cuda=use_cuda\n",
    "        ) for s in raw]\n",
    "    #print(\"hello\")\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"disjoint\", \n",
    "            \"decoding\":\"greedy\",\n",
    "            \"window_size\":window_size * 2,\n",
    "            \"weighting\":pd.NA,\n",
    "            #\"inference_seconds\":t() - start,\n",
    "            \"inference_seconds\":time.time() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "    #start = t()\n",
    "    start = time.time()\n",
    "    corrections = [\n",
    "        disjoint(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            document_progress_bar = document_progress_bar, \n",
    "            window_size = window_size * 2,\n",
    "            use_cuda=use_cuda\n",
    "        ) \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"disjoint\", \n",
    "            \"decoding\":\"greedy\",\n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":pd.NA,\n",
    "            #\"inference_seconds\":t() - start,\n",
    "            \"inference_seconds\":time.time() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "    print(\"    beam_search...\")\n",
    "    #start = t()\n",
    "    start = time.time()\n",
    "    corrections = [\n",
    "        disjoint(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            decoding_method = \"beam_search\", \n",
    "            document_progress_bar = document_progress_bar, \n",
    "            window_size = window_size,\n",
    "            use_cuda=use_cuda\n",
    "        ) \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"disjoint\", \n",
    "            \"decoding\":\"beam\", \n",
    "            \"window_size\":window_size * 2,\n",
    "            \"weighting\":pd.NA,\n",
    "            #\"inference_seconds\":t() - start,\n",
    "            \"inference_seconds\":time.time() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "    #start = t()\n",
    "    start = time.time()\n",
    "    corrections = [\n",
    "        disjoint(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            decoding_method = \"beam_search\", \n",
    "            document_progress_bar = document_progress_bar, \n",
    "            window_size = window_size * 2,\n",
    "            use_cuda=use_cuda\n",
    "        ) \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"disjoint\", \n",
    "            \"decoding\":\"beam\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":pd.NA,\n",
    "            #\"inference_seconds\":t() - start,\n",
    "            \"inference_seconds\":time.time() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "    # sliding\n",
    "    print(\"  sliding\")\n",
    "    print(\"    greedy...\")\n",
    "    ## greedy search\n",
    "    print(\"      uniform...\")\n",
    "    #start = t()\n",
    "    start = time.time()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            weighting = uniform,\n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size,\n",
    "            use_cuda=use_cuda\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"greedy\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"uniform\",\n",
    "            #\"inference_seconds\":t() - start,\n",
    "            \"inference_seconds\":time.time() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "\n",
    "    print(\"      triangle...\")\n",
    "    #start = t()\n",
    "    start = time.time()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            weighting = triangle, \n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size,\n",
    "            use_cuda=use_cuda\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"greedy\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"triangle\",\n",
    "            #\"inference_seconds\":t() - start,\n",
    "            \"inference_seconds\":time.time() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "\n",
    "    print(\"      bell...\")\n",
    "    #start = t()\n",
    "    start = time.time()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            weighting = bell, \n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size,\n",
    "            use_cuda=use_cuda\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"greedy\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"bell\",\n",
    "            #\"inference_seconds\":t() - start,\n",
    "            \"inference_seconds\":time.time() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "\n",
    "    ## beam search\n",
    "    print(\"    beam...\")\n",
    "    print(\"      uniform...\")\n",
    "    #start = t()\n",
    "    start = time.time()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            decoding_method = \"beam_search\", \n",
    "            weighting = uniform, \n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size,\n",
    "            use_cuda=use_cuda\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"beam\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"uniform\",\n",
    "            #\"inference_seconds\":t() - start,\n",
    "            \"inference_seconds\":time.time() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "\n",
    "    print(\"      triangle...\")\n",
    "    #start = t()\n",
    "    start = time.time()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            decoding_method = \"beam_search\", \n",
    "            weighting = triangle, \n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size,\n",
    "            use_cuda=use_cuda\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"beam\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"triangle\",\n",
    "            #\"inference_seconds\":t() - start,\n",
    "            \"inference_seconds\":time.time() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "\n",
    "    print(\"      bell...\")\n",
    "    #start = t()\n",
    "    start = time.time()\n",
    "    if use_cuda:\n",
    "        corrections = [\n",
    "            n_grams(\n",
    "                s, \n",
    "                model.cuda(), \n",
    "                source_index,\n",
    "                target_index,\n",
    "                decoding_method = \"beam_search\", \n",
    "                weighting = bell, \n",
    "                document_progress_bar = document_progress_bar,\n",
    "                window_size = window_size,\n",
    "                use_cuda=use_cuda\n",
    "            )[1] \n",
    "            for s in raw\n",
    "        ]\n",
    "    else:\n",
    "        corrections = [\n",
    "            n_grams(\n",
    "                s, \n",
    "                model.cpu(), \n",
    "                source_index,\n",
    "                target_index,\n",
    "                decoding_method = \"beam_search\", \n",
    "                weighting = bell, \n",
    "                document_progress_bar = document_progress_bar,\n",
    "                window_size = window_size,\n",
    "                use_cuda=use_cuda\n",
    "            )[1] \n",
    "            for s in raw\n",
    "        ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"beam\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"bell\",\n",
    "            #\"inference_seconds\":t() - start,\n",
    "            \"inference_seconds\":time.time() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "    print()\n",
    "    return pd.DataFrame(metrics).assign(\n",
    "        improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aligned sentences source</th>\n",
       "      <th>aligned sentences target</th>\n",
       "      <th>sentences source</th>\n",
       "      <th>sentences target</th>\n",
       "      <th>aligned sentences source types</th>\n",
       "      <th>aligned sentences target types</th>\n",
       "      <th>sentences source types</th>\n",
       "      <th>sentences target types</th>\n",
       "      <th>ocr_aligned</th>\n",
       "      <th>gs_aligned</th>\n",
       "      <th>ocr_to_input</th>\n",
       "      <th>words target</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thev suegested that optical {lis [rom the neu...</td>\n",
       "      <td>They suggested that optical flux from the neu...</td>\n",
       "      <td>Thev suegested that optical {lis [rom the neu...</td>\n",
       "      <td>They suggested that optical flux from the neu...</td>\n",
       "      <td>WWWW WWWWWWWWW WWWW WWWWWWW WWWW WWWW WWW WWW...</td>\n",
       "      <td>WWWW WWWWWWWWW WWWW WWWWWWW WWWW WWWW WWW WWW...</td>\n",
       "      <td>WWWW WWWWWWWWW WWWW WWWWWWW WWWW WWWW WWW WWW...</td>\n",
       "      <td>WWWW WWWWWWWWW WWWW WWWWWWW WWWW WWWW WWW WWW...</td>\n",
       "      <td>Thev suegested that optical {lis [rom the neu...</td>\n",
       "      <td>They suggested that optical flux from the neu...</td>\n",
       "      <td>Thev suegested that optical {lis [rom the neu...</td>\n",
       "      <td>They suggested that optical flux from the neu...</td>\n",
       "      <td>Thev suegested that optical {lis [rom the neu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The IRAS source would become another voung st...</td>\n",
       "      <td>The IRAS source would become another young st...</td>\n",
       "      <td>The IRAS source would become another voung st...</td>\n",
       "      <td>The IRAS source would become another young st...</td>\n",
       "      <td>WWW WWWW WWWWWW WWWWW WWWWWW WWWWWWW WWWWW WW...</td>\n",
       "      <td>WWW WWWW WWWWWW WWWWW WWWWWW WWWWWWW WWWWW WW...</td>\n",
       "      <td>WWW WWWW WWWWWW WWWWW WWWWWW WWWWWWW WWWWW WW...</td>\n",
       "      <td>WWW WWWW WWWWWW WWWWW WWWWWW WWWWWWW WWWWW WW...</td>\n",
       "      <td>The IRAS source would become another voung st...</td>\n",
       "      <td>The IRAS source would become another young st...</td>\n",
       "      <td>The IRAS source would become another voung st...</td>\n",
       "      <td>The IRAS source would become another young st...</td>\n",
       "      <td>The IRAS source would become another voung st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>^a^^^^N enuission frequency according to eq. ^...</td>\n",
       "      <td>maximum em@ission frequency according to eq. \\...</td>\n",
       "      <td>aN enuission frequency according to eq. (8)).</td>\n",
       "      <td>maximum emission frequency according to eq. \\r...</td>\n",
       "      <td>^W^^^^W WWWWWWWWW WWWWWWWWW WWWWWWWWW WW WWW ^...</td>\n",
       "      <td>WWWWWWW WW@WWWWWW WWWWWWWWW WWWWWWWWW WW WWW R...</td>\n",
       "      <td>WW WWWWWWWWW WWWWWWWWW WWWWWWWWW WW WWW RRWWW</td>\n",
       "      <td>WWWWWWW WWWWWWWW WWWWWWWWW WWWWWWWWW WW WWW RR...</td>\n",
       "      <td>@a@@@@N enuission frequency according to eq. )@).</td>\n",
       "      <td>maximum em@ission frequency according to eq. ) ),</td>\n",
       "      <td>aN enuission frequency according to eq. )).</td>\n",
       "      <td>maximum emission frequency according to eq. ) ),</td>\n",
       "      <td>aN enuission frequency according to eq. )).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Light curves for BATSE trigger 2193 are shown...</td>\n",
       "      <td>Light curves for BATSE trigger 2193 are shown...</td>\n",
       "      <td>Light curves for BATSE trigger 2193 are shown...</td>\n",
       "      <td>Light curves for BATSE trigger 2193 are shown...</td>\n",
       "      <td>WWWWW WWWWWW WWW WWWWW WWWWWWW WWWW WWW WWWWW...</td>\n",
       "      <td>WWWWW WWWWWW WWW WWWWW WWWWWWW WWWW WWW WWWWW...</td>\n",
       "      <td>WWWWW WWWWWW WWW WWWWW WWWWWWW WWWW WWW WWWWW...</td>\n",
       "      <td>WWWWW WWWWWW WWW WWWWW WWWWWWW WWWW WWW WWWWW...</td>\n",
       "      <td>Light curves for BATSE trigger 2193 are shown...</td>\n",
       "      <td>Light curves for BATSE trigger 2193 are shown...</td>\n",
       "      <td>Light curves for BATSE trigger 2193 are shown...</td>\n",
       "      <td>Light curves for BATSE trigger 2193 are shown...</td>\n",
       "      <td>Light curves for BATSE trigger 2193 are shown...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The same PSF is used for the off-source spect...</td>\n",
       "      <td>The same PSF is used for the off-source spect...</td>\n",
       "      <td>The same PSF is used for the off-source spect...</td>\n",
       "      <td>The same PSF is used for the off-source spect...</td>\n",
       "      <td>WWW WWWW WWW WW WWWW WWW WWW WWWWWWWWWW WWWWW...</td>\n",
       "      <td>WWW WWWW WWW WW WWWW WWW WWW WWWWWWWWWW WWWWW...</td>\n",
       "      <td>WWW WWWW WWW WW WWWW WWW WWW WWWWWWWWWW WWWWW...</td>\n",
       "      <td>WWW WWWW WWW WW WWWW WWW WWW WWWWWWWWWW WWWWW...</td>\n",
       "      <td>The same PSF is used for the off-source spect...</td>\n",
       "      <td>The same PSF is used for the off-source spect...</td>\n",
       "      <td>The same PSF is used for the off-source spect...</td>\n",
       "      <td>The same PSF is used for the off-source spect...</td>\n",
       "      <td>The same PSF is used for the off-source spect...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            aligned sentences source  \\\n",
       "0   Thev suegested that optical {lis [rom the neu...   \n",
       "1   The IRAS source would become another voung st...   \n",
       "2  ^a^^^^N enuission frequency according to eq. ^...   \n",
       "3   Light curves for BATSE trigger 2193 are shown...   \n",
       "4   The same PSF is used for the off-source spect...   \n",
       "\n",
       "                            aligned sentences target  \\\n",
       "0   They suggested that optical flux from the neu...   \n",
       "1   The IRAS source would become another young st...   \n",
       "2  maximum em@ission frequency according to eq. \\...   \n",
       "3   Light curves for BATSE trigger 2193 are shown...   \n",
       "4   The same PSF is used for the off-source spect...   \n",
       "\n",
       "                                    sentences source  \\\n",
       "0   Thev suegested that optical {lis [rom the neu...   \n",
       "1   The IRAS source would become another voung st...   \n",
       "2      aN enuission frequency according to eq. (8)).   \n",
       "3   Light curves for BATSE trigger 2193 are shown...   \n",
       "4   The same PSF is used for the off-source spect...   \n",
       "\n",
       "                                    sentences target  \\\n",
       "0   They suggested that optical flux from the neu...   \n",
       "1   The IRAS source would become another young st...   \n",
       "2  maximum emission frequency according to eq. \\r...   \n",
       "3   Light curves for BATSE trigger 2193 are shown...   \n",
       "4   The same PSF is used for the off-source spect...   \n",
       "\n",
       "                      aligned sentences source types  \\\n",
       "0   WWWW WWWWWWWWW WWWW WWWWWWW WWWW WWWW WWW WWW...   \n",
       "1   WWW WWWW WWWWWW WWWWW WWWWWW WWWWWWW WWWWW WW...   \n",
       "2  ^W^^^^W WWWWWWWWW WWWWWWWWW WWWWWWWWW WW WWW ^...   \n",
       "3   WWWWW WWWWWW WWW WWWWW WWWWWWW WWWW WWW WWWWW...   \n",
       "4   WWW WWWW WWW WW WWWW WWW WWW WWWWWWWWWW WWWWW...   \n",
       "\n",
       "                      aligned sentences target types  \\\n",
       "0   WWWW WWWWWWWWW WWWW WWWWWWW WWWW WWWW WWW WWW...   \n",
       "1   WWW WWWW WWWWWW WWWWW WWWWWW WWWWWWW WWWWW WW...   \n",
       "2  WWWWWWW WW@WWWWWW WWWWWWWWW WWWWWWWWW WW WWW R...   \n",
       "3   WWWWW WWWWWW WWW WWWWW WWWWWWW WWWW WWW WWWWW...   \n",
       "4   WWW WWWW WWW WW WWWW WWW WWW WWWWWWWWWW WWWWW...   \n",
       "\n",
       "                              sentences source types  \\\n",
       "0   WWWW WWWWWWWWW WWWW WWWWWWW WWWW WWWW WWW WWW...   \n",
       "1   WWW WWWW WWWWWW WWWWW WWWWWW WWWWWWW WWWWW WW...   \n",
       "2      WW WWWWWWWWW WWWWWWWWW WWWWWWWWW WW WWW RRWWW   \n",
       "3   WWWWW WWWWWW WWW WWWWW WWWWWWW WWWW WWW WWWWW...   \n",
       "4   WWW WWWW WWW WW WWWW WWW WWW WWWWWWWWWW WWWWW...   \n",
       "\n",
       "                              sentences target types  \\\n",
       "0   WWWW WWWWWWWWW WWWW WWWWWWW WWWW WWWW WWW WWW...   \n",
       "1   WWW WWWW WWWWWW WWWWW WWWWWW WWWWWWW WWWWW WW...   \n",
       "2  WWWWWWW WWWWWWWW WWWWWWWWW WWWWWWWWW WW WWW RR...   \n",
       "3   WWWWW WWWWWW WWW WWWWW WWWWWWW WWWW WWW WWWWW...   \n",
       "4   WWW WWWW WWW WW WWWW WWW WWW WWWWWWWWWW WWWWW...   \n",
       "\n",
       "                                         ocr_aligned  \\\n",
       "0   Thev suegested that optical {lis [rom the neu...   \n",
       "1   The IRAS source would become another voung st...   \n",
       "2  @a@@@@N enuission frequency according to eq. )@).   \n",
       "3   Light curves for BATSE trigger 2193 are shown...   \n",
       "4   The same PSF is used for the off-source spect...   \n",
       "\n",
       "                                          gs_aligned  \\\n",
       "0   They suggested that optical flux from the neu...   \n",
       "1   The IRAS source would become another young st...   \n",
       "2  maximum em@ission frequency according to eq. ) ),   \n",
       "3   Light curves for BATSE trigger 2193 are shown...   \n",
       "4   The same PSF is used for the off-source spect...   \n",
       "\n",
       "                                        ocr_to_input  \\\n",
       "0   Thev suegested that optical {lis [rom the neu...   \n",
       "1   The IRAS source would become another voung st...   \n",
       "2        aN enuission frequency according to eq. )).   \n",
       "3   Light curves for BATSE trigger 2193 are shown...   \n",
       "4   The same PSF is used for the off-source spect...   \n",
       "\n",
       "                                        words target  \\\n",
       "0   They suggested that optical flux from the neu...   \n",
       "1   The IRAS source would become another young st...   \n",
       "2   maximum emission frequency according to eq. ) ),   \n",
       "3   Light curves for BATSE trigger 2193 are shown...   \n",
       "4   The same PSF is used for the off-source spect...   \n",
       "\n",
       "                                              source  \n",
       "0   Thev suegested that optical {lis [rom the neu...  \n",
       "1   The IRAS source would become another voung st...  \n",
       "2        aN enuission frequency according to eq. )).  \n",
       "3   Light curves for BATSE trigger 2193 are shown...  \n",
       "4   The same PSF is used for the off-source spect...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "#source_index\n",
    "# Language Window  Window  Decoding  Weighting  Inference  μ CER  μ CER  % Improvement  % Baseline\n",
    "# en       n-grams  20      beam      uniform    10.37     19.47  18.00     7.52          11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ocr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    beam...\n",
      "      uniform...\n",
      "on 0 of 5000\n",
      "on 100 of 5000\n"
     ]
    }
   ],
   "source": [
    "do_full_eval = False\n",
    "\n",
    "window_type = 'n-grams'\n",
    "window_size = 20\n",
    "decoding = 'beam'\n",
    "weighting = 'uniform'\n",
    "ntest = 100\n",
    "\n",
    "if do_full_eval:\n",
    "    print(\"\\nevaluating all correction methods...\")\n",
    "    evaluation = full_evaluation(\n",
    "        ocr_list,\n",
    "        gs_list,\n",
    "        model,\n",
    "        source_index,\n",
    "        target_index,\n",
    "        use_cuda=use_cuda,\n",
    "        window_size=40\n",
    "    )\n",
    "\n",
    "    print(\"\\n--results--\")\n",
    "    # print(\"test data:\", test)\n",
    "    # print(\"plain beam search:\", just_beam)\n",
    "    # print(\"disjoint\")\n",
    "    # print(\"  greedy search:\", disjoint_greedy)\n",
    "    # print(\"  beam search:\", disjoint_beam)\n",
    "    # print(\"n_grams\")\n",
    "    # print(\"  greedy search:\", n_grams_greedy)\n",
    "    # print(\"  beam search:\", n_grams_beam)\n",
    "\n",
    "    #print(\"\\n\", evaluation)\n",
    "    evaluation.to_csv(output_folder + 'eval/evaluation_ntest'+str(ntest)+'new.csv',index=False)\n",
    "    evaluation = pd.read_csv(output_folder + 'eval/evaluation_ntest'+str(ntest)+'new.csv')\n",
    "    evaluation\n",
    "else:\n",
    "    ## start\n",
    "#     raw, \n",
    "#     gs, \n",
    "#     model, \n",
    "#     source_index,\n",
    "#     target_index,\n",
    "#     save_path = None, \n",
    "#     window_size = 40, \n",
    "#     document_progress_bar = False, \n",
    "#     use_cuda = True\n",
    "    if (window_type == 'n-grams') and (decoding == 'beam') and (weighting == 'uniform'):\n",
    "        ## beam search\n",
    "        print(\"    beam...\")\n",
    "        print(\"      uniform...\")\n",
    "        corrections = []\n",
    "        start = time.time()\n",
    "        for iis,s in enumerate(ocr_list):\n",
    "            if iis % 100 == 0:\n",
    "                print('on', iis, 'of', len(ocr_list))\n",
    "            c = n_grams(\n",
    "                s,\n",
    "                model,\n",
    "                source_index,\n",
    "                target_index,\n",
    "                decoding_method = \"beam_search\", \n",
    "                weighting = uniform, \n",
    "                document_progress_bar = False,\n",
    "                window_size = window_size,\n",
    "                use_cuda=use_cuda\n",
    "            )[1] \n",
    "            corrections.append(c)\n",
    "\n",
    "#         corrections = [\n",
    "#             n_grams(\n",
    "#                 s,\n",
    "#                 model,\n",
    "#                 source_index,\n",
    "#                 target_index,\n",
    "#                 decoding_method = \"beam_search\", \n",
    "#                 weighting = uniform, \n",
    "#                 document_progress_bar = False,\n",
    "#                 window_size = window_size,\n",
    "#                 use_cuda=use_cuda\n",
    "#             )[1] \n",
    "#             for s in ocr_list\n",
    "#         ]\n",
    "    with open(eval_save_dir + 'evalsavecorrs.pickle', 'wb') as f:\n",
    "        pickle.dump(corrections,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corrections' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_943674/18360255.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorrections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'corrections' is not defined"
     ]
    }
   ],
   "source": [
    "corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation.to_csv(output_folder + 'eval/evaluation_ntest'+str(ntest)+'new.csv',index=False)\n",
    "# evaluation = pd.read_csv(output_folder + 'eval/evaluation_ntest'+str(ntest)+'new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_list #take out @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.to_csv(output_folder + \"eval/evaluation60new.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_output = []\n",
    "sentence = \"\"\n",
    "for chunks in output:\n",
    "    #if chunks[0] == 'stars cataclysmic  ':#everytime it finds stars cataclysmic it outputs a new sentence #theres only 19?\n",
    "    if chunks[0] == '<START>advice charm touch  ':#everytime it finds advice charm touch   it outputs a new sentence\n",
    "        new_output.append(sentence)\n",
    "        sentence = \"\"# this empties the sentence variable whenever a sentence is added to the array\n",
    "    else:\n",
    "       new_sentence = chunks[0].replace(\"<START>\", \"\")\n",
    "       new_sentence = new_sentence.replace(\"<END>\", \"\")\n",
    "       sentence = sentence + new_sentence #combining the chunks of 20 characters together without the start and end\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "unnamed = list(range(1,49))\n",
    "#print(unnamed)\n",
    "test_data['new_output'] = new_output\n",
    "test_data['unnamed'] = unnamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data\n",
    "#test_data.to_csv('./github_output_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pybind11\n",
    "# !pip install fastwer\n",
    "#!pip install pytesseract\n",
    "#!sudo apt install tesseract-ocr\n",
    "\n",
    "import cv2\n",
    "#import pytesseract\n",
    "import fastwer\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test_data.iterrows():\n",
    "  filename = row['unnamed']\n",
    "  ref = row['gs_aligned']\n",
    "  ocr = row['ocr_aligned']\n",
    "  output = row['new_output']\n",
    "  cer = fastwer.score_sent(ocr, ref, char_level=True)\n",
    "  wer = fastwer.score_sent(ocr, ref, char_level=False)\n",
    "  test_data.loc[test_data['unnamed'] == filename, 'before_cer'] = round(cer,2) # Round value to 2 decimal places\n",
    "  test_data.loc[test_data['unnamed'] == filename, 'before_wer'] = round(wer,2)\n",
    "\n",
    "#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv(output_dir + 'eval/cer_wer_new50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test_data.iterrows():\n",
    "  filename = row['unnamed']\n",
    "  ref = row['gs_aligned']\n",
    "  ocr = row['ocr_aligned']\n",
    "  output = row['new_output']\n",
    "  cer = fastwer.score_sent(output, ref, char_level=True)\n",
    "  wer = fastwer.score_sent(output, ref, char_level=False)\n",
    "  test_data.loc[test_data['unnamed'] == filename, 'after_cer'] = round(cer,2) # Round value to 2 decimal places\n",
    "  test_data.loc[test_data['unnamed'] == filename, 'after_wer'] = round(wer,2)\n",
    "\n",
    "#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('./cer_wer_new50epochs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "new_output = tokenize.sent_tokenize(sentence)\n",
    "#new_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs =test_data['gs_aligned']\n",
    "gs_list = ' '.join(test_data['gs_aligned'].tolist())\n",
    "#gs_array = gs.to_numpy()\n",
    "\n",
    "#print(gs_array)\n",
    "new_gs = tokenize.sent_tokenize(gs_list) # trying to take gs column to match the length of the new_output.\n",
    "len(new_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTest():   \n",
    "    input_file = './post_ocr_correction/data/en/data/test'\n",
    "    \n",
    "    # reverse is true, therefore pair[0] is ocr and pair[1] is gs\n",
    "    input_lang, output_lang, pairs = prepareData(input_file,'gs', 'ocr', True)\n",
    "    \n",
    "    output_sentences = []\n",
    "    input_sentences = []\n",
    "    corrected_sentences = []\n",
    "    for pair in pairs:\n",
    "        sentence = pair[0]  #ocr sentence\n",
    "        try:\n",
    "            output_words, attentions = evaluate(encoder1, attn_decoder1, sentence)\n",
    "            input_sentences.append(pair[1])  #gs sentence\n",
    "            output_sentences.append(pair[0]) #ocr sentence\n",
    "            corrected_sentences.append(' '.join(output_words))\n",
    " \n",
    "            #testing_df.append({'output':''.join(output_words),'input':x},ignore_index=True)\n",
    "            #showAttention(x, output_words, attentions)\n",
    "        except KeyError:\n",
    "            print('KeyError =', sentence)\n",
    "    testing_df = pd.DataFrame({\"gs\":input_sentences, \"ocr\":output_sentences, \"output\":corrected_sentences})\n",
    "    return testing_df\n",
    "testing_dataframe1 = readTest()\n",
    "testing_dataframe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_df1 = pd.DataFrame({\"output\":new_output})\n",
    "output_df1.to_csv('./output_dataframe_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTest():   \n",
    "    input_file = './post_ocr_correction/data/en/data/test'\n",
    "    \n",
    "    # reverse is true, therefore pair[0] is ocr and pair[1] is gs\n",
    "    input_lang, output_lang, pairs = prepareData(input_file,'gs', 'ocr', True)\n",
    "    \n",
    "    output_sentences = []\n",
    "    input_sentences = []\n",
    "    corrected_sentences = []\n",
    "    for pair in pairs:\n",
    "        sentence = pair[0]  #ocr sentence\n",
    "        try:\n",
    "            output_words, attentions = evaluate(encoder1, attn_decoder1, sentence)\n",
    "            input_sentences.append(pair[1])  #gs sentence\n",
    "            output_sentences.append(pair[0]) #ocr sentence\n",
    "            corrected_sentences.append(' '.join(output_words))\n",
    " \n",
    "            #testing_df.append({'output':''.join(output_words),'input':x},ignore_index=True)\n",
    "            #showAttention(x, output_words, attentions)\n",
    "        except KeyError:\n",
    "            print('KeyError =', sentence)\n",
    "    testing_df = pd.DataFrame({\"gs\":input_sentences, \"ocr\":output_sentences, \"output\":corrected_sentences})\n",
    "    return testing_df\n",
    "testing_dataframe1 = readTest()\n",
    "testing_dataframe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# #dictionary = {'GS': \"\", 'OCR': \"\", 'index'=[0]} \n",
    "# #training_df = pd.DataFrame(dictionary)\n",
    "\n",
    "# ocr =[]\n",
    "# ocr_aligned=[]\n",
    "# gs_aligned=[]\n",
    "\n",
    "# for x in range(1008,1120):\n",
    "#     for y in range(len(PDF_OUT_SENT[x])):\n",
    "#         for z in range(len(PDF_OUT_SENT[x][y])):\n",
    "#             OCR = OCR_OUT_SENT[x][y][z]\n",
    "#             OCR_aligned_train = OCR_aligned_SENT[x][y][z]\n",
    "#             GS_aligned_train = PDF_OUT_SENT[x][y][z]\n",
    "#             #dictionary = {'GS': GS_train, 'OCR': OCR_train} \n",
    "#             #training_df = pd.DataFrame(dictionary)\n",
    "#             #gs_train = training_df.append(OCR_OUT_SENT)\n",
    "#             ocr.append(OCR)\n",
    "#             ocr_aligned.append(OCR_aligned_train)\n",
    "#             gs_aligned.append(GS_aligned_train)\n",
    "#             #training_df = pd.concat([training_df, pd.DataFrame(dictionary)], ignore_index = True)\n",
    "# data_ocr = pd.DataFrame({\"ocr_to_input\":ocr}) #\"ocr_aligned\":ocr_aligned, \"gs_aligned\":gs_aligned})\n",
    "# print(data_ocr.shape)\n",
    "# data_ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, probs = model.predict(dev_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tensor2text(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tensor2text(train_target[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tensor2text(train_source[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-OCRPC]",
   "language": "python",
   "name": "conda-env-.conda-OCRPC-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
