{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where is data?\n",
    "#output_folder = '/home/jnaiman/data/morgan/' # HAL\n",
    "output_folder = '/Users/jnaiman/Downloads/tmp/ocrpost/data/morgan/' # local\n",
    "\n",
    "ender = '_small_words' # small has 100,000 for training, 5000 for dev\n",
    "\n",
    "# model save dir\n",
    "#model_save_dir = '/home/jnaiman/data/morgan/models/'\n",
    "model_save_dir = '/Users/jnaiman/Downloads/tmp/ocrpost/data/morgan/models/' # local\n",
    "model_file = 'new_torch_file_new_pages_small_words.pt'\n",
    "\n",
    "file_dir = '/Users/jnaiman/Dropbox/wwt_image_extraction/OCRPostCorrection/alignments/'\n",
    "test_file = 'test_masked_n5000_20230510.csv'\n",
    "\n",
    "\n",
    "# its not 100% clear if we need this... setting a flag, but looks like we DO need it for memory issues\n",
    "use_train_dev_size = True\n",
    "train_size = 1000000\n",
    "dev_size = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char2i = pickle.load(open(output_folder + \"data/char2i_new_pages.pkl\", \"rb\"))\n",
    "# i2char = pickle.load(open(output_folder + \"data/i2char_new_pages.pkl\", \"rb\"))\n",
    "char2i = pickle.load(open(output_folder + \"data/char2i_new_pages\"+ender+\".pkl\", \"rb\"))\n",
    "i2char = pickle.load(open(output_folder + \"data/i2char_new_pages\"+ender+\".pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #source = [list(\"abcdefghijkl\"), list(\"mnopqrstwxyz\")]\n",
    "# #target = [list(\"ABCDEFGHIJKL\"), list(\"MNOPQRSTWXYZ\")] #update to astronomy vocab\n",
    "# source = []#yy\n",
    "# for s in model.source_index:\n",
    "#     sentence = \"\"\n",
    "#     for c in s:\n",
    "#         #nu = source_index[c]\n",
    "#         #sentence.append(c)\n",
    "#         #sentence.append(num)\n",
    "#         sentence += c\n",
    "#     source.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from nltk.lm import Vocabulary\n",
    "import pickle\n",
    "import sys\n",
    "import torch\n",
    "import importlib\n",
    "from timeit import default_timer as t\n",
    "sys.path.append(\"../\")\n",
    "#from metrics import levenshtein\n",
    "import post_ocr_correction\n",
    "from pytorch_beam_search import seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#!pip install Levenshtein\n",
    "from Levenshtein import distance\n",
    "import Levenshtein \n",
    "\n",
    "import re\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append('../scienceDigitization/text_mining_ocr_and_pdf/ocr_spell_check/')\n",
    "from importlib import reload\n",
    "import utils\n",
    "reload(utils)\n",
    "from utils import align_texts_fast, get_fill_in_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Seq2Seq Transformer\n",
      "Source index: {'\\t': 3, '\\n': 4, ' ': 5, '!': 6, '\"': 7, '#': 8, '$': 9, '%': 10, '&': 11, \"'\": 12, '(': 13, ')': 14, '*': 15, '+': 16, ',': 17, '-': 18, '.': 19, '/': 20, '0': 21, '1': 22, '2': 23, '3': 24, '4': 25, '5': 26, '6': 27, '7': 28, '8': 29, '9': 30, ':': 31, ';': 32, '<': 33, '<UNK>': 34, '=': 35, '>': 36, '?': 37, '@': 38, 'A': 39, 'B': 40, 'C': 41, 'D': 42, 'E': 43, 'F': 44, 'G': 45, 'H': 46, 'I': 47, 'J': 48, 'K': 49, 'L': 50, 'M': 51, 'N': 52, 'O': 53, 'P': 54, 'Q': 55, 'R': 56, 'S': 57, 'T': 58, 'U': 59, 'V': 60, 'W': 61, 'X': 62, 'Y': 63, 'Z': 64, '[': 65, '\\\\': 66, ']': 67, '_': 68, '`': 69, 'a': 70, 'b': 71, 'c': 72, 'd': 73, 'e': 74, 'f': 75, 'g': 76, 'h': 77, 'i': 78, 'j': 79, 'k': 80, 'l': 81, 'm': 82, 'n': 83, 'o': 84, 'p': 85, 'q': 86, 'r': 87, 's': 88, 't': 89, 'u': 90, 'v': 91, 'w': 92, 'x': 93, 'y': 94, 'z': 95, '{': 96, '|': 97, '}': 98, '~': 99, '\\xa0': 100, '¡': 101, '¢': 102, '£': 103, '¥': 104, '§': 105, '©': 106, 'ª': 107, '«': 108, '\\xad': 109, '®': 110, '¯': 111, '°': 112, '±': 113, '³': 114, '´': 115, 'µ': 116, '¶': 117, '»': 118, '¼': 119, '½': 120, '¿': 121, 'À': 122, 'Á': 123, 'Â': 124, 'Ã': 125, 'Å': 126, 'É': 127, 'Ê': 128, 'Í': 129, 'Ð': 130, 'Ò': 131, 'Ó': 132, 'Õ': 133, 'Ö': 134, 'Ü': 135, 'Þ': 136, 'à': 137, 'á': 138, 'â': 139, 'ä': 140, 'ç': 141, 'è': 142, 'é': 143, 'ê': 144, 'ë': 145, 'í': 146, 'î': 147, 'ï': 148, 'ò': 149, 'ó': 150, 'ô': 151, 'ö': 152, 'ø': 153, 'ú': 154, 'ü': 155, 'ý': 156, 'ć': 157, 'Č': 158, 'č': 159, 'İ': 160, 'ł': 161, 'ń': 162, 'ő': 163, 'ř': 164, 'ş': 165, 'š': 166, 'ź': 167, 'Ż': 168, 'ż': 169, 'ž': 170, '̆': 171, '̇': 172, '΄': 173, 'Ά': 174, 'Έ': 175, 'Ί': 176, 'Ό': 177, 'Ώ': 178, 'Α': 179, 'Β': 180, 'Γ': 181, 'Δ': 182, 'Ε': 183, 'Ζ': 184, 'Η': 185, 'Θ': 186, 'Ι': 187, 'Κ': 188, 'Λ': 189, 'Μ': 190, 'Ν': 191, 'Ξ': 192, 'Ο': 193, 'Π': 194, 'Ρ': 195, 'Σ': 196, 'Τ': 197, 'Υ': 198, 'Φ': 199, 'Χ': 200, 'Ω': 201, 'ά': 202, 'έ': 203, 'ή': 204, 'ί': 205, 'α': 206, 'β': 207, 'γ': 208, 'δ': 209, 'ε': 210, 'ζ': 211, 'η': 212, 'θ': 213, 'ι': 214, 'κ': 215, 'λ': 216, 'μ': 217, 'ν': 218, 'ξ': 219, 'ο': 220, 'π': 221, 'ρ': 222, 'ς': 223, 'σ': 224, 'τ': 225, 'υ': 226, 'φ': 227, 'χ': 228, 'ψ': 229, 'ω': 230, 'ϊ': 231, 'ό': 232, 'ύ': 233, 'ώ': 234, 'ϐ': 235, 'ϱ': 236, 'ἀ': 237, 'ἁ': 238, 'ἂ': 239, 'ἃ': 240, 'ἄ': 241, 'ἆ': 242, 'Ἀ': 243, 'Ἅ': 244, 'ἐ': 245, 'ἓ': 246, 'Ἐ': 247, 'ἣ': 248, 'Ἠ': 249, 'Ἡ': 250, 'ἰ': 251, 'ἱ': 252, 'ἲ': 253, 'ἳ': 254, 'ἴ': 255, 'ἵ': 256, 'ἶ': 257, 'ἷ': 258, 'Ἰ': 259, 'Ἱ': 260, 'Ἴ': 261, 'ὁ': 262, 'ὃ': 263, 'ὅ': 264, 'Ὀ': 265, 'Ὁ': 266, 'Ὃ': 267, 'Ὅ': 268, 'ὐ': 269, 'ὑ': 270, 'ὓ': 271, 'ὖ': 272, 'ὠ': 273, 'Ὦ': 274, 'ὰ': 275, 'ὲ': 276, 'ὴ': 277, 'ὶ': 278, 'ὸ': 279, 'ᾱ': 280, 'ᾳ': 281, '᾿': 282, 'ῃ': 283, 'ῇ': 284, 'ῖ': 285, 'ῥ': 286, 'ῦ': 287, 'Ῥ': 288, '–': 289, '—': 290, '‘': 291, '’': 292, '“': 293, '”': 294, '€': 295, '™': 296, '↑': 297, '→': 298, '↓': 299, '↔': 300, '↕': 301, '↖': 302, '↗': 303, '↘': 304, '↙': 305, '↛': 306, '↜': 307, '↝': 308, '↠': 309, '↡': 310, '↢': 311, '↣': 312, '↤': 313, '↥': 314, '↦': 315, '↧': 316, '↨': 317, '↩': 318, '↪': 319, '↫': 320, '↭': 321, '↰': 322, '↱': 323, '↲': 324, '↳': 325, '↴': 326, '↵': 327, '↶': 328, '↷': 329, '↸': 330, '↺': 331, '↻': 332, '↼': 333, '↽': 334, '↾': 335, '↿': 336, '⇀': 337, '⇁': 338, '⇂': 339, '⇃': 340, '⇄': 341, '⇆': 342, '⇇': 343, '⇈': 344, '⇉': 345, '⇍': 346, '⇖': 347, '⇜': 348, '⇠': 349, '⇡': 350, '⇥': 351, '⇪': 352, '∀': 353, '∁': 354, '∂': 355, '∃': 356, '∆': 357, '∇': 358, '∉': 359, '∊': 360, '∋': 361, '∍': 362, '∎': 363, '∏': 364, '∐': 365, '∑': 366, '−': 367, '∓': 368, '∔': 369, '∕': 370, '∖': 371, '∙': 372, '∞': 373, '∟': 374, '∠': 375, '∡': 376, '∢': 377, '∣': 378, '∤': 379, '∥': 380, '∩': 381, '∪': 382, '∫': 383, '∱': 384, '∶': 385, '∷': 386, '∸': 387, '∺': 388, '∼': 389, '∾': 390, '∿': 391, '≀': 392, '≃': 393, '≈': 394, '≊': 395, '≋': 396, '≏': 397, '≒': 398, '≓': 399, '≖': 400, '≙': 401, '≚': 402, '≜': 403, '≝': 404, '≞': 405, '≟': 406, '≡': 407, '≣': 408, '≤': 409, '≥': 410, '≦': 411, '≧': 412, '≨': 413, '≩': 414, '≪': 415, '≯': 416, '≱': 417, '≲': 418, '≳': 419, '≴': 420, '≵': 421, '≶': 422, '≸': 423, '≹': 424, '≺': 425, '≻': 426, '≼': 427, '≽': 428, '≿': 429, '⊀': 430, '⊂': 431, '⊃': 432, '⊆': 433, '⊇': 434, '⊊': 435, '⊋': 436, '⊍': 437, '⊏': 438, '⊐': 439, '⊑': 440, '⊒': 441, '⊓': 442, '⊔': 443, '⊖': 444, '⊙': 445, '⊜': 446, '⊟': 447, '⊡': 448, '⊢': 449, '⊣': 450, '⊤': 451, '⊥': 452, '⊨': 453, '⊪': 454, '⊰': 455, '⊱': 456, '⊲': 457, '⊳': 458, '⊴': 459, '⊸': 460, '⊹': 461, '⊺': 462, '⊻': 463, '⊼': 464, '⊽': 465, '⊾': 466, '⋀': 467, '⋁': 468, '⋂': 469, '⋃': 470, '⋅': 471, '⋈': 472, '⋉': 473, '⋊': 474, '⋋': 475, '⋍': 476, '⋎': 477, '⋏': 478, '⋔': 479, '⋖': 480, '⋗': 481, '⋚': 482, '⋜': 483, '⋝': 484, '⋞': 485, '⋟': 486, '⋠': 487, '⋡': 488, '⋣': 489, '⋤': 490, '⋥': 491, '⋦': 492, '⋨': 493, '⋪': 494, '⋮': 495, '⋯': 496, '⋰': 497, '⋱': 498, '<PAD>': 0, '<START>': 1, '<END>': 2}\n",
      "Target index: {3: '\\t', 4: '\\n', 5: ' ', 6: '!', 7: '\"', 8: '#', 9: '$', 10: '%', 11: '&', 12: \"'\", 13: '(', 14: ')', 15: '*', 16: '+', 17: ',', 18: '-', 19: '.', 20: '/', 21: '0', 22: '1', 23: '2', 24: '3', 25: '4', 26: '5', 27: '6', 28: '7', 29: '8', 30: '9', 31: ':', 32: ';', 33: '<', 34: '<UNK>', 35: '=', 36: '>', 37: '?', 38: '@', 39: 'A', 40: 'B', 41: 'C', 42: 'D', 43: 'E', 44: 'F', 45: 'G', 46: 'H', 47: 'I', 48: 'J', 49: 'K', 50: 'L', 51: 'M', 52: 'N', 53: 'O', 54: 'P', 55: 'Q', 56: 'R', 57: 'S', 58: 'T', 59: 'U', 60: 'V', 61: 'W', 62: 'X', 63: 'Y', 64: 'Z', 65: '[', 66: '\\\\', 67: ']', 68: '_', 69: '`', 70: 'a', 71: 'b', 72: 'c', 73: 'd', 74: 'e', 75: 'f', 76: 'g', 77: 'h', 78: 'i', 79: 'j', 80: 'k', 81: 'l', 82: 'm', 83: 'n', 84: 'o', 85: 'p', 86: 'q', 87: 'r', 88: 's', 89: 't', 90: 'u', 91: 'v', 92: 'w', 93: 'x', 94: 'y', 95: 'z', 96: '{', 97: '|', 98: '}', 99: '~', 100: '\\xa0', 101: '¡', 102: '¢', 103: '£', 104: '¥', 105: '§', 106: '©', 107: 'ª', 108: '«', 109: '\\xad', 110: '®', 111: '¯', 112: '°', 113: '±', 114: '³', 115: '´', 116: 'µ', 117: '¶', 118: '»', 119: '¼', 120: '½', 121: '¿', 122: 'À', 123: 'Á', 124: 'Â', 125: 'Ã', 126: 'Å', 127: 'É', 128: 'Ê', 129: 'Í', 130: 'Ð', 131: 'Ò', 132: 'Ó', 133: 'Õ', 134: 'Ö', 135: 'Ü', 136: 'Þ', 137: 'à', 138: 'á', 139: 'â', 140: 'ä', 141: 'ç', 142: 'è', 143: 'é', 144: 'ê', 145: 'ë', 146: 'í', 147: 'î', 148: 'ï', 149: 'ò', 150: 'ó', 151: 'ô', 152: 'ö', 153: 'ø', 154: 'ú', 155: 'ü', 156: 'ý', 157: 'ć', 158: 'Č', 159: 'č', 160: 'İ', 161: 'ł', 162: 'ń', 163: 'ő', 164: 'ř', 165: 'ş', 166: 'š', 167: 'ź', 168: 'Ż', 169: 'ż', 170: 'ž', 171: '̆', 172: '̇', 173: '΄', 174: 'Ά', 175: 'Έ', 176: 'Ί', 177: 'Ό', 178: 'Ώ', 179: 'Α', 180: 'Β', 181: 'Γ', 182: 'Δ', 183: 'Ε', 184: 'Ζ', 185: 'Η', 186: 'Θ', 187: 'Ι', 188: 'Κ', 189: 'Λ', 190: 'Μ', 191: 'Ν', 192: 'Ξ', 193: 'Ο', 194: 'Π', 195: 'Ρ', 196: 'Σ', 197: 'Τ', 198: 'Υ', 199: 'Φ', 200: 'Χ', 201: 'Ω', 202: 'ά', 203: 'έ', 204: 'ή', 205: 'ί', 206: 'α', 207: 'β', 208: 'γ', 209: 'δ', 210: 'ε', 211: 'ζ', 212: 'η', 213: 'θ', 214: 'ι', 215: 'κ', 216: 'λ', 217: 'μ', 218: 'ν', 219: 'ξ', 220: 'ο', 221: 'π', 222: 'ρ', 223: 'ς', 224: 'σ', 225: 'τ', 226: 'υ', 227: 'φ', 228: 'χ', 229: 'ψ', 230: 'ω', 231: 'ϊ', 232: 'ό', 233: 'ύ', 234: 'ώ', 235: 'ϐ', 236: 'ϱ', 237: 'ἀ', 238: 'ἁ', 239: 'ἂ', 240: 'ἃ', 241: 'ἄ', 242: 'ἆ', 243: 'Ἀ', 244: 'Ἅ', 245: 'ἐ', 246: 'ἓ', 247: 'Ἐ', 248: 'ἣ', 249: 'Ἠ', 250: 'Ἡ', 251: 'ἰ', 252: 'ἱ', 253: 'ἲ', 254: 'ἳ', 255: 'ἴ', 256: 'ἵ', 257: 'ἶ', 258: 'ἷ', 259: 'Ἰ', 260: 'Ἱ', 261: 'Ἴ', 262: 'ὁ', 263: 'ὃ', 264: 'ὅ', 265: 'Ὀ', 266: 'Ὁ', 267: 'Ὃ', 268: 'Ὅ', 269: 'ὐ', 270: 'ὑ', 271: 'ὓ', 272: 'ὖ', 273: 'ὠ', 274: 'Ὦ', 275: 'ὰ', 276: 'ὲ', 277: 'ὴ', 278: 'ὶ', 279: 'ὸ', 280: 'ᾱ', 281: 'ᾳ', 282: '᾿', 283: 'ῃ', 284: 'ῇ', 285: 'ῖ', 286: 'ῥ', 287: 'ῦ', 288: 'Ῥ', 289: '–', 290: '—', 291: '‘', 292: '’', 293: '“', 294: '”', 295: '€', 296: '™', 297: '↑', 298: '→', 299: '↓', 300: '↔', 301: '↕', 302: '↖', 303: '↗', 304: '↘', 305: '↙', 306: '↛', 307: '↜', 308: '↝', 309: '↠', 310: '↡', 311: '↢', 312: '↣', 313: '↤', 314: '↥', 315: '↦', 316: '↧', 317: '↨', 318: '↩', 319: '↪', 320: '↫', 321: '↭', 322: '↰', 323: '↱', 324: '↲', 325: '↳', 326: '↴', 327: '↵', 328: '↶', 329: '↷', 330: '↸', 331: '↺', 332: '↻', 333: '↼', 334: '↽', 335: '↾', 336: '↿', 337: '⇀', 338: '⇁', 339: '⇂', 340: '⇃', 341: '⇄', 342: '⇆', 343: '⇇', 344: '⇈', 345: '⇉', 346: '⇍', 347: '⇖', 348: '⇜', 349: '⇠', 350: '⇡', 351: '⇥', 352: '⇪', 353: '∀', 354: '∁', 355: '∂', 356: '∃', 357: '∆', 358: '∇', 359: '∉', 360: '∊', 361: '∋', 362: '∍', 363: '∎', 364: '∏', 365: '∐', 366: '∑', 367: '−', 368: '∓', 369: '∔', 370: '∕', 371: '∖', 372: '∙', 373: '∞', 374: '∟', 375: '∠', 376: '∡', 377: '∢', 378: '∣', 379: '∤', 380: '∥', 381: '∩', 382: '∪', 383: '∫', 384: '∱', 385: '∶', 386: '∷', 387: '∸', 388: '∺', 389: '∼', 390: '∾', 391: '∿', 392: '≀', 393: '≃', 394: '≈', 395: '≊', 396: '≋', 397: '≏', 398: '≒', 399: '≓', 400: '≖', 401: '≙', 402: '≚', 403: '≜', 404: '≝', 405: '≞', 406: '≟', 407: '≡', 408: '≣', 409: '≤', 410: '≥', 411: '≦', 412: '≧', 413: '≨', 414: '≩', 415: '≪', 416: '≯', 417: '≱', 418: '≲', 419: '≳', 420: '≴', 421: '≵', 422: '≶', 423: '≸', 424: '≹', 425: '≺', 426: '≻', 427: '≼', 428: '≽', 429: '≿', 430: '⊀', 431: '⊂', 432: '⊃', 433: '⊆', 434: '⊇', 435: '⊊', 436: '⊋', 437: '⊍', 438: '⊏', 439: '⊐', 440: '⊑', 441: '⊒', 442: '⊓', 443: '⊔', 444: '⊖', 445: '⊙', 446: '⊜', 447: '⊟', 448: '⊡', 449: '⊢', 450: '⊣', 451: '⊤', 452: '⊥', 453: '⊨', 454: '⊪', 455: '⊰', 456: '⊱', 457: '⊲', 458: '⊳', 459: '⊴', 460: '⊸', 461: '⊹', 462: '⊺', 463: '⊻', 464: '⊼', 465: '⊽', 466: '⊾', 467: '⋀', 468: '⋁', 469: '⋂', 470: '⋃', 471: '⋅', 472: '⋈', 473: '⋉', 474: '⋊', 475: '⋋', 476: '⋍', 477: '⋎', 478: '⋏', 479: '⋔', 480: '⋖', 481: '⋗', 482: '⋚', 483: '⋜', 484: '⋝', 485: '⋞', 486: '⋟', 487: '⋠', 488: '⋡', 489: '⋣', 490: '⋤', 491: '⋥', 492: '⋦', 493: '⋨', 494: '⋪', 495: '⋮', 496: '⋯', 497: '⋰', 498: '⋱', 0: '<PAD>', 1: '<START>', 2: '<END>'}\n",
      "Max sequence length: 110\n",
      "Embedding dimension: 512\n",
      "Feedforward dimension: 2048\n",
      "Encoder layers: 4\n",
      "Decoder layers: 4\n",
      "Attention heads: 8\n",
      "Activation: relu\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 30,250,995\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# model = Transformer(char2i, \n",
    "#                     i2char, \n",
    "#                     max_sequence_length = 110,\n",
    "#                     embedding_dimension = 512, #256,\n",
    "#                     feedforward_dimension = 2048, #1024,\n",
    "#                     attention_heads = 8,\n",
    "#                     encoder_layers = 4,\n",
    "#                     decoder_layers = 4)\n",
    "#                    #dropout = .5)\n",
    "# print(\"model created\")\n",
    "# model.to(device)\n",
    "\n",
    "# model = Transformer(char2i, \n",
    "#                     i2char, \n",
    "#                     max_sequence_length = 110,\n",
    "#                     embedding_dimension = 512, #256,\n",
    "#                     feedforward_dimension = 2048, #1024,\n",
    "#                     attention_heads = 8,\n",
    "#                     encoder_layers = 4,\n",
    "#                     decoder_layers = 4)\n",
    "\n",
    "\n",
    "model = Transformer(char2i, \n",
    "                    i2char, \n",
    "                    max_sequence_length = 110,\n",
    "                    embedding_dimension = 512,\n",
    "                    feedforward_dimension = 2048,\n",
    "                    attention_heads = 8,\n",
    "                    encoder_layers = 4,\n",
    "                    decoder_layers = 4)#,\n",
    "                   #dropout = .5)\n",
    "\n",
    "model.load_state_dict(torch.load(model_save_dir + model_file, map_location=torch.device('cpu')))\n",
    "#model = torch.load(\"../../data/torch_file60.pt\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pd.read_csv('./source_dataframe.csv')\n",
    "# print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(file_dir + test_file)\n",
    "\n",
    "# start\n",
    "ntest = 1\n",
    "\n",
    "test_data = test_data.iloc[:ntest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = []\n",
    "target = []\n",
    "source_aligned = []\n",
    "target_aligned = []\n",
    "for i in range(len(test_data)):\n",
    "    d = test_data.iloc[i]\n",
    "    s = np.array(list(d['aligned sentences source'])) # aligned source\n",
    "    t = np.array(list(d['aligned sentences target'])) # aligned target\n",
    "    a = np.array(list(get_fill_in_types(d['aligned sentences target types'])))\n",
    "    ss = \"\".join(s[np.where( (a == ' ') | (a == 'W') | (a == 'w'))[0]].tolist())\n",
    "    tt = \"\".join(t[np.where( (a == ' ') | (a == 'W') | (a == 'w'))[0]].tolist())\n",
    "    source_aligned.append(ss)\n",
    "    target_aligned.append(tt)\n",
    "    source.append(ss.replace('@',''))\n",
    "    target.append(tt.replace('^',''))\n",
    "    \n",
    "test_data['words source aligned'] = source_aligned\n",
    "test_data['words target aligned'] = target_aligned\n",
    "test_data['words source'] = source\n",
    "test_data['words target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (pd.isnull(test_data['words source'])) | (pd.isnull(test_data['words target']))\n",
    "\n",
    "test_data = test_data[~mask]\n",
    "\n",
    "# now align\n",
    "gs_aligned = []; ocr_aligned = []\n",
    "gs = []; ocr = []\n",
    "for o,p in zip(test_data['words source'].values, test_data['words target'].values):\n",
    "    eops = Levenshtein.editops(o, p)\n",
    "    ocr_text_aligned, pdf_text_aligned = align_texts_fast(o,p,eops)\n",
    "    gs_aligned.append(pdf_text_aligned)\n",
    "    # futze with ocr aligned so it matches data input\n",
    "    ocr_text_aligned = ocr_text_aligned.replace('^', '@')\n",
    "    ocr_aligned.append(ocr_text_aligned)\n",
    "    gs.append(pdf_text_aligned.replace('@',''))\n",
    "    ocr.append(ocr_text_aligned.replace('@',''))\n",
    "    \n",
    "test_data['gs_aligned'] = gs_aligned\n",
    "test_data['ocr_aligned'] = ocr_aligned\n",
    "test_data['gs'] = gs\n",
    "test_data['ocr'] = ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.assign(source = lambda df: df.ocr_aligned.str.replace(\"@\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(reference, hypothesis, progress_bar = False):\n",
    "    #assert len(reference) == len(hypothesis)\n",
    "    text = zip(reference, hypothesis)\n",
    "    if progress_bar:\n",
    "        text = tqdm(text, total = len(reference))\n",
    "    d = [distance(r, h) for r, h in text]\n",
    "    output = pd.DataFrame({\"reference\":reference, \"hypothesis\":hypothesis})\\\n",
    "    .assign(distance = lambda df: d)\\\n",
    "    .assign(\n",
    "        cer = lambda df: df.apply(\n",
    "            lambda r: 100 * r[\"distance\"] / max(len(r[\"reference\"]), 1), \n",
    "            axis = 1\n",
    "        )\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arr = test_data[\"ocr_to_input\"].to_numpy()\n",
    "# gs_arr = test_data[\"gs_aligned\"].to_numpy()\n",
    "ocr_list = test_data[\"ocr_aligned\"].tolist()\n",
    "gs_list = test_data[\"gs_aligned\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def uniform(j, window_size):\n",
    "    return 1.0\n",
    "\n",
    "def triangle(j, window_size):\n",
    "    m = window_size//2\n",
    "    return m - 0.5 * abs(m - j)\n",
    "\n",
    "def bell(j, window_size):\n",
    "    m = window_size // 2\n",
    "    s = window_size // 2\n",
    "    return exp(-((m-j)/s)**2)\n",
    "\n",
    "def disjoint(\n",
    "    string,\n",
    "    model, \n",
    "    source_index,\n",
    "    target_index,\n",
    "    window_size = 50,\n",
    "    decoding_method = \"greedy_search\", \n",
    "    document_progress_bar = False, \n",
    "    document_batch_progress_bar = 0, \n",
    "    *arcorrect\n",
    "):\n",
    "    model.eval()\n",
    "    windows = [string[i:i+window_size] \n",
    "        for i in range(0, len(string), window_size)]\n",
    "    windows = [\"\".join([source_index.vocabulary.lookup(c) for c in s])\\\n",
    "        .replace(\"<UNK>\", \" \") for s in windows]\n",
    "    #X = source_index.text2tensor(windows, progress_bar = False).cuda()\n",
    "    X = source_index.text2tensor(windows, progress_bar = False).cpu()\n",
    "    if decoding_method == \"greedy_search\":\n",
    "        predictions, probs = seq2seq.greedy_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )\n",
    "    elif decoding_method == \"beam_search\":\n",
    "        predictions, probs = seq2seq.beam_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )   \n",
    "        predictions = predictions[:, 0, :]\n",
    "    output = target_index.tensor2text(predictions)\n",
    "    output = [re.sub(r\"<START>|<PAD>|<UNK>|<END>.*\", \"\", s) for s in output]\n",
    "    return \"\".join(output)\n",
    "\n",
    "def n_grams(\n",
    "    string,\n",
    "    model,\n",
    "    source_index,\n",
    "    target_index,\n",
    "    window_size = 50, \n",
    "    decoding_method = \"greedy_search\", \n",
    "    weighting = \"uniform\",\n",
    "    document_progress_bar = False, \n",
    "    document_batch_progress_bar = 0,      \n",
    "    main_batch_size = 1024,\n",
    "    *arcorrect\n",
    "):\n",
    "    model.eval()\n",
    "    if len(string) <= window_size:\n",
    "        windows = [string]\n",
    "    else:\n",
    "        windows = [string[i:i + window_size] \n",
    "            for i in range(len(string) - window_size + 1)]\n",
    "    windows = [\"\".join([source_index.vocabulary.lookup(c) for c in s])\\\n",
    "    .replace(\"<UNK>\", \" \") for s in windows]\n",
    "    #X = source_index.text2tensor(windows, progress_bar = False).cuda()\n",
    "    X = source_index.text2tensor(windows, progress_bar = False).cpu()\n",
    "    if decoding_method == \"greedy_search\":\n",
    "        predictions, probs = seq2seq.greedy_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = False, \n",
    "            *arcorrect\n",
    "        )\n",
    "    if decoding_method == \"beam_search\":\n",
    "        predictions, probs = seq2seq.beam_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )   \n",
    "        predictions = predictions[:, 0, :]   \n",
    "    output = target_index.tensor2text(predictions)\n",
    "    output = [re.sub(r\"<START>|<PAD>|<UNK>|<END>.*\", \"\", s) for s in output]\n",
    "    if weighting == \"uniform\":\n",
    "        weighting = uniform\n",
    "    elif weighting == \"triangle\":\n",
    "        weighting = triangle\n",
    "    elif weighting == \"bell\":\n",
    "        weighting = bell\n",
    "    votes = [\n",
    "        {k:0.0 for k in target_index.vocabulary} \n",
    "        for c in string\n",
    "    ]\n",
    "    for i, s in enumerate(output):\n",
    "        for j, (counter, char)\\\n",
    "        in enumerate(zip(votes[i:i + window_size], s)):\n",
    "            counter[char] += weighting(j, window_size)\n",
    "    output = [max(c.keys(), key = lambda x: c[x]) for c in votes]\n",
    "    output = \"\".join(output)\n",
    "    return votes, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source = [list(\"abcdefghijkl\"), list(\"mnopqrstwxyz\")]\n",
    "#target = [list(\"ABCDEFGHIJKL\"), list(\"MNOPQRSTWXYZ\")] #update to astronomy vocab\n",
    "source = []#yy\n",
    "for s in model.source_index:\n",
    "    sentence = \"\"\n",
    "    for c in s:\n",
    "        #nu = source_index[c]\n",
    "        #sentence.append(c)\n",
    "        #sentence.append(num)\n",
    "        sentence += c\n",
    "    source.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_source = []\n",
    "n = 20 # chunk length  \n",
    "#for s in arr:\n",
    "for x in range(min([48,ntest])):#only process n paragraphs?\n",
    "    s = ocr_list[x]\n",
    "    #sentence_array = list(s) #convert string to an array of characters\n",
    "    chunks = [s[i:i+n] for i in range(0, len(s), n)]\n",
    "    for sentence_chunk in chunks:\n",
    "        new_source.append(list(sentence_chunk))#append array to new_source array\n",
    "\n",
    "    #new_source.append(list(\"stars cataclysmic  \"))#mark end of sentences with stars cataclysmic\n",
    "    new_source.append( list(\"advice charm touch  \"))#new end of sentence with 20 chars\n",
    "    #because the model only allows 20 characters at a time, each row is 20 chars chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_beam_search import seq2seq\n",
    "source_index = seq2seq.Index(source)\n",
    "target_index =  source_index\n",
    "X_new = source_index.text2tensor(new_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions, log_probabilities = beam_search(\n",
    "#     model,\n",
    "#     X_new.cuda())\n",
    "\n",
    "# local\n",
    "predictions, log_probabilities = beam_search(\n",
    "    model,\n",
    "    X_new.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions.cuda()\n",
    "#predictions.cpu() # local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  5, 58, 77, 74, 94,  5, 88, 90, 74, 76, 74, 88, 89, 74, 73,  5, 89,\n",
       "         77, 70, 89],\n",
       "        [ 1,  5, 58, 77, 74, 91,  5, 88, 90, 74, 76, 74, 88, 89, 74, 73,  5, 89,\n",
       "         77, 70, 89],\n",
       "        [ 1,  5, 58, 77, 74, 94,  5, 88, 90, 74, 76, 38, 88, 89, 74, 73,  5, 89,\n",
       "         77, 70, 89],\n",
       "        [ 1,  5, 58, 77, 74, 94,  5, 88, 90, 74, 76, 38, 38, 38, 74, 73,  5, 89,\n",
       "         77, 70, 89],\n",
       "        [ 1,  5,  5, 58, 77, 74, 94,  5, 88, 90, 74, 76, 74, 88, 89, 74, 73,  5,\n",
       "         89, 77, 70]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [target_index.tensor2text(p) for p in predictions]\n",
    "# output = []\n",
    "# for p in predictions:\n",
    "#     try:\n",
    "#         xo = target_index.tensor2text(p)\n",
    "#         output.append(xo)\n",
    "#     except:\n",
    "#         print('key error', np.unique(p))\n",
    "#         output.append('KEY ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output\n",
    "#output2 = [source_index.tensor2text(p) for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  5, 58,  ..., 77, 70, 89],\n",
       "         [ 1,  5, 58,  ..., 77, 70, 89],\n",
       "         [ 1,  5, 58,  ..., 77, 70, 89],\n",
       "         [ 1,  5, 58,  ..., 77, 70, 89],\n",
       "         [ 1,  5,  5,  ..., 89, 77, 70]],\n",
       "\n",
       "        [[ 1,  5, 84,  ..., 82,  5,  5],\n",
       "         [ 1,  5, 84,  ..., 82,  5,  5],\n",
       "         [ 1,  5, 84,  ..., 84, 82,  5],\n",
       "         [ 1,  5, 84,  ..., 87, 84, 82],\n",
       "         [ 1,  5, 84,  ..., 84, 82,  5]],\n",
       "\n",
       "        [[ 1, 77, 74,  ..., 72, 72, 87],\n",
       "         [ 1, 89, 77,  ..., 70, 72, 72],\n",
       "         [ 1, 71, 74,  ..., 72, 72, 87],\n",
       "         [ 1,  5, 89,  ...,  5, 70, 72],\n",
       "         [ 1, 77, 74,  ..., 70, 72, 72]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1, 81,  5,  ...,  5, 76, 70],\n",
       "         [ 1, 81,  5,  ...,  5, 74, 70],\n",
       "         [ 1, 81,  5,  ...,  5, 76, 70],\n",
       "         [ 1, 89,  5,  ...,  5, 76, 70],\n",
       "         [ 1, 81,  5,  ...,  5,  5, 35]],\n",
       "\n",
       "        [[ 1, 87, 81,  ...,  2, 46, 14],\n",
       "         [ 1, 87, 81,  ..., 19,  2,  2],\n",
       "         [ 1, 87, 81,  ..., 19,  2,  2],\n",
       "         [ 1, 87, 81,  ...,  2, 23, 24],\n",
       "         [ 1, 87, 81,  ..., 19,  2, 28]],\n",
       "\n",
       "        [[ 1, 70, 73,  ..., 83,  5,  5],\n",
       "         [ 1, 70, 73,  ..., 38,  5,  5],\n",
       "         [ 1, 70, 73,  ..., 73,  5,  5],\n",
       "         [ 1, 70, 73,  ...,  5, 89, 77],\n",
       "         [ 1, 70, 73,  ..., 83,  5, 89]]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "predictions.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "predictions.to(device)\n",
    "use_cuda = True\n",
    "if device == 'cpu': use_cuda = False\n",
    "device\n",
    "#torch.cuda.get_device_name(0)\n",
    "#torch.cpu.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def uniform(j, window_size):\n",
    "    return 1.0\n",
    "\n",
    "def triangle(j, window_size):\n",
    "    m = window_size//2\n",
    "    return m - 0.5 * abs(m - j)\n",
    "\n",
    "def bell(j, window_size):\n",
    "    m = window_size // 2\n",
    "    s = window_size // 2\n",
    "    return exp(-((m-j)/s)**2)\n",
    "\n",
    "def disjoint(\n",
    "    string,\n",
    "    model, \n",
    "    source_index,\n",
    "    target_index,\n",
    "    window_size = 50,\n",
    "    decoding_method = \"greedy_search\", \n",
    "    document_progress_bar = False, \n",
    "    document_batch_progress_bar = 0, \n",
    "    use_cuda = True,\n",
    "    *arcorrect\n",
    "):\n",
    "    model.eval()\n",
    "    windows = [string[i:i+window_size] \n",
    "        for i in range(0, len(string), window_size)]\n",
    "    windows = [\"\".join([source_index.vocabulary.lookup(c) for c in s])\\\n",
    "        .replace(\"<UNK>\", \" \") for s in windows]\n",
    "    if use_cuda:\n",
    "        X = source_index.text2tensor(windows, progress_bar = False).cuda()\n",
    "    else:\n",
    "        X = source_index.text2tensor(windows, progress_bar = False).cpu()\n",
    "    if decoding_method == \"greedy_search\":\n",
    "        predictions, probs = seq2seq.greedy_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )\n",
    "    elif decoding_method == \"beam_search\":\n",
    "        predictions, probs = seq2seq.beam_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )   \n",
    "        predictions = predictions[:, 0, :]\n",
    "    output = target_index.tensor2text(predictions)\n",
    "    output = [re.sub(r\"<START>|<PAD>|<UNK>|<END>.*\", \"\", s) for s in output]\n",
    "    return \"\".join(output)\n",
    "\n",
    "def n_grams(\n",
    "    string,\n",
    "    model,\n",
    "    source_index,\n",
    "    target_index,\n",
    "    window_size = 50, \n",
    "    decoding_method = \"greedy_search\", \n",
    "    weighting = \"uniform\",\n",
    "    document_progress_bar = False, \n",
    "    document_batch_progress_bar = 0,      \n",
    "    main_batch_size = 1024,\n",
    "    use_cuda=True,\n",
    "    *arcorrect\n",
    "):\n",
    "    model.eval()\n",
    "    if len(string) <= window_size:\n",
    "        windows = [string]\n",
    "    else:\n",
    "        windows = [string[i:i + window_size] \n",
    "            for i in range(len(string) - window_size + 1)]\n",
    "    windows = [\"\".join([source_index.vocabulary.lookup(c) for c in s])\\\n",
    "    .replace(\"<UNK>\", \" \") for s in windows]\n",
    "    if use_cuda:\n",
    "        X = source_index.text2tensor(windows, progress_bar = False).cuda()\n",
    "    else:\n",
    "        X = source_index.text2tensor(windows, progress_bar = False).cpu()\n",
    "    if decoding_method == \"greedy_search\":\n",
    "        predictions, probs = seq2seq.greedy_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = False, \n",
    "            *arcorrect\n",
    "        )\n",
    "    if decoding_method == \"beam_search\":\n",
    "        predictions, probs = seq2seq.beam_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )   \n",
    "        predictions = predictions[:, 0, :]   \n",
    "    output = target_index.tensor2text(predictions)\n",
    "    output = [re.sub(r\"<START>|<PAD>|<UNK>|<END>.*\", \"\", s) for s in output]\n",
    "    if weighting == \"uniform\":\n",
    "        weighting = uniform\n",
    "    elif weighting == \"triangle\":\n",
    "        weighting = triangle\n",
    "    elif weighting == \"bell\":\n",
    "        weighting = bell\n",
    "    votes = [\n",
    "        {k:0.0 for k in target_index.vocabulary} \n",
    "        for c in string\n",
    "    ]\n",
    "    for i, s in enumerate(output):\n",
    "        for j, (counter, char)\\\n",
    "        in enumerate(zip(votes[i:i + window_size], s)):\n",
    "            counter[char] += weighting(j, window_size)\n",
    "    output = [max(c.keys(), key = lambda x: c[x]) for c in votes]\n",
    "    output = \"\".join(output)\n",
    "    return votes, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_evaluation(\n",
    "    raw, \n",
    "    gs, \n",
    "    model, \n",
    "    source_index,\n",
    "    target_index,\n",
    "    save_path = None, \n",
    "    window_size = 40, \n",
    "    document_progress_bar = False, \n",
    "    use_cuda = True\n",
    "):\n",
    "    print(\"evaluating all methods...\")\n",
    "    metrics = []\n",
    "    old = levenshtein(reference = gs, hypothesis = raw).cer.mean()\n",
    "    # disjoint\n",
    "    print(\"  disjoint window...\")\n",
    "    print(\"    greedy_search...\")\n",
    "    #start = t()\n",
    "    start = time.time()\n",
    "    corrections = [\n",
    "        disjoint(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            document_progress_bar = document_progress_bar, \n",
    "            window_size = window_size,\n",
    "            use_cuda=use_cuda\n",
    "        ) for s in raw]\n",
    "    #print(\"hello\")\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"disjoint\", \n",
    "            \"decoding\":\"greedy\",\n",
    "            \"window_size\":window_size * 2,\n",
    "            \"weighting\":pd.NA,\n",
    "            #\"inference_seconds\":t() - start,\n",
    "            \"inference_seconds\":time.time() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "    #start = t()\n",
    "    start = time.time()\n",
    "    corrections = [\n",
    "        disjoint(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            document_progress_bar = document_progress_bar, \n",
    "            window_size = window_size * 2,\n",
    "            use_cuda=use_cuda\n",
    "        ) \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"disjoint\", \n",
    "            \"decoding\":\"greedy\",\n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":pd.NA,\n",
    "            #\"inference_seconds\":t() - start,\n",
    "            \"inference_seconds\":time.time() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "    print(\"    beam_search...\")\n",
    "    #start = t()\n",
    "    start = time.time()\n",
    "    corrections = [\n",
    "        disjoint(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            decoding_method = \"beam_search\", \n",
    "            document_progress_bar = document_progress_bar, \n",
    "            window_size = window_size,\n",
    "            use_cuda=use_cuda\n",
    "        ) \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"disjoint\", \n",
    "            \"decoding\":\"beam\", \n",
    "            \"window_size\":window_size * 2,\n",
    "            \"weighting\":pd.NA,\n",
    "            #\"inference_seconds\":t() - start,\n",
    "            \"inference_seconds\":time.time() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "    #start = t()\n",
    "    start = time.time()\n",
    "    corrections = [\n",
    "        disjoint(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            decoding_method = \"beam_search\", \n",
    "            document_progress_bar = document_progress_bar, \n",
    "            window_size = window_size * 2,\n",
    "            use_cuda=use_cuda\n",
    "        ) \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"disjoint\", \n",
    "            \"decoding\":\"beam\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":pd.NA,\n",
    "            #\"inference_seconds\":t() - start,\n",
    "            \"inference_seconds\":time.time() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "    # sliding\n",
    "    print(\"  sliding\")\n",
    "    print(\"    greedy...\")\n",
    "    ## greedy search\n",
    "    print(\"      uniform...\")\n",
    "    #start = t()\n",
    "    start = time.time()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            weighting = uniform,\n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size,\n",
    "            use_cuda=use_cuda\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"greedy\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"uniform\",\n",
    "            #\"inference_seconds\":t() - start,\n",
    "            \"inference_seconds\":time.time() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "\n",
    "    print(\"      triangle...\")\n",
    "    #start = t()\n",
    "    start = time.time()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            weighting = triangle, \n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size,\n",
    "            use_cuda=use_cuda\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"greedy\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"triangle\",\n",
    "            #\"inference_seconds\":t() - start,\n",
    "            \"inference_seconds\":time.time() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "\n",
    "    print(\"      bell...\")\n",
    "    #start = t()\n",
    "    start = time.time()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            weighting = bell, \n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size,\n",
    "            use_cuda=use_cuda\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"greedy\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"bell\",\n",
    "            #\"inference_seconds\":t() - start,\n",
    "            \"inference_seconds\":time.time() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "\n",
    "    ## beam search\n",
    "    print(\"    beam...\")\n",
    "    print(\"      uniform...\")\n",
    "    #start = t()\n",
    "    start = time.time()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            decoding_method = \"beam_search\", \n",
    "            weighting = uniform, \n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size,\n",
    "            use_cuda=use_cuda\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"beam\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"uniform\",\n",
    "            #\"inference_seconds\":t() - start,\n",
    "            \"inference_seconds\":time.time() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "\n",
    "    print(\"      triangle...\")\n",
    "    #start = t()\n",
    "    start = time.time()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            decoding_method = \"beam_search\", \n",
    "            weighting = triangle, \n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size,\n",
    "            use_cuda=use_cuda\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"beam\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"triangle\",\n",
    "            #\"inference_seconds\":t() - start,\n",
    "            \"inference_seconds\":time.time() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "\n",
    "    print(\"      bell...\")\n",
    "    #start = t()\n",
    "    start = time.time()\n",
    "    if use_cuda:\n",
    "        corrections = [\n",
    "            n_grams(\n",
    "                s, \n",
    "                model.cuda(), \n",
    "                source_index,\n",
    "                target_index,\n",
    "                decoding_method = \"beam_search\", \n",
    "                weighting = bell, \n",
    "                document_progress_bar = document_progress_bar,\n",
    "                window_size = window_size,\n",
    "                use_cuda=use_cuda\n",
    "            )[1] \n",
    "            for s in raw\n",
    "        ]\n",
    "    else:\n",
    "        corrections = [\n",
    "            n_grams(\n",
    "                s, \n",
    "                model.cpu(), \n",
    "                source_index,\n",
    "                target_index,\n",
    "                decoding_method = \"beam_search\", \n",
    "                weighting = bell, \n",
    "                document_progress_bar = document_progress_bar,\n",
    "                window_size = window_size,\n",
    "                use_cuda=use_cuda\n",
    "            )[1] \n",
    "            for s in raw\n",
    "        ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"beam\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"bell\",\n",
    "            #\"inference_seconds\":t() - start,\n",
    "            \"inference_seconds\":time.time() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "    print()\n",
    "    return pd.DataFrame(metrics).assign(\n",
    "        improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "#source_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "evaluating all correction methods...\n",
      "evaluating all methods...\n",
      "  disjoint window...\n",
      "    greedy_search...\n",
      "    beam_search...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nevaluating all correction methods...\")\n",
    "evaluation = full_evaluation(\n",
    "    ocr_list,\n",
    "    gs_list,\n",
    "    model,\n",
    "    source_index,\n",
    "    target_index,\n",
    "    use_cuda=use_cuda\n",
    ")\n",
    "\n",
    "print(\"\\n--results--\")\n",
    "print(\"test data:\", test)\n",
    "print(\"plain beam search:\", just_beam)\n",
    "print(\"disjoint\")\n",
    "print(\"  greedy search:\", disjoint_greedy)\n",
    "print(\"  beam search:\", disjoint_beam)\n",
    "print(\"n_grams\")\n",
    "print(\"  greedy search:\", n_grams_greedy)\n",
    "print(\"  beam search:\", n_grams_beam)\n",
    "\n",
    "print(\"\\n\", evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_list #take out @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.to_csv(output_folder + \"eval/evaluation60new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_output = []\n",
    "sentence = \"\"\n",
    "for chunks in output:\n",
    "    #if chunks[0] == 'stars cataclysmic  ':#everytime it finds stars cataclysmic it outputs a new sentence #theres only 19?\n",
    "    if chunks[0] == '<START>advice charm touch  ':#everytime it finds advice charm touch   it outputs a new sentence\n",
    "        new_output.append(sentence)\n",
    "        sentence = \"\"# this empties the sentence variable whenever a sentence is added to the array\n",
    "    else:\n",
    "       new_sentence = chunks[0].replace(\"<START>\", \"\")\n",
    "       new_sentence = new_sentence.replace(\"<END>\", \"\")\n",
    "       sentence = sentence + new_sentence #combining the chunks of 20 characters together without the start and end\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "unnamed = list(range(1,49))\n",
    "#print(unnamed)\n",
    "test_data['new_output'] = new_output\n",
    "test_data['unnamed'] = unnamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data\n",
    "#test_data.to_csv('./github_output_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pybind11\n",
    "# !pip install fastwer\n",
    "#!pip install pytesseract\n",
    "#!sudo apt install tesseract-ocr\n",
    "\n",
    "import cv2\n",
    "#import pytesseract\n",
    "import fastwer\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test_data.iterrows():\n",
    "  filename = row['unnamed']\n",
    "  ref = row['gs_aligned']\n",
    "  ocr = row['ocr_aligned']\n",
    "  output = row['new_output']\n",
    "  cer = fastwer.score_sent(ocr, ref, char_level=True)\n",
    "  wer = fastwer.score_sent(ocr, ref, char_level=False)\n",
    "  test_data.loc[test_data['unnamed'] == filename, 'before_cer'] = round(cer,2) # Round value to 2 decimal places\n",
    "  test_data.loc[test_data['unnamed'] == filename, 'before_wer'] = round(wer,2)\n",
    "\n",
    "#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv(output_dir + 'eval/cer_wer_new50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test_data.iterrows():\n",
    "  filename = row['unnamed']\n",
    "  ref = row['gs_aligned']\n",
    "  ocr = row['ocr_aligned']\n",
    "  output = row['new_output']\n",
    "  cer = fastwer.score_sent(output, ref, char_level=True)\n",
    "  wer = fastwer.score_sent(output, ref, char_level=False)\n",
    "  test_data.loc[test_data['unnamed'] == filename, 'after_cer'] = round(cer,2) # Round value to 2 decimal places\n",
    "  test_data.loc[test_data['unnamed'] == filename, 'after_wer'] = round(wer,2)\n",
    "\n",
    "#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('./cer_wer_new50epochs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "new_output = tokenize.sent_tokenize(sentence)\n",
    "#new_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs =test_data['gs_aligned']\n",
    "gs_list = ' '.join(test_data['gs_aligned'].tolist())\n",
    "#gs_array = gs.to_numpy()\n",
    "\n",
    "#print(gs_array)\n",
    "new_gs = tokenize.sent_tokenize(gs_list) # trying to take gs column to match the length of the new_output.\n",
    "len(new_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTest():   \n",
    "    input_file = './post_ocr_correction/data/en/data/test'\n",
    "    \n",
    "    # reverse is true, therefore pair[0] is ocr and pair[1] is gs\n",
    "    input_lang, output_lang, pairs = prepareData(input_file,'gs', 'ocr', True)\n",
    "    \n",
    "    output_sentences = []\n",
    "    input_sentences = []\n",
    "    corrected_sentences = []\n",
    "    for pair in pairs:\n",
    "        sentence = pair[0]  #ocr sentence\n",
    "        try:\n",
    "            output_words, attentions = evaluate(encoder1, attn_decoder1, sentence)\n",
    "            input_sentences.append(pair[1])  #gs sentence\n",
    "            output_sentences.append(pair[0]) #ocr sentence\n",
    "            corrected_sentences.append(' '.join(output_words))\n",
    " \n",
    "            #testing_df.append({'output':''.join(output_words),'input':x},ignore_index=True)\n",
    "            #showAttention(x, output_words, attentions)\n",
    "        except KeyError:\n",
    "            print('KeyError =', sentence)\n",
    "    testing_df = pd.DataFrame({\"gs\":input_sentences, \"ocr\":output_sentences, \"output\":corrected_sentences})\n",
    "    return testing_df\n",
    "testing_dataframe1 = readTest()\n",
    "testing_dataframe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_df1 = pd.DataFrame({\"output\":new_output})\n",
    "output_df1.to_csv('./output_dataframe_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTest():   \n",
    "    input_file = './post_ocr_correction/data/en/data/test'\n",
    "    \n",
    "    # reverse is true, therefore pair[0] is ocr and pair[1] is gs\n",
    "    input_lang, output_lang, pairs = prepareData(input_file,'gs', 'ocr', True)\n",
    "    \n",
    "    output_sentences = []\n",
    "    input_sentences = []\n",
    "    corrected_sentences = []\n",
    "    for pair in pairs:\n",
    "        sentence = pair[0]  #ocr sentence\n",
    "        try:\n",
    "            output_words, attentions = evaluate(encoder1, attn_decoder1, sentence)\n",
    "            input_sentences.append(pair[1])  #gs sentence\n",
    "            output_sentences.append(pair[0]) #ocr sentence\n",
    "            corrected_sentences.append(' '.join(output_words))\n",
    " \n",
    "            #testing_df.append({'output':''.join(output_words),'input':x},ignore_index=True)\n",
    "            #showAttention(x, output_words, attentions)\n",
    "        except KeyError:\n",
    "            print('KeyError =', sentence)\n",
    "    testing_df = pd.DataFrame({\"gs\":input_sentences, \"ocr\":output_sentences, \"output\":corrected_sentences})\n",
    "    return testing_df\n",
    "testing_dataframe1 = readTest()\n",
    "testing_dataframe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# #dictionary = {'GS': \"\", 'OCR': \"\", 'index'=[0]} \n",
    "# #training_df = pd.DataFrame(dictionary)\n",
    "\n",
    "# ocr =[]\n",
    "# ocr_aligned=[]\n",
    "# gs_aligned=[]\n",
    "\n",
    "# for x in range(1008,1120):\n",
    "#     for y in range(len(PDF_OUT_SENT[x])):\n",
    "#         for z in range(len(PDF_OUT_SENT[x][y])):\n",
    "#             OCR = OCR_OUT_SENT[x][y][z]\n",
    "#             OCR_aligned_train = OCR_aligned_SENT[x][y][z]\n",
    "#             GS_aligned_train = PDF_OUT_SENT[x][y][z]\n",
    "#             #dictionary = {'GS': GS_train, 'OCR': OCR_train} \n",
    "#             #training_df = pd.DataFrame(dictionary)\n",
    "#             #gs_train = training_df.append(OCR_OUT_SENT)\n",
    "#             ocr.append(OCR)\n",
    "#             ocr_aligned.append(OCR_aligned_train)\n",
    "#             gs_aligned.append(GS_aligned_train)\n",
    "#             #training_df = pd.concat([training_df, pd.DataFrame(dictionary)], ignore_index = True)\n",
    "# data_ocr = pd.DataFrame({\"ocr_to_input\":ocr}) #\"ocr_aligned\":ocr_aligned, \"gs_aligned\":gs_aligned})\n",
    "# print(data_ocr.shape)\n",
    "# data_ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, probs = model.predict(dev_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tensor2text(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tensor2text(train_target[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tensor2text(train_source[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
