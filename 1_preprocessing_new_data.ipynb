{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '/home/jnaiman/data/morgan/'\n",
    "\n",
    "ender = '_regular'\n",
    "\n",
    "vocabulary_file = \"data/vocabulary_new_pages_new\"+ender+\".pkl\"\n",
    "train_file = \"data/train_aligned_new_pages\"+ender+\".pkl\"\n",
    "val_file = \"data/dev_aligned_new_pages\"+ender+\".pkl\"\n",
    "\n",
    "window_length = 100 # for subsetting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.lm import Vocabulary\n",
    "import torch\n",
    "import torch.utils.data as tud\n",
    "# import sys\n",
    "# sys.path.append(\"../../lib\")\n",
    "#from metrics import levenshtein\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import re\n",
    "import gc\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_folder+vocabulary_file, \"rb\") as file:\n",
    "    vocabulary = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "585"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30922727, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_pickle(output_folder+train_file)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Char to integer/integer to char:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2i = {c:i for i, c in enumerate(sorted(vocabulary), 3)}\n",
    "char2i[\"<PAD>\"] = 0\n",
    "char2i[\"<START>\"] = 1\n",
    "char2i[\"<END>\"] = 2\n",
    "len(char2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2char = {i:c for i, c in enumerate(sorted(vocabulary), 3)}\n",
    "i2char[0] = \"<PAD>\"\n",
    "i2char[1] = \"<START>\"\n",
    "i2char[2] = \"<END>\"\n",
    "len(i2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding sequences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2880e7eb41e04458bfb6605d06644caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30922727 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#length = 100\n",
    "#window_length = 100 # I think this is the same as length?\n",
    "\n",
    "output = []\n",
    "for s in tqdm(train.source):\n",
    "    output.append(torch.tensor([1] + [char2i[c] for c in s] + [2]))\n",
    "    \n",
    "train_source = torch.nn.utils.rnn.pad_sequence(output, batch_first = True)\n",
    "print(train_source.shape)\n",
    "\n",
    "output = []\n",
    "for s in tqdm(train.target):\n",
    "    output.append(torch.tensor([1] + [char2i[c] for c in s] + [2]))\n",
    "    \n",
    "train_target = torch.nn.utils.rnn.pad_sequence(output, batch_first = True)\n",
    "print(train_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the padding maybe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.source[0] == re.sub(r\"<START>|<END>|<PAD>\", \"\", \"\".join([i2char[c] for c in train_source[0].tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.target[0] == re.sub(r\"<START>|<END>|<PAD>\", \"\", \"\".join([i2char[c] for c in train_target[0].tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_source, output_folder+\"data/train_source_new_pages.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_target, output_folder+\"data/train_target_new_pages.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing with the dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = pd.read_pickle(output_folder + val_file)\n",
    "dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for s in tqdm(dev.source):\n",
    "    output.append(torch.tensor([1] + [char2i[c] for c in s] + [2]))\n",
    "    \n",
    "dev_source = torch.nn.utils.rnn.pad_sequence(output, batch_first = True)\n",
    "print(dev_source.shape)\n",
    "\n",
    "output = []\n",
    "for s in tqdm(dev.target):\n",
    "    output.append(torch.tensor([1] + [char2i[c] for c in s] + [2]))\n",
    "    \n",
    "dev_target = torch.nn.utils.rnn.pad_sequence(output, batch_first = True)\n",
    "print(dev_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.source[0] == re.sub(r\"<START>|<END>|<PAD>\", \"\", \"\".join([i2char[c] for c in dev_source[0].tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.target[0] == re.sub(r\"<START>|<END>|<PAD>\", \"\", \"\".join([i2char[c] for c in dev_target[0].tolist()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all the things for training later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dev_source, output_folder+\"data/dev_source_new_pages.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dev_target, output_folder+\"data/dev_target_new_pages.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_folder+\"data/char2i_new_pages.pkl\", \"wb\") as file:\n",
    "    pickle.dump(char2i, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_folder+\"data/i2char_new_pages.pkl\", \"wb\") as file:\n",
    "    pickle.dump(i2char, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------- orig after here ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"./post_ocr_correction/data/en/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9011, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_pickle(folder+\"/\"+\"train_aligned_new_pages.pkl\")\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(727, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev = pd.read_pickle(folder+\"/\"+\"dev_aligned.pkl\")\n",
    "dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder+\"/\"+\"vocabulary_new.pkl\", \"rb\") as file:\n",
    "    vocabulary = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_pattern = \"train_aligned_new_full*.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_pickle_files = glob.glob(folder + \"/\" + file_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = []\n",
    "# for pkl_file in train_pickle_files:\n",
    "#     with open(pkl_file, 'rb') as f:\n",
    "#         df = pickle.load(f)\n",
    "#         dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all = pd.concat(dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_pattern' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2054426/2139379222.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load each batch of pickle files into a list of dataframes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file_pattern' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 1000\n",
    "\n",
    "pickle_files = glob.glob(folder + \"/\" + file_pattern)\n",
    "\n",
    "# Load each batch of pickle files into a list of dataframes\n",
    "dfs_processed = []\n",
    "for i in range(0, len(pickle_files), batch_size):\n",
    "    dfs = []\n",
    "    for pkl_file in pickle_files[i:i+batch_size]:\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "            dfs.append(df)\n",
    "    # Concatenate all of the dataframes in one dataframe batch\n",
    "    df_batch = pd.concat(dfs, axis=0)\n",
    "    # Append the dataframe batch to the list of dataframes\n",
    "    dfs_processed.append(df_batch)\n",
    "\n",
    "# Concatenate all of the batches into a single large dataframe\n",
    "df_all = pd.concat(dfs_processed, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91919976, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df_all\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_pickle(folder/\"train_aligned_new.pkl\")\n",
    "# train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(635, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev = pd.read_pickle(folder + \"/\" + \"dev_aligned_new_full.pkl\")\n",
    "dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder + \"/\" + \"vocabulary_new.pkl\", \"rb\") as file:\n",
    "    vocabulary = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<UNK>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.lookup('put')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "668"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2i = {c:i for i, c in enumerate(sorted(vocabulary), 3)}\n",
    "char2i[\"<PAD>\"] = 0\n",
    "char2i[\"<START>\"] = 1\n",
    "char2i[\"<END>\"] = 2\n",
    "len(char2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2char = {i:c for i, c in enumerate(sorted(vocabulary), 3)}\n",
    "i2char[0] = \"<PAD>\"\n",
    "i2char[1] = \"<START>\"\n",
    "i2char[2] = \"<END>\"\n",
    "len(i2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1463e7be92ac4c66b784ac5fc66851b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9011, 80])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74db18a1c8244d184c99f90ec3e8929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9011, 80])\n"
     ]
    }
   ],
   "source": [
    "length = 100\n",
    "\n",
    "output = []\n",
    "for s in tqdm(train.source):\n",
    "    output.append(torch.tensor([1] + [char2i[c] for c in s] + [2]))\n",
    "    \n",
    "train_source = torch.nn.utils.rnn.pad_sequence(output, batch_first = True)\n",
    "print(train_source.shape)\n",
    "\n",
    "output = []\n",
    "for s in tqdm(train.target):\n",
    "    output.append(torch.tensor([1] + [char2i[c] for c in s] + [2]))\n",
    "    \n",
    "train_target = torch.nn.utils.rnn.pad_sequence(output, batch_first = True)\n",
    "print(train_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.source[0] == re.sub(r\"<START>|<END>|<PAD>\", \"\", \"\".join([i2char[c] for c in train_source[0].tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.target[0] == re.sub(r\"<START>|<END>|<PAD>\", \"\", \"\".join([i2char[c] for c in train_target[0].tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#char2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char2i[\":\"] = len(char2i)\n",
    "# char2i[\"W\"] = len(char2i)\n",
    "# char2i[\"*\"] = len(char2i)\n",
    "# char2i[\"I\"] = len(char2i)\n",
    "# char2i[\"k\"] = len(char2i)\n",
    "# char2i[\"(\"] = len(char2i)\n",
    "# char2i[\"~\"] = len(char2i)\n",
    "# char2i[\"B\"] = len(char2i)\n",
    "# char2i[\"O\"] = len(char2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426a82c380954aaba4049924e15a65bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([635, 102])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab46315c59a54ae7b3acd040f945ca01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([635, 102])\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for s in tqdm(dev.source):\n",
    "    output.append(torch.tensor([1] + [char2i[c] for c in s] + [2]))\n",
    "    \n",
    "dev_source = torch.nn.utils.rnn.pad_sequence(output, batch_first = True)\n",
    "print(dev_source.shape)\n",
    "\n",
    "output = []\n",
    "for s in tqdm(dev.target):\n",
    "    output.append(torch.tensor([1] + [char2i[c] for c in s] + [2]))\n",
    "    \n",
    "dev_target = torch.nn.utils.rnn.pad_sequence(output, batch_first = True)\n",
    "print(dev_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev.source[0] == re.sub(r\"<START>|<END>|<PAD>\", \"\", \"\".join([i2char[c] for c in dev_source[0].tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev.target[0] == re.sub(r\"<START>|<END>|<PAD>\", \"\", \"\".join([i2char[c] for c in dev_target[0].tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_source, folder+\"/\"+\"train_source_new_pages.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_target, folder+\"/\"+\"train_target_new_pages.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dev_source, folder+\"/\"+\"dev_source_new_pages.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dev_target, folder+\"/\"+\"dev_target_new_pages.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder+\"/\"+\"char2i_new_pages.pkl\", \"wb\") as file:\n",
    "    pickle.dump(char2i, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder+\"/\"+\"i2char_new_pages.pkl\", \"wb\") as file:\n",
    "    pickle.dump(i2char, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./post_ocr_correction/data/en/data/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_files = glob.glob(folder + \"/\" + file_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 1 of 92 in 45.022669315338135 seconds\n",
      "On 2 of 92 in 44.53941750526428 seconds\n",
      "On 3 of 92 in 45.85177445411682 seconds\n",
      "On 4 of 92 in 44.169872522354126 seconds\n",
      "On 5 of 92 in 44.02352285385132 seconds\n",
      "On 6 of 92 in 44.474716901779175 seconds\n",
      "On 7 of 92 in 44.30797505378723 seconds\n",
      "On 8 of 92 in 43.99881410598755 seconds\n",
      "On 9 of 92 in 44.1687376499176 seconds\n",
      "On 10 of 92 in 45.215468406677246 seconds\n",
      "On 11 of 92 in 45.70466995239258 seconds\n",
      "On 12 of 92 in 44.08570861816406 seconds\n",
      "On 13 of 92 in 44.3284113407135 seconds\n",
      "On 14 of 92 in 44.312944412231445 seconds\n",
      "On 15 of 92 in 44.34751105308533 seconds\n",
      "On 16 of 92 in 44.12048316001892 seconds\n",
      "On 17 of 92 in 44.28013849258423 seconds\n",
      "On 18 of 92 in 44.58359146118164 seconds\n",
      "On 19 of 92 in 44.67673921585083 seconds\n",
      "On 20 of 92 in 44.10096311569214 seconds\n",
      "On 21 of 92 in 44.14881420135498 seconds\n",
      "On 22 of 92 in 44.15916895866394 seconds\n",
      "On 23 of 92 in 44.510260820388794 seconds\n",
      "On 24 of 92 in 44.21414589881897 seconds\n",
      "On 25 of 92 in 44.68138933181763 seconds\n",
      "On 26 of 92 in 44.47167420387268 seconds\n",
      "On 27 of 92 in 44.49728059768677 seconds\n",
      "On 28 of 92 in 44.4281210899353 seconds\n",
      "On 29 of 92 in 44.263352155685425 seconds\n",
      "On 30 of 92 in 44.32894682884216 seconds\n",
      "On 31 of 92 in 44.332841873168945 seconds\n",
      "On 32 of 92 in 44.279656410217285 seconds\n",
      "On 33 of 92 in 44.36030387878418 seconds\n",
      "On 34 of 92 in 44.31056046485901 seconds\n",
      "On 35 of 92 in 44.281084299087524 seconds\n",
      "On 36 of 92 in 44.43903684616089 seconds\n",
      "On 37 of 92 in 44.41080284118652 seconds\n",
      "On 38 of 92 in 44.092437505722046 seconds\n",
      "On 39 of 92 in 44.234864473342896 seconds\n",
      "On 40 of 92 in 44.36394715309143 seconds\n",
      "On 41 of 92 in 44.1585431098938 seconds\n",
      "On 42 of 92 in 44.4214825630188 seconds\n",
      "On 43 of 92 in 44.485772371292114 seconds\n",
      "On 44 of 92 in 44.66470956802368 seconds\n",
      "On 45 of 92 in 44.695000886917114 seconds\n",
      "On 46 of 92 in 44.66902780532837 seconds\n",
      "On 47 of 92 in 44.351699352264404 seconds\n",
      "On 48 of 92 in 44.3346221446991 seconds\n",
      "On 49 of 92 in 44.484076738357544 seconds\n",
      "On 50 of 92 in 44.56997346878052 seconds\n",
      "On 51 of 92 in 44.33606791496277 seconds\n",
      "On 52 of 92 in 44.42721343040466 seconds\n",
      "On 53 of 92 in 44.50646257400513 seconds\n",
      "On 54 of 92 in 44.744186878204346 seconds\n",
      "On 55 of 92 in 44.35411262512207 seconds\n",
      "On 56 of 92 in 44.260377645492554 seconds\n",
      "On 57 of 92 in 44.22631812095642 seconds\n",
      "On 58 of 92 in 44.442829608917236 seconds\n",
      "On 59 of 92 in 44.17685556411743 seconds\n",
      "On 60 of 92 in 44.1217246055603 seconds\n",
      "On 61 of 92 in 44.08952617645264 seconds\n",
      "On 62 of 92 in 44.178017377853394 seconds\n",
      "On 63 of 92 in 44.78150510787964 seconds\n",
      "On 64 of 92 in 44.14414644241333 seconds\n",
      "On 65 of 92 in 44.11457014083862 seconds\n",
      "On 66 of 92 in 44.361292600631714 seconds\n",
      "On 67 of 92 in 44.5833625793457 seconds\n",
      "On 68 of 92 in 44.51130223274231 seconds\n",
      "On 69 of 92 in 44.29086756706238 seconds\n",
      "On 70 of 92 in 44.78608846664429 seconds\n",
      "On 71 of 92 in 44.436230421066284 seconds\n",
      "On 72 of 92 in 44.393664598464966 seconds\n",
      "On 73 of 92 in 44.566731691360474 seconds\n",
      "On 74 of 92 in 43.85071873664856 seconds\n",
      "On 75 of 92 in 44.381075859069824 seconds\n",
      "On 76 of 92 in 44.05804491043091 seconds\n",
      "On 77 of 92 in 45.085416078567505 seconds\n",
      "On 78 of 92 in 43.755281925201416 seconds\n",
      "On 79 of 92 in 44.05678081512451 seconds\n",
      "On 80 of 92 in 43.69415998458862 seconds\n",
      "On 81 of 92 in 44.38617467880249 seconds\n",
      "On 82 of 92 in 44.03320384025574 seconds\n",
      "On 83 of 92 in 44.15154695510864 seconds\n",
      "On 84 of 92 in 44.36648106575012 seconds\n",
      "On 85 of 92 in 44.718220710754395 seconds\n",
      "On 86 of 92 in 44.30392503738403 seconds\n",
      "On 87 of 92 in 44.381035566329956 seconds\n",
      "On 88 of 92 in 44.68705368041992 seconds\n",
      "On 89 of 92 in 44.313796520233154 seconds\n",
      "On 90 of 92 in 43.9623658657074 seconds\n",
      "On 91 of 92 in 44.50649118423462 seconds\n",
      "On 92 of 92 in 44.17710280418396 seconds\n"
     ]
    }
   ],
   "source": [
    "i_count = 0\n",
    "train = ''\n",
    "for p in pickle_files:\n",
    "    del train\n",
    "    time_start = time.time()\n",
    "    with open(p, 'rb') as f:\n",
    "        train = pickle.load(f)\n",
    "#     print(\"done with read in\", len(train))\n",
    "        # Generate tensors for source\n",
    "    batch_output_source = []\n",
    "    for s in train.source:\n",
    "        batch_output_source.append(torch.tensor([1] + [char2i[c] for c in s] + [2]))\n",
    "#  print(\"done with source\")\n",
    "    # Generate tensors for target\n",
    "    batch_output_target = []\n",
    "    for s in train.target:\n",
    "        batch_output_target.append(torch.tensor([1] + [char2i[c] for c in s] + [2]))\n",
    "#     print(\"done with target\") \n",
    "    train_source = torch.nn.utils.rnn.pad_sequence(batch_output_source, batch_first=True)\n",
    "    train_target = torch.nn.utils.rnn.pad_sequence(batch_output_target, batch_first=True)\n",
    "#     print(\"done with tensors\")\n",
    "\n",
    "    torch.save(train_source, folder+\"train_source_new\"+str(i_count).zfill(4)+\".pt\")\n",
    "    torch.save(train_target, folder+\"train_target_new\"+str(i_count).zfill(4)+\".pt\")\n",
    "    #print(i_count)\n",
    "    i_count += 1\n",
    "    del train_source\n",
    "    del train_target\n",
    "    del batch_output_target\n",
    "    del batch_output_source\n",
    "    \n",
    "    \n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"On\", i_count, \"of\", len(pickle_files), \"in\", end_time - time_start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 1000\n",
    "\n",
    "# pickle_files = glob.glob(folder + \"/\" + file_pattern)\n",
    "\n",
    "# # Load each batch of pickle files into a list of dataframes\n",
    "# for i in range(0, len(pickle_files), batch_size):\n",
    "#     for pkl_file in pickle_files[i:i+batch_size]:\n",
    "#         with open(pkl_file, 'rb') as f:\n",
    "#             df = pickle.load(f)\n",
    "#             dfs.append(df)\n",
    "#     # Concatenate all of the dataframes in one dataframe batch\n",
    "#     df_batch = pd.concat(dfs, axis=0)\n",
    "#     # Append the dataframe batch to the list of dataframes\n",
    "#     dfs_processed.append(df_batch)\n",
    "\n",
    "# # Concatenate all of the batches into a single large dataframe\n",
    "# df_all = pd.concat(dfs_processed, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3037115/2693055873.py\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Split train.source and train.target into smaller chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0msource_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mtarget_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36marray_split\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36marray_split\u001b[0;34m(ary, indices_or_sections, axis)\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiv_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiv_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0msub_arys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msub_arys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mis_bool_indexer\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \"\"\"\n\u001b[1;32m    133\u001b[0m     if isinstance(key, (ABCSeries, np.ndarray, ABCIndex)) or (\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mis_array_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     ):\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/pandas/core/dtypes/inference.py\u001b[0m in \u001b[0;36mis_array_like\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \"\"\"\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# length = 100\n",
    "\n",
    "# output = []\n",
    "# for s in tqdm(train.source):\n",
    "#     output.append(torch.tensor([1] + [char2i[c] for c in s] + [2]))\n",
    "    \n",
    "# train_source = torch.nn.utils.rnn.pad_sequence(output, batch_first = True)\n",
    "# print(train_source.shape)\n",
    "\n",
    "# output = []\n",
    "# for s in tqdm(train.target):\n",
    "#     output.append(torch.tensor([1] + [char2i[c] for c in s] + [2]))\n",
    "    \n",
    "# train_target = torch.nn.utils.rnn.pad_sequence(output, batch_first = True)\n",
    "# print(train_target.shape)\n",
    "\n",
    "\n",
    "# batch_size = 10000\n",
    "# output_source = []\n",
    "# output_target = []\n",
    "\n",
    "# # Split train.source and train.target into smaller chunks\n",
    "# source_chunks = np.array_split(train.source, len(train.source) // batch_size + 1)\n",
    "# target_chunks = np.array_split(train.target, len(train.target) // batch_size + 1)\n",
    "\n",
    "# for i in range(len(source_chunks)):\n",
    "#     # Get the current source and target chunk\n",
    "#     batch_source = source_chunks[i]\n",
    "#     batch_target = target_chunks[i]\n",
    "\n",
    "#     # Generate tensors \n",
    "#     batch_output_source = []\n",
    "#     batch_output_target = []\n",
    "#     for s in tqdm(batch_source):\n",
    "#         batch_output_source.append(torch.tensor([1] + [char2i[c] for c in s] + [2]))\n",
    "#     for s in tqdm(batch_target):\n",
    "#         batch_output_target.append(torch.tensor([1] + [char2i[c] for c in s] + [2]))\n",
    "\n",
    "#     # Append batch tensors to the output lists\n",
    "#     output_source.extend(batch_output_source)\n",
    "#     output_target.extend(batch_output_target)\n",
    "\n",
    "# #create the final tensors\n",
    "# train_source = torch.nn.utils.rnn.pad_sequence(output_source, batch_first=True)\n",
    "# train_target = torch.nn.utils.rnn.pad_sequence(output_target, batch_first=True)\n",
    "\n",
    "# print(train_source.shape)\n",
    "# print(train_target.shape)\n",
    "\n",
    "\n",
    "\n",
    "# batch_size = 100\n",
    "\n",
    "# # Split train.source and train.target into smaller chunks\n",
    "# source_chunks = np.array_split(train.source, len(train.source) // batch_size + 1)\n",
    "# target_chunks = np.array_split(train.target, len(train.target) // batch_size + 1)\n",
    "\n",
    "# output_source = []\n",
    "# output_target = []\n",
    "\n",
    "# # Process each chunk separately\n",
    "# i_count = 0\n",
    "# for source_chunk, target_chunk in tqdm(zip(source_chunks, target_chunks), total=len(source_chunks)):\n",
    "#     # Generate tensors for source\n",
    "#     batch_output_source = []\n",
    "#     for s in source_chunk:\n",
    "#         batch_output_source.append(torch.tensor([1] + [char2i[c] for c in s] + [2]))\n",
    "\n",
    "#     # Generate tensors for target\n",
    "#     batch_output_target = []\n",
    "#     for s in target_chunk:\n",
    "#         batch_output_target.append(torch.tensor([1] + [char2i[c] for c in s] + [2]))\n",
    "        \n",
    "#     train_source = torch.nn.utils.rnn.pad_sequence(batch_output_source, batch_first=True)\n",
    "#     train_target = torch.nn.utils.rnn.pad_sequence(batch_output_target, batch_first=True)\n",
    "\n",
    "#     torch.save(train_source, folder/\"train_source_new\"+str(i_count).zfill(3)+\".pt\")\n",
    "#     torch.save(train_target, folder/\"train_target_new\"+str(i_count).zfill(3)+\".pt\")\n",
    "#     print(i_count)\n",
    "#     i_count += 1\n",
    "    \n",
    "    \n",
    "    # Append batch tensors to the output lists\n",
    "#     output_source.extend(batch_output_source)\n",
    "#     output_target.extend(batch_output_target)\n",
    "\n",
    "#create the final tensors\n",
    "# train_source = torch.nn.utils.rnn.pad_sequence(output_source, batch_first=True)\n",
    "# train_target = torch.nn.utils.rnn.pad_sequence(output_target, batch_first=True)\n",
    "\n",
    "# print(train_source.shape)\n",
    "# print(train_target.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.source[0] == re.sub(r\"<START>|<END>|<PAD>\", \"\", \"\".join([i2char[c] for c in train_source[0].tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.target[0] == re.sub(r\"<START>|<END>|<PAD>\", \"\", \"\".join([i2char[c] for c in train_target[0].tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d42473086a54fd58213cfcb21949aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([635, 102])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71899cf3f994676944df4ddd293e21f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([635, 102])\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for s in tqdm(dev.source):\n",
    "    output.append(torch.tensor([1] + [char2i[c] for c in s] + [2]))\n",
    "    \n",
    "dev_source = torch.nn.utils.rnn.pad_sequence(output, batch_first = True)\n",
    "print(dev_source.shape)\n",
    "\n",
    "output = []\n",
    "for s in tqdm(dev.target):\n",
    "    output.append(torch.tensor([1] + [char2i[c] for c in s] + [2]))\n",
    "    \n",
    "dev_target = torch.nn.utils.rnn.pad_sequence(output, batch_first = True)\n",
    "print(dev_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dev.source[0] == re.sub(r\"<START>|<END>|<PAD>\", \"\", \"\".join([i2char[c] for c in dev_source[0].tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dev.target[0] == re.sub(r\"<START>|<END>|<PAD>\", \"\", \"\".join([i2char[c] for c in dev_target[0].tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(train_source, folder/\"train_source_new.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(train_target, folder/\"train_target_new.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dev_source, folder + \"/\"+ \"dev_source_new_full.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dev_target, folder+\"/\"+\"dev_target_new_full.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder+\"/\"+\"char2i_new_full.pkl\", \"wb\") as file:\n",
    "    pickle.dump(char2i, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder+\"/\"+\"i2char_new_full.pkl\", \"wb\") as file:\n",
    "    pickle.dump(i2char, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([962857, 102])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opence-v1.5.1]",
   "language": "python",
   "name": "conda-env-opence-v1.5.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
