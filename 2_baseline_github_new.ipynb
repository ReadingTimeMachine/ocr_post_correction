{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import sys\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mcosi153\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'pytorch_beam_search' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/jarobyte91/pytorch_beam_search.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/jarobyte91/pytorch_beam_search.git\n",
      "  Cloning https://github.com/jarobyte91/pytorch_beam_search.git to /tmp/pip-req-build-hgmxnucq\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/jarobyte91/pytorch_beam_search.git /tmp/pip-req-build-hgmxnucq\n",
      "  Resolved https://github.com/jarobyte91/pytorch_beam_search.git to commit 4f6c55d51556d731f3fff49d6032fe417de63c3f\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages (from pytorch-beam-search==1.2.2) (1.10.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages (from pytorch-beam-search==1.2.2) (2.8.2)\n",
      "Requirement already satisfied: nltk>=3.6.5 in ./.local/lib/python3.9/site-packages (from pytorch-beam-search==1.2.2) (3.7)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in ./.local/lib/python3.9/site-packages (from pytorch-beam-search==1.2.2) (4.2.0)\n",
      "Requirement already satisfied: tqdm>=4.61.0 in /opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages (from pytorch-beam-search==1.2.2) (4.64.0)\n",
      "Requirement already satisfied: six>=1.16.0 in ./.local/lib/python3.9/site-packages (from pytorch-beam-search==1.2.2) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2021.1 in /opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages (from pytorch-beam-search==1.2.2) (2022.1)\n",
      "Requirement already satisfied: pandas>=1.2.4 in /opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages (from pytorch-beam-search==1.2.2) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages (from pytorch-beam-search==1.2.2) (1.20.3)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages (from pytorch-beam-search==1.2.2) (2021.10.8)\n",
      "Requirement already satisfied: click in /opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages (from nltk>=3.6.5->pytorch-beam-search==1.2.2) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages (from nltk>=3.6.5->pytorch-beam-search==1.2.2) (2022.3.15)\n",
      "Requirement already satisfied: joblib in /opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages (from nltk>=3.6.5->pytorch-beam-search==1.2.2) (1.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/jarobyte91/pytorch_beam_search.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/mcosi153/guemes/lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pytorch-beam-search\n",
      "Version: 1.2.2\n",
      "Summary: A simple library that implements search algorithms for sequence models written in PyTorch.\n",
      "Home-page: https://github.com/jarobyte91/pytorch_beam_search\n",
      "Author: Juan Ramirez-Orta\n",
      "Author-email: jarobyte91@gmail.com\n",
      "License: \n",
      "Location: /home/mcosi153/.local/lib/python3.9/site-packages\n",
      "Requires: certifi, nltk, numpy, pandas, python-dateutil, pytz, six, torch, tqdm, typing-extensions\n",
      "Required-by: post-ocr-correction\n"
     ]
    }
   ],
   "source": [
    "!pip show pytorch_beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_decoding.seq2seq import Transformer\n",
    "from pytorch_beam_search.seq2seq import Transformer, beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scratch = \"/home/jarobyte/scratch/guemes/icdar/en/\"\n",
    "scratch = \"./post_ocr_correction/data/en/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2i = pickle.load(open(scratch + \"data/char2i_new_pages.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2char = pickle.load(open(scratch + \"data/i2char_new_pages.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1000000\n",
    "\n",
    "dev_size = 1000000\n",
    "\n",
    "# train_size = 1000\n",
    "\n",
    "# dev_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9011, 80])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_source = torch.load(scratch + \"data/train_source_new_pages.pt\")[:train_size].to(device)#add to custom data\n",
    "train_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9011, 80])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target = torch.load(scratch + \"data/train_target_new_pages.pt\")[:train_size].to(device)\n",
    "train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([635, 102])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_source = torch.load(scratch + \"data/dev_source_new_pages.pt\")[:dev_size].to(device)\n",
    "dev_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([635, 102])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_target = torch.load(scratch + \"data/dev_target_new_pages.pt\")[:dev_size].to(device)\n",
    "dev_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_source[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_files = glob.glob(scratch + \"data/train_source_new0*.pt\")\n",
    "# target_files = glob.glob(scratch + \"data/train_target_new0*.pt\")\n",
    "# source_files.sort()\n",
    "# target_files.sort()\n",
    "# source_files = source_files\n",
    "# target_files = target_files\n",
    "# file_number = 0\n",
    "# train_source = torch.load(source_files[file_number])[:train_size].to(device)\n",
    "# train_target = torch.load(target_files[file_number])[:train_size].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_source' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3895282/209658776.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_source' is not defined"
     ]
    }
   ],
   "source": [
    "#train_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_target = torch.load(scratch + \"data/train_target_new.pt\")[:train_size].to(device)#\n",
    "# train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([635, 102])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dev_source = torch.load(scratch + \"data/dev_source_new_full.pt\")[:dev_size].to(device)\n",
    "# dev_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([635, 102])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dev_target = torch.load(scratch + \"data/dev_target_new_full.pt\")[:dev_size].to(device)\n",
    "# dev_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from torchvision.io import read_image\n",
    "\n",
    "# class CustomImageDataset(Dataset):\n",
    "#     def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "#         self.img_labels = pd.read_csv(annotations_file)\n",
    "#         self.img_dir = img_dir\n",
    "#         self.transform = transform\n",
    "#         self.target_transform = target_transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.img_labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "#         image = read_image(img_path)\n",
    "#         label = self.img_labels.iloc[idx, 1]\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         if self.target_transform:\n",
    "#             label = self.target_transform(label)\n",
    "#         return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        #self.source = torch.load(scratch + \"data/train_source_new.pt\")[:train_size].to(device)\n",
    "       # self.target = torch.load(scratch + \"data/train_target_new.pt\")[:train_size].to(device)\n",
    "\n",
    "        source_files = glob.glob(scratch + \"data/train_source_new0*.pt\")\n",
    "        target_files = glob.glob(scratch + \"data/train_target_new0*.pt\")\n",
    "        source_files.sort()\n",
    "        target_files.sort()\n",
    "        self.source_files = source_files\n",
    "        self.target_files = target_files\n",
    "        #self.file_number = \n",
    "        self.file_tracker = []\n",
    "        self.file_lengths = []\n",
    "        for s in source_files:\n",
    "            #print(s)\n",
    "            y = torch.load(s).to(device)\n",
    "            self.file_lengths.append(len(y))\n",
    "        print(self.file_lengths)\n",
    "        \n",
    "        #self.source = torch.load(source_files[self.file_number])[:train_size].to(device)\n",
    "        #self.target = torch.load(target_files[self.file_number])[:train_size].to(device)\n",
    "        #self.file_length = len(self.source)\n",
    "        #print(self.file_length)\n",
    "#idx is the batch index\n",
    "    def __getitem__(self, idx) -> torch.Tensor:\n",
    "        while True:\n",
    "            index = np.random.randint(len(self.source_files))\n",
    "            item_index = np.random.randint(self.file_lengths[index])\n",
    "            if (index, item_index) not in self.file_tracker:\n",
    "                self.file_tracker.append((index, item_index))\n",
    "                break\n",
    "        source_sample = torch.load(self.source_files[index])[item_index].to(device)\n",
    "        target_sample = torch.load(self.target_files[index])[item_index].to(device)\n",
    "\n",
    "        return source_sample, target_sample\n",
    "#     def __getitem__(self, idx) -> torch.Tensor:\n",
    "#         index = np.random.randint(len(self.source_files))\n",
    "#         item_index = np.random.randint(self.file_lengths[index])\n",
    "#         #print(index)\n",
    "#         #print(item_index)\n",
    "#         index_save = index\n",
    "#         item_index_save = item_index\n",
    "#         while [index_save, item_index_save] not in self.file_tracker:\n",
    "#             index_save = index\n",
    "#             item_index_save = item_index\n",
    "#             #print(index)\n",
    "#             #print(item_index)\n",
    "#             self.file_tracker.append([index_save, item_index_save])\n",
    "#             print(time.ctime())\n",
    "#             source_sample = torch.load(self.source_files[index_save])[item_index_save].to(device)\n",
    "#             target_sample = torch.load(self.target_files[index_save])[item_index_save].to(device)\n",
    "#             print(time.ctime())\n",
    "#             index = np.random.randint(len(self.source_files))\n",
    "#             item_index = np.random.randint(self.file_lengths[index])\n",
    "            \n",
    "# #         if idx < self.file_length -1:\n",
    "# #             # load one sample by index, e.g like this:\n",
    "# #             source_sample = self.source[idx]\n",
    "# #             target_sample = self.target[idx]\n",
    "# #         else:\n",
    "# #             self.file_number += 1\n",
    "# #             self.source = torch.load(source_files[self.file_number])[:train_size].to(device)\n",
    "# #             self.target = torch.load(target_files[self.file_number])[:train_size].to(device)\n",
    "# #             self.file_length = len(self.source)\n",
    "# #             source_sample = self.source[idx]\n",
    "# #             target_sample = self.target[idx]\n",
    "# #             print(\"here\")\n",
    "\n",
    "#             #pd.len() \n",
    "\n",
    "#         # do some preprocessing, convert to tensor and what not\n",
    "\n",
    "#         return source_sample, target_sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.file_lengths)\n",
    "        #return train_size\n",
    "        #return len(self.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[999130, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999131, 999130, 999130, 999130, 999130, 999130, 999130, 999131, 999130, 999130, 999130, 999130, 999130, 999131, 999130, 999130, 999130, 999130, 999130, 999131, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999131, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999131, 999130, 999130, 999130, 999131, 999130, 999130, 999130, 999130, 999131, 999131, 999130, 999131, 999130, 999130, 999130, 999131, 999130, 999131, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999131, 999130, 999130, 999130, 999131, 999130, 999130, 999130, 999130, 999131, 999131, 999130, 999130, 999130, 999130]\n"
     ]
    }
   ],
   "source": [
    "yourDataset = YourDataset()\n",
    "train_loader = torch.utils.data.DataLoader(yourDataset, batch_size= 32, num_workers=0, shuffle=False) #pin_memory=True works if data is loaded on the cpu first, and then pushed to gpu\n",
    "#from pin_memory RuntimeError: cannot pin 'torch.cuda.LongTensor' only dense CPU tensors can be pinned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2872500"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(time.ctime())\n",
    "# for i, (target, source) in enumerate(train_loader):\n",
    "#     if i%10 == 0:\n",
    "#         print(i)\n",
    "#         print(target)\n",
    "#         print(time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as tud\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module): \n",
    "    \"\"\"\n",
    "    A generic sequence-to-sequence model. All other sequence-to-sequence models should extend this class \n",
    "    with a __init__ and forward methods, in the same way as in normal PyTorch.\n",
    "    \"\"\"\n",
    "    def print_architecture(self):\n",
    "        \"\"\"\n",
    "        Displays the information about the model in standard output. \n",
    "        \"\"\"\n",
    "        for k in self.architecture.keys():\n",
    "            print(f\"{k.replace('_', ' ').capitalize()}: {self.architecture[k]}\")\n",
    "        print(f\"Trainable parameters: {sum([p.numel() for p in self.parameters()]):,}\")\n",
    "        print()\n",
    "\n",
    "    def fit(self,#train_loader, \n",
    "            X_train, \n",
    "            Y_train, \n",
    "            X_dev = None, \n",
    "            Y_dev = None, \n",
    "            batch_size = 100, \n",
    "            epochs = 5, \n",
    "            learning_rate = 10**-4, \n",
    "            weight_decay = 0, \n",
    "            progress_bar = 0, \n",
    "            save_path = None):\n",
    "        print(\"fit begins\")\n",
    "        best_dev_loss=float('inf')\n",
    "        best_epoch=float('inf')\n",
    "        \"\"\"\n",
    "        A generic training method with Adam and Cross Entropy.\n",
    "\n",
    "        Parameters\n",
    "        ----------    \n",
    "        X_train: LongTensor of shape (train_examples, train_input_length)\n",
    "            The input sequences of the training set.\n",
    "            \n",
    "        Y_train: LongTensor of shape (train_examples, train_output_length)\n",
    "            The output sequences of the training set.\n",
    "            \n",
    "        X_dev: LongTensor of shape (dev_examples, dev_input_length), optional\n",
    "            The input sequences for the development set.\n",
    "            \n",
    "        Y_train: LongTensor of shape (dev_examples, dev_output_length), optional\n",
    "            The output sequences for the development set.\n",
    "            \n",
    "        batch_size: int\n",
    "            The number of examples to process in each batch.\n",
    "\n",
    "        epochs: int\n",
    "            The number of epochs of the training process.\n",
    "            \n",
    "        learning_rate: float\n",
    "            The learning rate to use with Adam in the training process. \n",
    "            \n",
    "        weight_decay: float\n",
    "            The weight_decay parameter of Adam (L2 penalty), useful for regularizing models. For a deeper \n",
    "            documentation, go to https://pytorch.org/docs/stable/_modules/torch/optim/adam.html#Adam            \n",
    "\n",
    "        progress_bar: int\n",
    "            Shows a tqdm progress bar, useful for tracking progress with large tensors.\n",
    "            If equal to 0, no progress bar is shown. \n",
    "            If equal to 1, shows a bar with one step for every epoch.\n",
    "            If equal to 2, shows the bar when equal to 1 and also shows a bar with one step per batch for every epoch.\n",
    "            If equal to 3, shows the bars when equal to 2 and also shows a bar to track the progress of the evaluation\n",
    "            in the development set.\n",
    "            \n",
    "        save_path: string, optional\n",
    "            Path to save the .pt file containing the model parameters when the training ends.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        performance: Pandas DataFrame\n",
    "            DataFrame with the following columns: epoch, train_loss, train_error_rate, (optionally dev_loss and \n",
    "            dev_error_rate), minutes, learning_rate, weight_decay, model, encoder_embedding_dimension, \n",
    "            decoder_embedding_dimension, encoder_hidden_units, encoder_layers, decoder_hidden_units, decoder_layers, \n",
    "            dropout, parameters and one row for each of the epochs, containing information about the training process.\n",
    "        \"\"\"\n",
    "        assert X_train.shape[0] == Y_train.shape[0]\n",
    "        assert (X_dev is None and Y_dev is None) or (X_dev is not None and Y_dev is not None) \n",
    "        if (X_dev is not None and Y_dev is not None):\n",
    "            assert X_dev.shape[0] == Y_dev.shape[0]\n",
    "            dev = True\n",
    "        else:\n",
    "            dev = False\n",
    "            \n",
    "\n",
    "        train_dataset = tud.TensorDataset(X_train, Y_train)\n",
    "        train_loader = tud.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)# make own class data loader to read in batches at a time\n",
    "#         class YourDataset(torch.utils.data.Dataset):\n",
    "#             def __init__(self) -> None:\n",
    "#                 #self.source = torch.load(scratch + \"data/train_source_new.pt\")[:train_size].to(device)\n",
    "#                # self.target = torch.load(scratch + \"data/train_target_new.pt\")[:train_size].to(device)\n",
    "                \n",
    "#                 source_files = glob.glob(scratch + \"data/train_source_new0*.pt\")\n",
    "#                 target_files = glob.glob(scratch + \"data/train_target_new0*.pt\")\n",
    "#                 source_files.sort()\n",
    "#                 target_files.sort()\n",
    "#                 self.source_files = source_files\n",
    "#                 self.target_files = target_files\n",
    "#                 self.file_number = 0\n",
    "#                 self.source = torch.load(source_files[self.file_number])[:train_size].to(device)\n",
    "#                 self.target = torch.load(target_files[self.file_number])[:train_size].to(device)\n",
    "#                 self.file_length = len(self.source)\n",
    "                \n",
    "#             def __getitem__(self, idx) -> torch.Tensor:\n",
    "#                 if idx < self.file_length:\n",
    "#                     # load one sample by index, e.g like this:\n",
    "#                     source_sample = self.source[idx]\n",
    "#                     target_sample = self.target[idx]\n",
    "#                 else:\n",
    "#                     self.file_number += 1\n",
    "#                     self.source = torch.load(source_files[self.file_number])[:train_size].to(device)\n",
    "#                     self.target = torch.load(target_files[self.file_number])[:train_size].to(device)\n",
    "#                     self.file_length = len(self.source)\n",
    "#                     source_sample = self.source[idx]\n",
    "#                     target_sample = self.target[idx]\n",
    "            \n",
    "                    \n",
    "#                     #pd.len() \n",
    "\n",
    "#                 # do some preprocessing, convert to tensor and what not\n",
    "\n",
    "#                 return source_sample, target_sample\n",
    "\n",
    "#             def __len__(self):\n",
    "#                 return len(self.source)\n",
    "        #creating dataloader\n",
    "        #yourDataset = YourDataset()\n",
    "        #train_loader = torch.utils.data.DataLoader(yourDataset, batch_size= batch_size, num_workers=0, shuffle=True)\n",
    "    \n",
    "        criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "        performance = []\n",
    "        start = timer()\n",
    "        epochs_iterator = range(1, epochs + 1)\n",
    "        if progress_bar > 0:\n",
    "            epochs_iterator = tqdm(epochs_iterator)\n",
    "            print(\"Training started\")\n",
    "        print(\"X_train.shape:\", X_train.shape)\n",
    "        print(\"Y_train.shape:\", Y_train.shape)\n",
    "        if dev:\n",
    "            print(\"X_dev.shape:\", X_dev.shape)\n",
    "            print(\"Y_dev.shape:\", Y_dev.shape)\n",
    "        print(f\"Epochs: {epochs:,}\\nLearning rate: {learning_rate}\\nWeight decay: {weight_decay}\")\n",
    "        header_1 = \"Epoch | Train                \"\n",
    "        header_2 = \"      | Loss     | Error Rate\"\n",
    "        rule = \"-\" * 29\n",
    "        if dev:\n",
    "            header_1 += \" | Development          \"\n",
    "            header_2 += \" | Loss     | Error Rate\"\n",
    "            rule += \"-\" * 24\n",
    "        header_1 += \" | Minutes\"\n",
    "        header_2 += \" |\"\n",
    "        rule += \"-\" * 10\n",
    "        print(header_1, header_2, rule, sep = \"\\n\")\n",
    "        for e in epochs_iterator:\n",
    "            self.train()\n",
    "            losses = []\n",
    "            errors = []\n",
    "            sizes = []\n",
    "            train_iterator = train_loader\n",
    "            if progress_bar > 1:\n",
    "                train_iterator = tqdm(train_iterator)\n",
    "            for x, y in train_iterator:\n",
    "                # compute loss and backpropagate\n",
    "                probabilities = self.forward(x, y).transpose(1, 2)[:, :, :-1]\n",
    "                y = y[:, 1:]\n",
    "                loss = criterion(probabilities, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                # compute accuracy\n",
    "                predictions = probabilities.argmax(1)\n",
    "                batch_errors = (predictions != y)\n",
    "                # append the results\n",
    "                losses.append(loss.item())\n",
    "                errors.append(batch_errors.sum().item())\n",
    "                sizes.append(batch_errors.numel())\n",
    "            train_loss = sum(losses) / len(losses)\n",
    "            train_error_rate = 100 * sum(errors) / sum(sizes)\n",
    "            t = (timer() - start) / 60\n",
    "            status_string = f\"{e:>5} | {train_loss:>8.4f} | {train_error_rate:>10.3f}\"\n",
    "            status = {\"epoch\":e,\n",
    "                      \"train_loss\": train_loss,\n",
    "                      \"train_error_rate\": train_error_rate}\n",
    "            if dev:\n",
    "                dev_loss, dev_error_rate = self.evaluate(X_dev, \n",
    "                                                         Y_dev, \n",
    "                                                         batch_size = batch_size, \n",
    "                                                         progress_bar = progress_bar > 2, \n",
    "                                                         criterion = criterion)\n",
    "                status_string += f\" | {dev_loss:>8.4f} | {dev_error_rate:>10.3f}\"\n",
    "                status.update({\"dev_loss\": dev_loss, \"dev_error_rate\": dev_error_rate})\n",
    "            status.update({\"training_minutes\": t,\n",
    "                           \"learning_rate\": learning_rate,\n",
    "                           \"weight_decay\": weight_decay})\n",
    "            performance.append(status)\n",
    "            if save_path is not None: \n",
    "                print(\"dev =\", dev)\n",
    "                print(\"e =\", e)\n",
    "                print(\"dev loss =\", dev_loss)\n",
    "                print(\"best dev loss =\", best_dev_loss)\n",
    "                #if (not dev) or (e < 2) or (dev_loss < min([p[\"dev_loss\"] for p in performance[:-1]])):\n",
    "                if (not dev) or (e < 2) or (dev_loss < best_dev_loss):\n",
    "                    torch.save(self.state_dict(), save_path)\n",
    "                    print(status)\n",
    "                    best_dev_loss = dev_loss\n",
    "                    print(\"save path =\", save_path)\n",
    "            status_string += f\" | {t:>7.1f}\"\n",
    "            print(status_string)\n",
    "        print()\n",
    "        return pd.concat((pd.DataFrame(performance), \n",
    "                          pd.DataFrame([self.architecture for i in performance])), axis = 1)\\\n",
    "               .drop(columns = [\"source_index\", \"target_index\"])\n",
    "    \n",
    "            \n",
    "    def evaluate(self, \n",
    "                 X, \n",
    "                 Y, \n",
    "                 criterion = nn.CrossEntropyLoss(), \n",
    "                 batch_size = 128, \n",
    "                 progress_bar = False):\n",
    "        \"\"\"\n",
    "        Evaluates the model on a dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: LongTensor of shape (examples, input_length)\n",
    "            The input sequences of the dataset.\n",
    "            \n",
    "        Y: LongTensor of shape (examples, output_length)\n",
    "            The output sequences of the dataset.\n",
    "            \n",
    "        criterion: PyTorch module\n",
    "            The loss function to evalue the model on the dataset, has to be able to compare self.forward(X, Y) and Y\n",
    "            to produce a real number.\n",
    "            \n",
    "        batch_size: int\n",
    "            The batch size of the evaluation loop.\n",
    "            \n",
    "        progress_bar: bool\n",
    "            Shows a tqdm progress bar, useful for tracking progress with large tensors.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        loss: float\n",
    "            The average of criterion across the whole dataset.\n",
    "            \n",
    "        error_rate: float\n",
    "            The step-by-step accuracy of the model across the whole dataset. Useful as a sanity check, as it should\n",
    "            go to zero as the loss goes to zero.\n",
    "            \n",
    "        \"\"\"\n",
    "        dataset = tud.TensorDataset(X, Y)\n",
    "        loader = tud.DataLoader(dataset, batch_size = batch_size)\n",
    "        self.eval()\n",
    "        losses = []\n",
    "        errors = []\n",
    "        sizes = []\n",
    "        with torch.no_grad():\n",
    "            iterator = iter(loader)\n",
    "            if progress_bar:\n",
    "                iterator = tqdm(iterator)\n",
    "            for batch in iterator:\n",
    "                x, y = batch\n",
    "                # compute loss\n",
    "                probabilities = self.forward(x, y).transpose(1, 2)[:, :, :-1]\n",
    "                y = y[:, 1:]\n",
    "                loss = criterion(probabilities, y)\n",
    "                # compute accuracy\n",
    "                predictions = probabilities.argmax(1)\n",
    "                batch_errors = (predictions != y)\n",
    "                # append the results\n",
    "                losses.append(loss.item())\n",
    "                errors.append(batch_errors.sum().item())\n",
    "                sizes.append(batch_errors.numel())\n",
    "            loss = sum(losses) / len(losses)\n",
    "            error_rate = 100 * sum(errors) / sum(sizes)\n",
    "        return loss, error_rate \n",
    "    \n",
    "class LSTM(Seq2Seq):\n",
    "    def __init__(self, \n",
    "                 source_index, \n",
    "                 target_index, \n",
    "                 encoder_embedding_dimension = 32,\n",
    "                 decoder_embedding_dimension = 32,\n",
    "                 encoder_hidden_units = 128, \n",
    "                 encoder_layers = 2,\n",
    "                 decoder_hidden_units = 128,\n",
    "                 decoder_layers = 2,\n",
    "                 dropout = 0.0):\n",
    "        \"\"\"\n",
    "        A standard Seq2Seq LSTM model as in 'Learning Phrase Representations using RNN Encoder-Decoder \n",
    "        for Statistical Machine Translation' by Cho et al. (2014). \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        in_vocabulary: dictionary\n",
    "            Vocabulary with the index:token pairs for the inputs of the model.\n",
    "            \n",
    "        out_vocabulary: dictionary\n",
    "            Vocabulary with the token:index pairs for the outputs of the model.\n",
    "            \n",
    "        encoder_embedding_dimension: int\n",
    "            Dimension of the embeddings to feed into the encoder.\n",
    "            \n",
    "        decoder_embedding_dimension: int\n",
    "            Dimension of the embeddings to feed into the decoder.\n",
    "            \n",
    "        encoder_hidden_units: int\n",
    "            Hidden size of the encoder.\n",
    "            \n",
    "        encoder_layers: int\n",
    "            Hidden layers of the encoder.\n",
    "            \n",
    "        decoder_hidden_units: int\n",
    "            Hidden units of the decoder.\n",
    "            \n",
    "        decoder_layers: int\n",
    "            Hidden layers of the decoder.\n",
    "            \n",
    "        dropout: float between 0.0 and 1.0\n",
    "            Dropout rate to apply to whole model.\n",
    "        \"\"\"\n",
    "        self.source_index = source_index\n",
    "        self.target_index = target_index\n",
    "        super().__init__()\n",
    "        self.source_embeddings = nn.Embedding(len(source_index), encoder_embedding_dimension)\n",
    "        self.target_embeddings = nn.Embedding(len(target_index), decoder_embedding_dimension)\n",
    "        self.encoder_rnn = nn.LSTM(input_size = encoder_embedding_dimension, \n",
    "                                   hidden_size = encoder_hidden_units, \n",
    "                                   num_layers = encoder_layers,\n",
    "                                   dropout = dropout)\n",
    "        self.decoder_rnn = nn.LSTM(input_size = encoder_layers * encoder_hidden_units + decoder_embedding_dimension, \n",
    "                                   hidden_size = decoder_hidden_units, \n",
    "                                   num_layers = decoder_layers,\n",
    "                                   dropout = dropout)\n",
    "        self.output_layer = nn.Linear(decoder_hidden_units, len(target_index))\n",
    "        self.architecture = dict(model = \"Seq2Seq LSTM\",\n",
    "                                 source_index = source_index, \n",
    "                                 target_index = target_index, \n",
    "                                 encoder_embedding_dimension = encoder_embedding_dimension,\n",
    "                                 decoder_embedding_dimension = decoder_embedding_dimension,\n",
    "                                 encoder_hidden_units = encoder_hidden_units, \n",
    "                                 encoder_layers = encoder_layers,\n",
    "                                 decoder_hidden_units = decoder_hidden_units,\n",
    "                                 decoder_layers = decoder_layers,\n",
    "                                 dropout = dropout)\n",
    "        self.print_architecture()\n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"\n",
    "        Forward method of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: LongTensor of shape (batch_size, input_length)\n",
    "            Tensor of integers containing the inputs for the model.\n",
    "            \n",
    "        Y: LongTensor of shape (batch_size, output_length)\n",
    "            Tensor of integers containing the output produced so far.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        output: FloatTensor of shape (batch_size, output_length, len(out_vocabulary))\n",
    "            Tensor of floats containing the inputs for the final Softmax layer (usually integrated into the loss function).\n",
    "        \"\"\"\n",
    "        X = self.source_embeddings(X.T)\n",
    "        encoder, (encoder_last_hidden, encoder_last_memory) = self.encoder_rnn(X)\n",
    "        encoder_last_hidden = encoder_last_hidden.transpose(0, 1).flatten(start_dim = 1)\n",
    "        encoder_last_hidden = encoder_last_hidden.repeat((Y.shape[1], 1, 1))\n",
    "        Y = self.target_embeddings(Y.T)\n",
    "        Y = torch.cat((Y, encoder_last_hidden), axis = -1)\n",
    "        decoder, (decoder_last_hidden, decoder_last_memory) = self.decoder_rnn(Y)\n",
    "        output = self.output_layer(decoder.transpose(0, 1))\n",
    "        return output        \n",
    "    \n",
    "    \n",
    "class ReversingLSTM(Seq2Seq):\n",
    "    def __init__(self, \n",
    "                 source_index, \n",
    "                 target_index, \n",
    "                 encoder_embedding_dimension = 32,\n",
    "                 decoder_embedding_dimension = 32,\n",
    "                 encoder_hidden_units = 128, \n",
    "                 encoder_layers = 2,\n",
    "                 decoder_hidden_units = 128,\n",
    "                 decoder_layers = 2,\n",
    "                 dropout = 0.0):\n",
    "        \"\"\"\n",
    "        A standard Seq2Seq LSTM model that reverses the order of the input as in \n",
    "        'Sequence to sequence learning with Neural Networks' by Sutskever et al. (2014). \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        in_vocabulary: dictionary\n",
    "            Vocabulary with the index:token pairs for the inputs of the model.\n",
    "            \n",
    "        out_vocabulary: dictionary\n",
    "            Vocabulary with the token:index pairs for the outputs of the model.\n",
    "            \n",
    "        encoder_embedding_dimension: int\n",
    "            Dimension of the embeddings to feed into the encoder.\n",
    "            \n",
    "        decoder_embedding_dimension: int\n",
    "            Dimension of the embeddings to feed into the decoder.\n",
    "            \n",
    "        encoder_hidden_units: int\n",
    "            Hidden size of the encoder.\n",
    "            \n",
    "        encoder_layers: int\n",
    "            Hidden layers of the encoder.\n",
    "            \n",
    "        decoder_hidden_units: int\n",
    "            Hidden units of the decoder.\n",
    "            \n",
    "        decoder_layers: int\n",
    "            Hidden layers of the decoder.\n",
    "            \n",
    "        dropout: float between 0.0 and 1.0\n",
    "            Dropout rate to apply to whole model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.source_index = source_index\n",
    "        self.target_index = target_index\n",
    "        self.source_embeddings = nn.Embedding(len(source_index), encoder_embedding_dimension)\n",
    "        self.target_embeddings = nn.Embedding(len(target_index), decoder_embedding_dimension)\n",
    "        self.encoder_rnn = nn.LSTM(input_size = encoder_embedding_dimension, \n",
    "                                   hidden_size = encoder_hidden_units, \n",
    "                                   num_layers = encoder_layers,\n",
    "                                   dropout = dropout)\n",
    "        self.decoder_rnn = nn.LSTM(input_size = decoder_embedding_dimension, \n",
    "                                   hidden_size = decoder_hidden_units, \n",
    "                                   num_layers = decoder_layers,\n",
    "                                   dropout = dropout)\n",
    "        self.output_layer = nn.Linear(decoder_hidden_units, len(target_index))\n",
    "        self.enc2dec = nn.Linear(encoder_hidden_units * encoder_layers, decoder_hidden_units * decoder_layers)\n",
    "        self.architecture = dict(model = \"Seq2Seq Reversing LSTM\",\n",
    "                                 source_index = source_index, \n",
    "                                 target_index = target_index, \n",
    "                                 encoder_embedding_dimension = encoder_embedding_dimension,\n",
    "                                 decoder_embedding_dimension = decoder_embedding_dimension,\n",
    "                                 encoder_hidden_units = encoder_hidden_units, \n",
    "                                 encoder_layers = encoder_layers,\n",
    "                                 decoder_hidden_units = decoder_hidden_units,\n",
    "                                 decoder_layers = decoder_layers,\n",
    "                                 dropout = dropout)\n",
    "        self.print_architecture()\n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"\n",
    "        Forward method of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: LongTensor of shape (batch_size, input_length)\n",
    "            Tensor of integers containing the inputs for the model.\n",
    "            \n",
    "        Y: LongTensor of shape (batch_size, output_length)\n",
    "            Tensor of integers containing the output produced so far.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        output: FloatTensor of shape (batch_size, output_length, len(out_vocabulary))\n",
    "            Tensor of floats containing the inputs for the final Softmax layer (usually integrated into the loss function).\n",
    "        \"\"\"\n",
    "        X = self.source_embeddings(torch.flip(X.T, dims = (1, )))\n",
    "        encoder, (encoder_last_hidden, encoder_last_memory) = self.encoder_rnn(X)\n",
    "        encoder_last_hidden = encoder_last_hidden.transpose(0, 1).flatten(start_dim = 1)\n",
    "        enc2dec = self.enc2dec(encoder_last_hidden)\\\n",
    "        .reshape(-1, self.decoder_rnn.num_layers, self.decoder_rnn.hidden_size)\\\n",
    "        .transpose(0, 1)\\\n",
    "        .contiguous()\n",
    "        Y = self.target_embeddings(Y.T)\n",
    "        decoder, (decoder_last_hidden, decoder_last_memory) = self.decoder_rnn(Y, (enc2dec, torch.zeros_like(enc2dec)))\n",
    "        output = self.output_layer(decoder.transpose(0, 1))\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class Transformer(Seq2Seq):\n",
    "    def __init__(self, \n",
    "                 source_index, \n",
    "                 target_index,\n",
    "                 max_sequence_length = 32,\n",
    "                 embedding_dimension = 32,\n",
    "                 feedforward_dimension = 128,\n",
    "                 encoder_layers = 2,\n",
    "                 decoder_layers = 2,\n",
    "                 attention_heads = 2,\n",
    "                 activation = \"relu\",\n",
    "                 dropout = 0.0):\n",
    "        \"\"\"\n",
    "        The standard PyTorch implementation of a Transformer model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        in_vocabulary: dictionary\n",
    "            Vocabulary with the index:token pairs for the inputs of the model.\n",
    "            \n",
    "        out_vocabulary: dictionary\n",
    "            Vocabulary with the token:index pairs for the outputs of the model.\n",
    "            \n",
    "        max_sequence_length: int\n",
    "            Maximum sequence length accepted by the model, both for the encoder and the decoder.\n",
    "            \n",
    "        embedding_dimension: int\n",
    "            Dimension of the embeddings of the model.\n",
    "            \n",
    "        feedforward_dimension: int\n",
    "            Dimension of the feedforward network inside the self-attention layers of the model.\n",
    "            \n",
    "        encoder_layers: int\n",
    "            Hidden layers of the encoder.\n",
    "            \n",
    "        decoder_layers: int\n",
    "            Hidden layers of the decoder.\n",
    "            \n",
    "        attention_heads: int\n",
    "            Attention heads inside every self-attention layer of the model.\n",
    "            \n",
    "        activation: string\n",
    "            Activation function of the feedforward network inside the self-attention layers of the model. Can\n",
    "            be either 'relu' or 'gelu'.\n",
    "            \n",
    "        dropout: float between 0.0 and 1.0\n",
    "            Dropout rate to apply to whole model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.source_index = source_index\n",
    "        self.target_index = target_index\n",
    "        self.source_embeddings = nn.Embedding(len(source_index), embedding_dimension)\n",
    "        self.target_embeddings = nn.Embedding(len(target_index), embedding_dimension)\n",
    "        self.positional_embeddings = nn.Embedding(max_sequence_length, embedding_dimension)\n",
    "        self.transformer = nn.Transformer(d_model = embedding_dimension, \n",
    "                                          dim_feedforward = feedforward_dimension,\n",
    "                                          nhead = attention_heads, \n",
    "                                          num_encoder_layers = encoder_layers, \n",
    "                                          num_decoder_layers = decoder_layers,\n",
    "                                          activation = activation,\n",
    "                                          dropout = dropout)\n",
    "        self.output_layer = nn.Linear(embedding_dimension, len(target_index))\n",
    "        self.architecture = dict(model = \"Seq2Seq Transformer\",\n",
    "                                 source_index = source_index,\n",
    "                                 target_index = target_index,\n",
    "                                 max_sequence_length = max_sequence_length,\n",
    "                                 embedding_dimension = embedding_dimension,\n",
    "                                 feedforward_dimension = feedforward_dimension,\n",
    "                                 encoder_layers = encoder_layers,\n",
    "                                 decoder_layers = decoder_layers,\n",
    "                                 attention_heads = attention_heads,\n",
    "                                 activation = activation,\n",
    "                                 dropout = dropout)\n",
    "        self.print_architecture()\n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"\n",
    "        Forward method of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: LongTensor of shape (batch_size, input_length)\n",
    "            Tensor of integers containing the inputs for the model.\n",
    "            \n",
    "        Y: LongTensor of shape (batch_size, output_length)\n",
    "            Tensor of integers containing the output produced so far.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        output: FloatTensor of shape (batch_size, output_length, len(out_vocabulary))\n",
    "            Tensor of floats containing the inputs for the final Softmax layer (usually integrated in the loss function).\n",
    "        \"\"\"\n",
    "        assert X.shape[1] <= self.architecture[\"max_sequence_length\"]\n",
    "        assert Y.shape[1] <= self.architecture[\"max_sequence_length\"]\n",
    "        X = self.source_embeddings(X)\n",
    "        X_positional = torch.arange(X.shape[1], device = X.device).repeat((X.shape[0], 1))\n",
    "        X_positional = self.positional_embeddings(X_positional)\n",
    "        X = (X + X_positional).transpose(0, 1)\n",
    "        Y = self.target_embeddings(Y)\n",
    "        Y_positional = torch.arange(Y.shape[1], device = Y.device).repeat((Y.shape[0], 1))\n",
    "        Y_positional = self.positional_embeddings(Y_positional)\n",
    "        Y = (Y + Y_positional).transpose(0, 1)\n",
    "        mask = self.transformer.generate_square_subsequent_mask(Y.shape[0]).to(Y.device)\n",
    "        transformer_output = self.transformer.forward(src = X,\n",
    "                                                      tgt = Y, \n",
    "                                                      tgt_mask = mask)\n",
    "        transformer_output = transformer_output.transpose(0, 1)\n",
    "        return self.output_layer(transformer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Seq2Seq Transformer\n",
      "Source index: {'\\t': 3, '\\n': 4, ' ': 5, '!': 6, '\"': 7, '#': 8, '$': 9, '%': 10, '&': 11, \"'\": 12, '(': 13, ')': 14, '*': 15, '+': 16, ',': 17, '-': 18, '.': 19, '/': 20, '0': 21, '1': 22, '2': 23, '3': 24, '4': 25, '5': 26, '6': 27, '7': 28, '8': 29, '9': 30, ':': 31, ';': 32, '<': 33, '<UNK>': 34, '=': 35, '>': 36, '?': 37, '@': 38, 'A': 39, 'B': 40, 'C': 41, 'D': 42, 'E': 43, 'F': 44, 'G': 45, 'H': 46, 'I': 47, 'J': 48, 'K': 49, 'L': 50, 'M': 51, 'N': 52, 'O': 53, 'P': 54, 'Q': 55, 'R': 56, 'S': 57, 'T': 58, 'U': 59, 'V': 60, 'W': 61, 'X': 62, 'Y': 63, 'Z': 64, '[': 65, '\\\\': 66, ']': 67, '^': 68, '_': 69, '`': 70, 'a': 71, 'b': 72, 'c': 73, 'd': 74, 'e': 75, 'f': 76, 'g': 77, 'h': 78, 'i': 79, 'j': 80, 'k': 81, 'l': 82, 'm': 83, 'n': 84, 'o': 85, 'p': 86, 'q': 87, 'r': 88, 's': 89, 't': 90, 'u': 91, 'v': 92, 'w': 93, 'x': 94, 'y': 95, 'z': 96, '{': 97, '|': 98, '}': 99, '~': 100, '\\xa0': 101, '¡': 102, '¢': 103, '£': 104, '¥': 105, '§': 106, '¨': 107, '©': 108, '«': 109, '\\xad': 110, '®': 111, '°': 112, '´': 113, 'µ': 114, '·': 115, '¸': 116, '¹': 117, '»': 118, 'À': 119, 'Á': 120, 'Â': 121, 'Å': 122, 'Ç': 123, 'É': 124, 'Ê': 125, 'Ò': 126, 'Ó': 127, 'Õ': 128, 'Ö': 129, '×': 130, 'Ü': 131, 'à': 132, 'á': 133, 'ã': 134, 'ä': 135, 'ç': 136, 'è': 137, 'é': 138, 'ê': 139, 'ë': 140, 'ì': 141, 'í': 142, 'î': 143, 'ï': 144, 'ñ': 145, 'ò': 146, 'ó': 147, 'ô': 148, 'ö': 149, 'ù': 150, 'ú': 151, 'û': 152, 'ü': 153, 'ý': 154, 'ć': 155, 'Č': 156, 'č': 157, 'Ė': 158, 'ė': 159, 'ğ': 160, 'İ': 161, 'ı': 162, 'ł': 163, 'ń': 164, 'ō': 165, 'ő': 166, 'ř': 167, 'Ś': 168, 'Ş': 169, 'ş': 170, 'Š': 171, 'š': 172, 'ű': 173, 'ź': 174, 'Ż': 175, 'ż': 176, 'Ž': 177, 'ž': 178, 'ǹ': 179, 'ȩ': 180, 'Ȯ': 181, '́': 182, '̄': 183, '̆': 184, '̇': 185, '̊': 186, '̋': 187, '͵': 188, '΄': 189, 'Ά': 190, 'Έ': 191, 'Ί': 192, 'Ό': 193, 'Ύ': 194, 'Ώ': 195, 'ΐ': 196, 'Α': 197, 'Β': 198, 'Γ': 199, 'Δ': 200, 'Ε': 201, 'Ζ': 202, 'Η': 203, 'Θ': 204, 'Ι': 205, 'Κ': 206, 'Λ': 207, 'Μ': 208, 'Ν': 209, 'Ξ': 210, 'Ο': 211, 'Π': 212, 'Ρ': 213, 'Σ': 214, 'Τ': 215, 'Υ': 216, 'Φ': 217, 'Χ': 218, 'Ψ': 219, 'Ω': 220, 'ά': 221, 'έ': 222, 'ή': 223, 'ί': 224, 'α': 225, 'β': 226, 'γ': 227, 'δ': 228, 'ε': 229, 'ζ': 230, 'η': 231, 'θ': 232, 'ι': 233, 'κ': 234, 'λ': 235, 'μ': 236, 'ν': 237, 'ξ': 238, 'ο': 239, 'π': 240, 'ρ': 241, 'ς': 242, 'σ': 243, 'τ': 244, 'υ': 245, 'φ': 246, 'χ': 247, 'ψ': 248, 'ω': 249, 'ϊ': 250, 'ό': 251, 'ύ': 252, 'ώ': 253, 'ϐ': 254, 'ϕ': 255, 'ϱ': 256, 'Ḋ': 257, 'Ḟ': 258, 'Ḣ': 259, 'Ṁ': 260, 'ṁ': 261, 'Ṗ': 262, 'ṫ': 263, 'ἀ': 264, 'ἁ': 265, 'ἂ': 266, 'ἃ': 267, 'ἄ': 268, 'ἅ': 269, 'ἆ': 270, 'Ἀ': 271, 'Ἁ': 272, 'Ἂ': 273, 'Ἄ': 274, 'Ἅ': 275, 'Ἆ': 276, 'ἐ': 277, 'ἑ': 278, 'ἒ': 279, 'ἓ': 280, 'ἔ': 281, 'ἕ': 282, 'Ἐ': 283, 'Ἑ': 284, 'Ἓ': 285, 'Ἔ': 286, 'Ἕ': 287, 'ἠ': 288, 'ἡ': 289, 'ἢ': 290, 'ἣ': 291, 'ἤ': 292, 'ἥ': 293, 'ἦ': 294, 'Ἠ': 295, 'Ἡ': 296, 'ἰ': 297, 'ἱ': 298, 'ἲ': 299, 'ἳ': 300, 'ἴ': 301, 'ἵ': 302, 'ἶ': 303, 'ἷ': 304, 'Ἰ': 305, 'Ἱ': 306, 'Ἴ': 307, 'Ἵ': 308, 'Ἶ': 309, 'ὀ': 310, 'ὁ': 311, 'ὂ': 312, 'ὃ': 313, 'ὄ': 314, 'ὅ': 315, 'Ὀ': 316, 'Ὁ': 317, 'Ὃ': 318, 'Ὄ': 319, 'Ὅ': 320, 'ὐ': 321, 'ὑ': 322, 'ὓ': 323, 'ὔ': 324, 'ὕ': 325, 'ὖ': 326, 'ὗ': 327, 'Ὑ': 328, 'Ὕ': 329, 'ὠ': 330, 'ὡ': 331, 'ὢ': 332, 'ὤ': 333, 'ὥ': 334, 'ὦ': 335, 'ὧ': 336, 'Ὠ': 337, 'Ὡ': 338, 'Ὢ': 339, 'Ὥ': 340, 'Ὦ': 341, 'ὰ': 342, 'ὲ': 343, 'ὴ': 344, 'ὶ': 345, 'ὸ': 346, 'ὺ': 347, 'ὼ': 348, 'ᾖ': 349, 'ᾗ': 350, 'ᾠ': 351, 'ᾧ': 352, 'ᾱ': 353, 'ᾳ': 354, 'ᾶ': 355, 'ᾷ': 356, '᾽': 357, '᾿': 358, 'ῃ': 359, 'ῄ': 360, 'ῆ': 361, 'ῇ': 362, 'ῑ': 363, 'ῖ': 364, 'ῤ': 365, 'ῥ': 366, 'ῦ': 367, 'Ῥ': 368, 'ῳ': 369, 'ῶ': 370, 'ῷ': 371, '–': 372, '—': 373, '‘': 374, '’': 375, '“': 376, '”': 377, '…': 378, '€': 379, '™': 380, '←': 381, '↑': 382, '↓': 383, '↔': 384, '↕': 385, '↖': 386, '↗': 387, '↘': 388, '↙': 389, '↛': 390, '↜': 391, '↝': 392, '↠': 393, '↡': 394, '↢': 395, '↣': 396, '↤': 397, '↥': 398, '↦': 399, '↧': 400, '↨': 401, '↩': 402, '↪': 403, '↫': 404, '↭': 405, '↯': 406, '↰': 407, '↱': 408, '↲': 409, '↳': 410, '↴': 411, '↵': 412, '↶': 413, '↷': 414, '↸': 415, '↹': 416, '↺': 417, '↻': 418, '↼': 419, '↽': 420, '↾': 421, '↿': 422, '⇀': 423, '⇁': 424, '⇂': 425, '⇃': 426, '⇄': 427, '⇅': 428, '⇆': 429, '⇇': 430, '⇈': 431, '⇉': 432, '⇊': 433, '⇋': 434, '⇌': 435, '⇍': 436, '⇎': 437, '⇖': 438, '⇗': 439, '⇘': 440, '⇙': 441, '⇜': 442, '⇝': 443, '⇟': 444, '⇠': 445, '⇡': 446, '⇢': 447, '⇣': 448, '⇤': 449, '⇥': 450, '⇧': 451, '⇩': 452, '⇪': 453, '∀': 454, '∁': 455, '∂': 456, '∃': 457, '∄': 458, '∅': 459, '∆': 460, '∇': 461, '∈': 462, '∉': 463, '∊': 464, '∋': 465, '∌': 466, '∍': 467, '∎': 468, '∏': 469, '∐': 470, '∑': 471, '−': 472, '∓': 473, '∔': 474, '∕': 475, '∖': 476, '∗': 477, '∘': 478, '∙': 479, '√': 480, '∜': 481, '∝': 482, '∟': 483, '∠': 484, '∡': 485, '∢': 486, '∣': 487, '∤': 488, '∥': 489, '∦': 490, '∧': 491, '∨': 492, '∩': 493, '∪': 494, '∫': 495, '∮': 496, '∱': 497, '∴': 498, '∶': 499, '∷': 500, '∸': 501, '∹': 502, '∺': 503, '∻': 504, '∼': 505, '∽': 506, '∾': 507, '∿': 508, '≀': 509, '≁': 510, '≂': 511, '≃': 512, '≆': 513, '≈': 514, '≊': 515, '≋': 516, '≌': 517, '≍': 518, '≏': 519, '≐': 520, '≒': 521, '≓': 522, '≔': 523, '≖': 524, '≗': 525, '≘': 526, '≙': 527, '≚': 528, '≛': 529, '≜': 530, '≝': 531, '≞': 532, '≟': 533, '≠': 534, '≡': 535, '≢': 536, '≣': 537, '≤': 538, '≥': 539, '≦': 540, '≧': 541, '≨': 542, '≩': 543, '≪': 544, '≫': 545, '≬': 546, '≭': 547, '≮': 548, '≯': 549, '≰': 550, '≱': 551, '≲': 552, '≳': 553, '≴': 554, '≵': 555, '≶': 556, '≸': 557, '≹': 558, '≺': 559, '≻': 560, '≼': 561, '≽': 562, '≾': 563, '≿': 564, '⊀': 565, '⊁': 566, '⊂': 567, '⊃': 568, '⊄': 569, '⊅': 570, '⊆': 571, '⊇': 572, '⊈': 573, '⊉': 574, '⊊': 575, '⊋': 576, '⊌': 577, '⊍': 578, '⊏': 579, '⊐': 580, '⊑': 581, '⊒': 582, '⊓': 583, '⊔': 584, '⊕': 585, '⊖': 586, '⊗': 587, '⊘': 588, '⊙': 589, '⊚': 590, '⊛': 591, '⊜': 592, '⊝': 593, '⊞': 594, '⊟': 595, '⊠': 596, '⊡': 597, '⊢': 598, '⊣': 599, '⊤': 600, '⊥': 601, '⊦': 602, '⊧': 603, '⊨': 604, '⊪': 605, '⊭': 606, '⊮': 607, '⊯': 608, '⊰': 609, '⊱': 610, '⊲': 611, '⊳': 612, '⊴': 613, '⊵': 614, '⊷': 615, '⊸': 616, '⊹': 617, '⊺': 618, '⊻': 619, '⊼': 620, '⊽': 621, '⊾': 622, '⊿': 623, '⋀': 624, '⋁': 625, '⋂': 626, '⋃': 627, '⋅': 628, '⋆': 629, '⋇': 630, '⋈': 631, '⋉': 632, '⋊': 633, '⋋': 634, '⋍': 635, '⋎': 636, '⋏': 637, '⋐': 638, '⋑': 639, '⋒': 640, '⋓': 641, '⋔': 642, '⋕': 643, '⋖': 644, '⋗': 645, '⋚': 646, '⋛': 647, '⋜': 648, '⋝': 649, '⋞': 650, '⋟': 651, '⋠': 652, '⋡': 653, '⋢': 654, '⋣': 655, '⋤': 656, '⋥': 657, '⋦': 658, '⋧': 659, '⋨': 660, '⋩': 661, '⋪': 662, '⋫': 663, '⋬': 664, '⋭': 665, '⋮': 666, '⋯': 667, '⋰': 668, '⋱': 669, '⩽': 670, '<PAD>': 0, '<START>': 1, '<END>': 2}\n",
      "Target index: {3: '\\t', 4: '\\n', 5: ' ', 6: '!', 7: '\"', 8: '#', 9: '$', 10: '%', 11: '&', 12: \"'\", 13: '(', 14: ')', 15: '*', 16: '+', 17: ',', 18: '-', 19: '.', 20: '/', 21: '0', 22: '1', 23: '2', 24: '3', 25: '4', 26: '5', 27: '6', 28: '7', 29: '8', 30: '9', 31: ':', 32: ';', 33: '<', 34: '<UNK>', 35: '=', 36: '>', 37: '?', 38: '@', 39: 'A', 40: 'B', 41: 'C', 42: 'D', 43: 'E', 44: 'F', 45: 'G', 46: 'H', 47: 'I', 48: 'J', 49: 'K', 50: 'L', 51: 'M', 52: 'N', 53: 'O', 54: 'P', 55: 'Q', 56: 'R', 57: 'S', 58: 'T', 59: 'U', 60: 'V', 61: 'W', 62: 'X', 63: 'Y', 64: 'Z', 65: '[', 66: '\\\\', 67: ']', 68: '^', 69: '_', 70: '`', 71: 'a', 72: 'b', 73: 'c', 74: 'd', 75: 'e', 76: 'f', 77: 'g', 78: 'h', 79: 'i', 80: 'j', 81: 'k', 82: 'l', 83: 'm', 84: 'n', 85: 'o', 86: 'p', 87: 'q', 88: 'r', 89: 's', 90: 't', 91: 'u', 92: 'v', 93: 'w', 94: 'x', 95: 'y', 96: 'z', 97: '{', 98: '|', 99: '}', 100: '~', 101: '\\xa0', 102: '¡', 103: '¢', 104: '£', 105: '¥', 106: '§', 107: '¨', 108: '©', 109: '«', 110: '\\xad', 111: '®', 112: '°', 113: '´', 114: 'µ', 115: '·', 116: '¸', 117: '¹', 118: '»', 119: 'À', 120: 'Á', 121: 'Â', 122: 'Å', 123: 'Ç', 124: 'É', 125: 'Ê', 126: 'Ò', 127: 'Ó', 128: 'Õ', 129: 'Ö', 130: '×', 131: 'Ü', 132: 'à', 133: 'á', 134: 'ã', 135: 'ä', 136: 'ç', 137: 'è', 138: 'é', 139: 'ê', 140: 'ë', 141: 'ì', 142: 'í', 143: 'î', 144: 'ï', 145: 'ñ', 146: 'ò', 147: 'ó', 148: 'ô', 149: 'ö', 150: 'ù', 151: 'ú', 152: 'û', 153: 'ü', 154: 'ý', 155: 'ć', 156: 'Č', 157: 'č', 158: 'Ė', 159: 'ė', 160: 'ğ', 161: 'İ', 162: 'ı', 163: 'ł', 164: 'ń', 165: 'ō', 166: 'ő', 167: 'ř', 168: 'Ś', 169: 'Ş', 170: 'ş', 171: 'Š', 172: 'š', 173: 'ű', 174: 'ź', 175: 'Ż', 176: 'ż', 177: 'Ž', 178: 'ž', 179: 'ǹ', 180: 'ȩ', 181: 'Ȯ', 182: '́', 183: '̄', 184: '̆', 185: '̇', 186: '̊', 187: '̋', 188: '͵', 189: '΄', 190: 'Ά', 191: 'Έ', 192: 'Ί', 193: 'Ό', 194: 'Ύ', 195: 'Ώ', 196: 'ΐ', 197: 'Α', 198: 'Β', 199: 'Γ', 200: 'Δ', 201: 'Ε', 202: 'Ζ', 203: 'Η', 204: 'Θ', 205: 'Ι', 206: 'Κ', 207: 'Λ', 208: 'Μ', 209: 'Ν', 210: 'Ξ', 211: 'Ο', 212: 'Π', 213: 'Ρ', 214: 'Σ', 215: 'Τ', 216: 'Υ', 217: 'Φ', 218: 'Χ', 219: 'Ψ', 220: 'Ω', 221: 'ά', 222: 'έ', 223: 'ή', 224: 'ί', 225: 'α', 226: 'β', 227: 'γ', 228: 'δ', 229: 'ε', 230: 'ζ', 231: 'η', 232: 'θ', 233: 'ι', 234: 'κ', 235: 'λ', 236: 'μ', 237: 'ν', 238: 'ξ', 239: 'ο', 240: 'π', 241: 'ρ', 242: 'ς', 243: 'σ', 244: 'τ', 245: 'υ', 246: 'φ', 247: 'χ', 248: 'ψ', 249: 'ω', 250: 'ϊ', 251: 'ό', 252: 'ύ', 253: 'ώ', 254: 'ϐ', 255: 'ϕ', 256: 'ϱ', 257: 'Ḋ', 258: 'Ḟ', 259: 'Ḣ', 260: 'Ṁ', 261: 'ṁ', 262: 'Ṗ', 263: 'ṫ', 264: 'ἀ', 265: 'ἁ', 266: 'ἂ', 267: 'ἃ', 268: 'ἄ', 269: 'ἅ', 270: 'ἆ', 271: 'Ἀ', 272: 'Ἁ', 273: 'Ἂ', 274: 'Ἄ', 275: 'Ἅ', 276: 'Ἆ', 277: 'ἐ', 278: 'ἑ', 279: 'ἒ', 280: 'ἓ', 281: 'ἔ', 282: 'ἕ', 283: 'Ἐ', 284: 'Ἑ', 285: 'Ἓ', 286: 'Ἔ', 287: 'Ἕ', 288: 'ἠ', 289: 'ἡ', 290: 'ἢ', 291: 'ἣ', 292: 'ἤ', 293: 'ἥ', 294: 'ἦ', 295: 'Ἠ', 296: 'Ἡ', 297: 'ἰ', 298: 'ἱ', 299: 'ἲ', 300: 'ἳ', 301: 'ἴ', 302: 'ἵ', 303: 'ἶ', 304: 'ἷ', 305: 'Ἰ', 306: 'Ἱ', 307: 'Ἴ', 308: 'Ἵ', 309: 'Ἶ', 310: 'ὀ', 311: 'ὁ', 312: 'ὂ', 313: 'ὃ', 314: 'ὄ', 315: 'ὅ', 316: 'Ὀ', 317: 'Ὁ', 318: 'Ὃ', 319: 'Ὄ', 320: 'Ὅ', 321: 'ὐ', 322: 'ὑ', 323: 'ὓ', 324: 'ὔ', 325: 'ὕ', 326: 'ὖ', 327: 'ὗ', 328: 'Ὑ', 329: 'Ὕ', 330: 'ὠ', 331: 'ὡ', 332: 'ὢ', 333: 'ὤ', 334: 'ὥ', 335: 'ὦ', 336: 'ὧ', 337: 'Ὠ', 338: 'Ὡ', 339: 'Ὢ', 340: 'Ὥ', 341: 'Ὦ', 342: 'ὰ', 343: 'ὲ', 344: 'ὴ', 345: 'ὶ', 346: 'ὸ', 347: 'ὺ', 348: 'ὼ', 349: 'ᾖ', 350: 'ᾗ', 351: 'ᾠ', 352: 'ᾧ', 353: 'ᾱ', 354: 'ᾳ', 355: 'ᾶ', 356: 'ᾷ', 357: '᾽', 358: '᾿', 359: 'ῃ', 360: 'ῄ', 361: 'ῆ', 362: 'ῇ', 363: 'ῑ', 364: 'ῖ', 365: 'ῤ', 366: 'ῥ', 367: 'ῦ', 368: 'Ῥ', 369: 'ῳ', 370: 'ῶ', 371: 'ῷ', 372: '–', 373: '—', 374: '‘', 375: '’', 376: '“', 377: '”', 378: '…', 379: '€', 380: '™', 381: '←', 382: '↑', 383: '↓', 384: '↔', 385: '↕', 386: '↖', 387: '↗', 388: '↘', 389: '↙', 390: '↛', 391: '↜', 392: '↝', 393: '↠', 394: '↡', 395: '↢', 396: '↣', 397: '↤', 398: '↥', 399: '↦', 400: '↧', 401: '↨', 402: '↩', 403: '↪', 404: '↫', 405: '↭', 406: '↯', 407: '↰', 408: '↱', 409: '↲', 410: '↳', 411: '↴', 412: '↵', 413: '↶', 414: '↷', 415: '↸', 416: '↹', 417: '↺', 418: '↻', 419: '↼', 420: '↽', 421: '↾', 422: '↿', 423: '⇀', 424: '⇁', 425: '⇂', 426: '⇃', 427: '⇄', 428: '⇅', 429: '⇆', 430: '⇇', 431: '⇈', 432: '⇉', 433: '⇊', 434: '⇋', 435: '⇌', 436: '⇍', 437: '⇎', 438: '⇖', 439: '⇗', 440: '⇘', 441: '⇙', 442: '⇜', 443: '⇝', 444: '⇟', 445: '⇠', 446: '⇡', 447: '⇢', 448: '⇣', 449: '⇤', 450: '⇥', 451: '⇧', 452: '⇩', 453: '⇪', 454: '∀', 455: '∁', 456: '∂', 457: '∃', 458: '∄', 459: '∅', 460: '∆', 461: '∇', 462: '∈', 463: '∉', 464: '∊', 465: '∋', 466: '∌', 467: '∍', 468: '∎', 469: '∏', 470: '∐', 471: '∑', 472: '−', 473: '∓', 474: '∔', 475: '∕', 476: '∖', 477: '∗', 478: '∘', 479: '∙', 480: '√', 481: '∜', 482: '∝', 483: '∟', 484: '∠', 485: '∡', 486: '∢', 487: '∣', 488: '∤', 489: '∥', 490: '∦', 491: '∧', 492: '∨', 493: '∩', 494: '∪', 495: '∫', 496: '∮', 497: '∱', 498: '∴', 499: '∶', 500: '∷', 501: '∸', 502: '∹', 503: '∺', 504: '∻', 505: '∼', 506: '∽', 507: '∾', 508: '∿', 509: '≀', 510: '≁', 511: '≂', 512: '≃', 513: '≆', 514: '≈', 515: '≊', 516: '≋', 517: '≌', 518: '≍', 519: '≏', 520: '≐', 521: '≒', 522: '≓', 523: '≔', 524: '≖', 525: '≗', 526: '≘', 527: '≙', 528: '≚', 529: '≛', 530: '≜', 531: '≝', 532: '≞', 533: '≟', 534: '≠', 535: '≡', 536: '≢', 537: '≣', 538: '≤', 539: '≥', 540: '≦', 541: '≧', 542: '≨', 543: '≩', 544: '≪', 545: '≫', 546: '≬', 547: '≭', 548: '≮', 549: '≯', 550: '≰', 551: '≱', 552: '≲', 553: '≳', 554: '≴', 555: '≵', 556: '≶', 557: '≸', 558: '≹', 559: '≺', 560: '≻', 561: '≼', 562: '≽', 563: '≾', 564: '≿', 565: '⊀', 566: '⊁', 567: '⊂', 568: '⊃', 569: '⊄', 570: '⊅', 571: '⊆', 572: '⊇', 573: '⊈', 574: '⊉', 575: '⊊', 576: '⊋', 577: '⊌', 578: '⊍', 579: '⊏', 580: '⊐', 581: '⊑', 582: '⊒', 583: '⊓', 584: '⊔', 585: '⊕', 586: '⊖', 587: '⊗', 588: '⊘', 589: '⊙', 590: '⊚', 591: '⊛', 592: '⊜', 593: '⊝', 594: '⊞', 595: '⊟', 596: '⊠', 597: '⊡', 598: '⊢', 599: '⊣', 600: '⊤', 601: '⊥', 602: '⊦', 603: '⊧', 604: '⊨', 605: '⊪', 606: '⊭', 607: '⊮', 608: '⊯', 609: '⊰', 610: '⊱', 611: '⊲', 612: '⊳', 613: '⊴', 614: '⊵', 615: '⊷', 616: '⊸', 617: '⊹', 618: '⊺', 619: '⊻', 620: '⊼', 621: '⊽', 622: '⊾', 623: '⊿', 624: '⋀', 625: '⋁', 626: '⋂', 627: '⋃', 628: '⋅', 629: '⋆', 630: '⋇', 631: '⋈', 632: '⋉', 633: '⋊', 634: '⋋', 635: '⋍', 636: '⋎', 637: '⋏', 638: '⋐', 639: '⋑', 640: '⋒', 641: '⋓', 642: '⋔', 643: '⋕', 644: '⋖', 645: '⋗', 646: '⋚', 647: '⋛', 648: '⋜', 649: '⋝', 650: '⋞', 651: '⋟', 652: '⋠', 653: '⋡', 654: '⋢', 655: '⋣', 656: '⋤', 657: '⋥', 658: '⋦', 659: '⋧', 660: '⋨', 661: '⋩', 662: '⋪', 663: '⋫', 664: '⋬', 665: '⋭', 666: '⋮', 667: '⋯', 668: '⋰', 669: '⋱', 670: '⩽', 0: '<PAD>', 1: '<START>', 2: '<END>'}\n",
      "Max sequence length: 110\n",
      "Embedding dimension: 512\n",
      "Feedforward dimension: 2048\n",
      "Encoder layers: 4\n",
      "Decoder layers: 4\n",
      "Attention heads: 8\n",
      "Activation: relu\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 30,515,359\n",
      "\n",
      "model created\n",
      "fit begins\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926127d65a184a1fa8bb62cf08aa9b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "X_train.shape: torch.Size([9011, 80])\n",
      "Y_train.shape: torch.Size([9011, 80])\n",
      "X_dev.shape: torch.Size([635, 102])\n",
      "Y_dev.shape: torch.Size([635, 102])\n",
      "Epochs: 3\n",
      "Learning rate: 0.0001\n",
      "Weight decay: 0\n",
      "Epoch | Train                 | Development           | Minutes\n",
      "      | Loss     | Error Rate | Loss     | Error Rate |\n",
      "---------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b5fe155d53469c82162910aec552ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2054423/1067379432.py\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m log = model.fit(train_source, \n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;31m#train_loader,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2054423/2016112197.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, Y_train, X_dev, Y_dev, batch_size, epochs, learning_rate, weight_decay, progress_bar, save_path)\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0;31m# compute accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    134\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Transformer(char2i, \n",
    "                    i2char, \n",
    "                    max_sequence_length = 110,\n",
    "                    embedding_dimension = 512, #256,\n",
    "                    feedforward_dimension = 2048, #1024,\n",
    "                    attention_heads = 8,\n",
    "                    encoder_layers = 4,\n",
    "                    decoder_layers = 4)\n",
    "                   #dropout = .5)\n",
    "print(\"model created\")\n",
    "model.to(device)\n",
    "\n",
    "log = model.fit(train_source, \n",
    "                train_target,\n",
    "                #train_loader,\n",
    "                dev_source, \n",
    "                dev_target, \n",
    "                epochs = 3, \n",
    "                progress_bar = 2, \n",
    "                learning_rate = 10**-4,\n",
    "                save_path = \"new_torch_file_new_pages8.pt\")\n",
    "print(\"model.fit completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import matplotlib.pyplot as plt\n",
    "# # plt.style.use('ggplot')\n",
    "# class SaveBestModel:\n",
    "#     \"\"\"\n",
    "#     Class to save the best model while training. If the current epoch's \n",
    "#     validation loss is less than the previous least less, then save the\n",
    "#     model state.\n",
    "#     \"\"\"\n",
    "#     def __init__(\n",
    "#         self, best_dev_loss=float('inf')\n",
    "#     ):\n",
    "#         self.best_valid_loss = best_dev_loss\n",
    "        \n",
    "#     def __call__(\n",
    "#         self, current_dev_loss, \n",
    "#         epoch, model, optimizer, criterion\n",
    "#     ):\n",
    "#         if current_dev_loss < self.best_dev_loss:\n",
    "#             self.best_dev_loss = current_dev_loss\n",
    "#             print(f\"\\nBest validation loss: {self.best_dev_loss}\")\n",
    "#             print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch+1,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'loss': criterion,\n",
    "#                 }, 'outputs/best_model.pth')\n",
    "\n",
    "# # initialize SaveBestModel class\n",
    "# save_best_model = SaveBestModel()\n",
    "\n",
    "\n",
    "# save_best_model(\n",
    "#         valid_epoch_loss, epoch, model, optimizer, criterion\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(log['train_loss'])\n",
    "plt.plot(log['dev_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source = [list(\"abcdefghijkl\"), list(\"mnopqrstwxyz\")]\n",
    "#target = [list(\"ABCDEFGHIJKL\"), list(\"MNOPQRSTWXYZ\")] #update to astronomy vocab\n",
    "source = []yy\n",
    "for s in model.source_index:\n",
    "    sentence = \"\"\n",
    "    for c in s:\n",
    "        #nu = source_index[c]\n",
    "        #sentence.append(c)\n",
    "        #sentence.append(num)\n",
    "        sentence += c\n",
    "    source.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from nltk.lm import Vocabulary\n",
    "import pickle\n",
    "import sys\n",
    "import torch\n",
    "import importlib\n",
    "from timeit import default_timer as t\n",
    "sys.path.append(\"../../lib\")\n",
    "#from metrics import levenshtein\n",
    "import post_ocr_correction\n",
    "from pytorch_beam_search import seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = Transformer(char2i, \n",
    "                    i2char, \n",
    "                    max_sequence_length = 110,\n",
    "                    embedding_dimension = 256,\n",
    "                    feedforward_dimension = 1024,\n",
    "                    attention_heads = 8,\n",
    "                    encoder_layers = 2,\n",
    "                    decoder_layers = 2,\n",
    "                   dropout = .5)\n",
    "\n",
    "model.load_state_dict(torch.load(\"./new_torch_file60_7.pt\", map_location=torch.device('cpu')))\n",
    "#model = torch.load(\"../../data/torch_file60.pt\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./source_dataframe.csv')\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "!pip install Levenshtein\n",
    "from Levenshtein import distance\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "       \n",
    "def levenshtein(reference, hypothesis, progress_bar = False):\n",
    "    #assert len(reference) == len(hypothesis)\n",
    "    text = zip(reference, hypothesis)\n",
    "    if progress_bar:\n",
    "        text = tqdm(text, total = len(reference))\n",
    "    d = [distance(r, h) for r, h in text]\n",
    "    output = pd.DataFrame({\"reference\":reference, \"hypothesis\":hypothesis})\\\n",
    "    .assign(distance = lambda df: d)\\\n",
    "    .assign(\n",
    "        cer = lambda df: df.apply(\n",
    "            lambda r: 100 * r[\"distance\"] / max(len(r[\"reference\"]), 1), \n",
    "            axis = 1\n",
    "        )\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arr = test_data[\"ocr_to_input\"].to_numpy()\n",
    "# gs_arr = test_data[\"gs_aligned\"].to_numpy()\n",
    "ocr_list = test_data[\"ocr_aligned\"].tolist()\n",
    "gs_list = test_data[\"gs_aligned\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def uniform(j, window_size):\n",
    "    return 1.0\n",
    "\n",
    "def triangle(j, window_size):\n",
    "    m = window_size//2\n",
    "    return m - 0.5 * abs(m - j)\n",
    "\n",
    "def bell(j, window_size):\n",
    "    m = window_size // 2\n",
    "    s = window_size // 2\n",
    "    return exp(-((m-j)/s)**2)\n",
    "\n",
    "def disjoint(\n",
    "    string,\n",
    "    model, \n",
    "    source_index,\n",
    "    target_index,\n",
    "    window_size = 50,\n",
    "    decoding_method = \"greedy_search\", \n",
    "    document_progress_bar = False, \n",
    "    document_batch_progress_bar = 0, \n",
    "    *arcorrect\n",
    "):\n",
    "    model.eval()\n",
    "    windows = [string[i:i+window_size] \n",
    "        for i in range(0, len(string), window_size)]\n",
    "    windows = [\"\".join([source_index.vocabulary.lookup(c) for c in s])\\\n",
    "        .replace(\"<UNK>\", \" \") for s in windows]\n",
    "    X = source_index.text2tensor(windows, progress_bar = False).cuda()\n",
    "    if decoding_method == \"greedy_search\":\n",
    "        predictions, probs = seq2seq.greedy_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )\n",
    "    elif decoding_method == \"beam_search\":\n",
    "        predictions, probs = seq2seq.beam_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )   \n",
    "        predictions = predictions[:, 0, :]\n",
    "    output = target_index.tensor2text(predictions)\n",
    "    output = [re.sub(r\"<START>|<PAD>|<UNK>|<END>.*\", \"\", s) for s in output]\n",
    "    return \"\".join(output)\n",
    "\n",
    "def n_grams(\n",
    "    string,\n",
    "    model,\n",
    "    source_index,\n",
    "    target_index,\n",
    "    window_size = 50, \n",
    "    decoding_method = \"greedy_search\", \n",
    "    weighting = \"uniform\",\n",
    "    document_progress_bar = False, \n",
    "    document_batch_progress_bar = 0,      \n",
    "    main_batch_size = 1024,\n",
    "    *arcorrect\n",
    "):\n",
    "    model.eval()\n",
    "    if len(string) <= window_size:\n",
    "        windows = [string]\n",
    "    else:\n",
    "        windows = [string[i:i + window_size] \n",
    "            for i in range(len(string) - window_size + 1)]\n",
    "    windows = [\"\".join([source_index.vocabulary.lookup(c) for c in s])\\\n",
    "    .replace(\"<UNK>\", \" \") for s in windows]\n",
    "    X = source_index.text2tensor(windows, progress_bar = False).cuda()\n",
    "    if decoding_method == \"greedy_search\":\n",
    "        predictions, probs = seq2seq.greedy_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = False, \n",
    "            *arcorrect\n",
    "        )\n",
    "    if decoding_method == \"beam_search\":\n",
    "        predictions, probs = seq2seq.beam_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )   \n",
    "        predictions = predictions[:, 0, :]   \n",
    "    output = target_index.tensor2text(predictions)\n",
    "    output = [re.sub(r\"<START>|<PAD>|<UNK>|<END>.*\", \"\", s) for s in output]\n",
    "    if weighting == \"uniform\":\n",
    "        weighting = uniform\n",
    "    elif weighting == \"triangle\":\n",
    "        weighting = triangle\n",
    "    elif weighting == \"bell\":\n",
    "        weighting = bell\n",
    "    votes = [\n",
    "        {k:0.0 for k in target_index.vocabulary} \n",
    "        for c in string\n",
    "    ]\n",
    "    for i, s in enumerate(output):\n",
    "        for j, (counter, char)\\\n",
    "        in enumerate(zip(votes[i:i + window_size], s)):\n",
    "            counter[char] += weighting(j, window_size)\n",
    "    output = [max(c.keys(), key = lambda x: c[x]) for c in votes]\n",
    "    output = \"\".join(output)\n",
    "    return votes, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_source = []\n",
    "n = 20 # chunk length  \n",
    "#for s in arr:\n",
    "for x in range(48):#only process n paragraphs?\n",
    "    s = ocr_list[x]\n",
    "    #sentence_array = list(s) #convert string to an array of characters\n",
    "    chunks = [s[i:i+n] for i in range(0, len(s), n)]\n",
    "    for sentence_chunk in chunks:\n",
    "        new_source.append(list(sentence_chunk))#append array to new_source array\n",
    "\n",
    "    #new_source.append(list(\"stars cataclysmic  \"))#mark end of sentences with stars cataclysmic\n",
    "    new_source.append( list(\"advice charm touch  \"))#new end of sentence with 20 chars\n",
    "    #because the model only allows 20 characters at a time, each row is 20 chars chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_beam_search import seq2seq\n",
    "source_index = seq2seq.Index(source)\n",
    "target_index =  source_index\n",
    "X_new = source_index.text2tensor(new_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, log_probabilities = beam_search(\n",
    "    model,\n",
    "    X_new.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [target_index.tensor2text(p) for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output\n",
    "#output2 = [source_index.tensor2text(p) for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "predictions.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "predictions.to(device)\n",
    "device\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def uniform(j, window_size):\n",
    "    return 1.0\n",
    "\n",
    "def triangle(j, window_size):\n",
    "    m = window_size//2\n",
    "    return m - 0.5 * abs(m - j)\n",
    "\n",
    "def bell(j, window_size):\n",
    "    m = window_size // 2\n",
    "    s = window_size // 2\n",
    "    return exp(-((m-j)/s)**2)\n",
    "\n",
    "def disjoint(\n",
    "    string,\n",
    "    model, \n",
    "    source_index,\n",
    "    target_index,\n",
    "    window_size = 50,\n",
    "    decoding_method = \"greedy_search\", \n",
    "    document_progress_bar = False, \n",
    "    document_batch_progress_bar = 0, \n",
    "    *arcorrect\n",
    "):\n",
    "    model.eval()\n",
    "    windows = [string[i:i+window_size] \n",
    "        for i in range(0, len(string), window_size)]\n",
    "    windows = [\"\".join([source_index.vocabulary.lookup(c) for c in s])\\\n",
    "        .replace(\"<UNK>\", \" \") for s in windows]\n",
    "    X = source_index.text2tensor(windows, progress_bar = False).cuda()\n",
    "    if decoding_method == \"greedy_search\":\n",
    "        predictions, probs = seq2seq.greedy_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )\n",
    "    elif decoding_method == \"beam_search\":\n",
    "        predictions, probs = seq2seq.beam_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )   \n",
    "        predictions = predictions[:, 0, :]\n",
    "    output = target_index.tensor2text(predictions)\n",
    "    output = [re.sub(r\"<START>|<PAD>|<UNK>|<END>.*\", \"\", s) for s in output]\n",
    "    return \"\".join(output)\n",
    "\n",
    "def n_grams(\n",
    "    string,\n",
    "    model,\n",
    "    source_index,\n",
    "    target_index,\n",
    "    window_size = 50, \n",
    "    decoding_method = \"greedy_search\", \n",
    "    weighting = \"uniform\",\n",
    "    document_progress_bar = False, \n",
    "    document_batch_progress_bar = 0,      \n",
    "    main_batch_size = 1024,\n",
    "    *arcorrect\n",
    "):\n",
    "    model.eval()\n",
    "    if len(string) <= window_size:\n",
    "        windows = [string]\n",
    "    else:\n",
    "        windows = [string[i:i + window_size] \n",
    "            for i in range(len(string) - window_size + 1)]\n",
    "    windows = [\"\".join([source_index.vocabulary.lookup(c) for c in s])\\\n",
    "    .replace(\"<UNK>\", \" \") for s in windows]\n",
    "    X = source_index.text2tensor(windows, progress_bar = False).cuda()\n",
    "    if decoding_method == \"greedy_search\":\n",
    "        predictions, probs = seq2seq.greedy_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = False, \n",
    "            *arcorrect\n",
    "        )\n",
    "    if decoding_method == \"beam_search\":\n",
    "        predictions, probs = seq2seq.beam_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )   \n",
    "        predictions = predictions[:, 0, :]   \n",
    "    output = target_index.tensor2text(predictions)\n",
    "    output = [re.sub(r\"<START>|<PAD>|<UNK>|<END>.*\", \"\", s) for s in output]\n",
    "    if weighting == \"uniform\":\n",
    "        weighting = uniform\n",
    "    elif weighting == \"triangle\":\n",
    "        weighting = triangle\n",
    "    elif weighting == \"bell\":\n",
    "        weighting = bell\n",
    "    votes = [\n",
    "        {k:0.0 for k in target_index.vocabulary} \n",
    "        for c in string\n",
    "    ]\n",
    "    for i, s in enumerate(output):\n",
    "        for j, (counter, char)\\\n",
    "        in enumerate(zip(votes[i:i + window_size], s)):\n",
    "            counter[char] += weighting(j, window_size)\n",
    "    output = [max(c.keys(), key = lambda x: c[x]) for c in votes]\n",
    "    output = \"\".join(output)\n",
    "    return votes, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_evaluation(\n",
    "    raw, \n",
    "    gs, \n",
    "    model, \n",
    "    source_index,\n",
    "    target_index,\n",
    "    save_path = None, \n",
    "    window_size = 40, \n",
    "    document_progress_bar = False\n",
    "):\n",
    "    print(\"evaluating all methods...\")\n",
    "    metrics = []\n",
    "    old = levenshtein(reference = gs, hypothesis = raw).cer.mean()\n",
    "    # disjoint\n",
    "    print(\"  disjoint window...\")\n",
    "    print(\"    greedy_search...\")\n",
    "    start = t()\n",
    "    corrections = [\n",
    "        disjoint(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            document_progress_bar = document_progress_bar, \n",
    "            window_size = window_size\n",
    "        ) for s in raw]\n",
    "    print(\"hello\")\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"disjoint\", \n",
    "            \"decoding\":\"greedy\",\n",
    "            \"window_size\":window_size * 2,\n",
    "            \"weighting\":pd.NA,\n",
    "            \"inference_seconds\":t() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "    start = t()\n",
    "    corrections = [\n",
    "        disjoint(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            document_progress_bar = document_progress_bar, \n",
    "            window_size = window_size * 2\n",
    "        ) \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"disjoint\", \n",
    "            \"decoding\":\"greedy\",\n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":pd.NA,\n",
    "            \"inference_seconds\":t() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "    print(\"    beam_search...\")\n",
    "    start = t()\n",
    "    corrections = [\n",
    "        disjoint(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            decoding_method = \"beam_search\", \n",
    "            document_progress_bar = document_progress_bar, \n",
    "            window_size = window_size\n",
    "        ) \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"disjoint\", \n",
    "            \"decoding\":\"beam\", \n",
    "            \"window_size\":window_size * 2,\n",
    "            \"weighting\":pd.NA,\n",
    "            \"inference_seconds\":t() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "    start = t()\n",
    "    corrections = [\n",
    "        disjoint(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            decoding_method = \"beam_search\", \n",
    "            document_progress_bar = document_progress_bar, \n",
    "            window_size = window_size * 2\n",
    "        ) \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"disjoint\", \n",
    "            \"decoding\":\"beam\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":pd.NA,\n",
    "            \"inference_seconds\":t() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "    # sliding\n",
    "    print(\"  sliding\")\n",
    "    print(\"    greedy...\")\n",
    "    ## greedy search\n",
    "    print(\"      uniform...\")\n",
    "    start = t()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            weighting = uniform,\n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"greedy\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"uniform\",\n",
    "            \"inference_seconds\":t() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "\n",
    "    print(\"      triangle...\")\n",
    "    start = t()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            weighting = triangle, \n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"greedy\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"triangle\",\n",
    "            \"inference_seconds\":t() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "\n",
    "    print(\"      bell...\")\n",
    "    start = t()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            weighting = bell, \n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"greedy\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"bell\",\n",
    "            \"inference_seconds\":t() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "\n",
    "    ## beam search\n",
    "    print(\"    beam...\")\n",
    "    print(\"      uniform...\")\n",
    "    start = t()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            decoding_method = \"beam_search\", \n",
    "            weighting = uniform, \n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"beam\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"uniform\",\n",
    "            \"inference_seconds\":t() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "\n",
    "    print(\"      triangle...\")\n",
    "    start = t()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            decoding_method = \"beam_search\", \n",
    "            weighting = triangle, \n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"beam\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"triangle\",\n",
    "            \"inference_seconds\":t() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "\n",
    "    print(\"      bell...\")\n",
    "    start = t()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model.cuda(), \n",
    "            source_index,\n",
    "            target_index,\n",
    "            decoding_method = \"beam_search\", \n",
    "            weighting = bell, \n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"beam\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"bell\",\n",
    "            \"inference_seconds\":t() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "    print()\n",
    "    return pd.DataFrame(metrics).assign(\n",
    "        improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nevaluating all correction methods...\")\n",
    "evaluation = full_evaluation(\n",
    "    ocr_list,\n",
    "    gs_list,\n",
    "    model,\n",
    "    source_index,\n",
    "    target_index,\n",
    ")\n",
    "\n",
    "print(\"\\n--results--\")\n",
    "print(\"test data:\", test)\n",
    "print(\"plain beam search:\", just_beam)\n",
    "print(\"disjoint\")\n",
    "print(\"  greedy search:\", disjoint_greedy)\n",
    "print(\"  beam search:\", disjoint_beam)\n",
    "print(\"n_grams\")\n",
    "print(\"  greedy search:\", n_grams_greedy)\n",
    "print(\"  beam search:\", n_grams_beam)\n",
    "\n",
    "print(\"\\n\", evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_list #take out @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.to_csv(\"evaluation60new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_output = []\n",
    "sentence = \"\"\n",
    "for chunks in output:\n",
    "    #if chunks[0] == 'stars cataclysmic  ':#everytime it finds stars cataclysmic it outputs a new sentence #theres only 19?\n",
    "    if chunks[0] == '<START>advice charm touch  ':#everytime it finds advice charm touch   it outputs a new sentence\n",
    "        new_output.append(sentence)\n",
    "        sentence = \"\"# this empties the sentence variable whenever a sentence is added to the array\n",
    "    else:\n",
    "       new_sentence = chunks[0].replace(\"<START>\", \"\")\n",
    "       new_sentence = new_sentence.replace(\"<END>\", \"\")\n",
    "       sentence = sentence + new_sentence #combining the chunks of 20 characters together without the start and end\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "unnamed = list(range(1,49))\n",
    "#print(unnamed)\n",
    "test_data['new_output'] = new_output\n",
    "test_data['unnamed'] = unnamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data\n",
    "#test_data.to_csv('./github_output_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pybind11\n",
    "!pip install fastwer\n",
    "#!pip install pytesseract\n",
    "#!sudo apt install tesseract-ocr\n",
    "\n",
    "import cv2\n",
    "#import pytesseract\n",
    "import fastwer\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test_data.iterrows():\n",
    "  filename = row['unnamed']\n",
    "  ref = row['gs_aligned']\n",
    "  ocr = row['ocr_aligned']\n",
    "  output = row['new_output']\n",
    "  cer = fastwer.score_sent(ocr, ref, char_level=True)\n",
    "  wer = fastwer.score_sent(ocr, ref, char_level=False)\n",
    "  test_data.loc[test_data['unnamed'] == filename, 'before_cer'] = round(cer,2) # Round value to 2 decimal places\n",
    "  test_data.loc[test_data['unnamed'] == filename, 'before_wer'] = round(wer,2)\n",
    "\n",
    "#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('./cer_wer_new50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test_data.iterrows():\n",
    "  filename = row['unnamed']\n",
    "  ref = row['gs_aligned']\n",
    "  ocr = row['ocr_aligned']\n",
    "  output = row['new_output']\n",
    "  cer = fastwer.score_sent(output, ref, char_level=True)\n",
    "  wer = fastwer.score_sent(output, ref, char_level=False)\n",
    "  test_data.loc[test_data['unnamed'] == filename, 'after_cer'] = round(cer,2) # Round value to 2 decimal places\n",
    "  test_data.loc[test_data['unnamed'] == filename, 'after_wer'] = round(wer,2)\n",
    "\n",
    "#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('./cer_wer_new50epochs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "new_output = tokenize.sent_tokenize(sentence)\n",
    "#new_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs =test_data['gs_aligned']\n",
    "gs_list = ' '.join(test_data['gs_aligned'].tolist())\n",
    "#gs_array = gs.to_numpy()\n",
    "\n",
    "#print(gs_array)\n",
    "new_gs = tokenize.sent_tokenize(gs_list) # trying to take gs column to match the length of the new_output.\n",
    "len(new_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTest():   \n",
    "    input_file = './post_ocr_correction/data/en/data/test'\n",
    "    \n",
    "    # reverse is true, therefore pair[0] is ocr and pair[1] is gs\n",
    "    input_lang, output_lang, pairs = prepareData(input_file,'gs', 'ocr', True)\n",
    "    \n",
    "    output_sentences = []\n",
    "    input_sentences = []\n",
    "    corrected_sentences = []\n",
    "    for pair in pairs:\n",
    "        sentence = pair[0]  #ocr sentence\n",
    "        try:\n",
    "            output_words, attentions = evaluate(encoder1, attn_decoder1, sentence)\n",
    "            input_sentences.append(pair[1])  #gs sentence\n",
    "            output_sentences.append(pair[0]) #ocr sentence\n",
    "            corrected_sentences.append(' '.join(output_words))\n",
    " \n",
    "            #testing_df.append({'output':''.join(output_words),'input':x},ignore_index=True)\n",
    "            #showAttention(x, output_words, attentions)\n",
    "        except KeyError:\n",
    "            print('KeyError =', sentence)\n",
    "    testing_df = pd.DataFrame({\"gs\":input_sentences, \"ocr\":output_sentences, \"output\":corrected_sentences})\n",
    "    return testing_df\n",
    "testing_dataframe1 = readTest()\n",
    "testing_dataframe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_df1 = pd.DataFrame({\"output\":new_output})\n",
    "output_df1.to_csv('./output_dataframe_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTest():   \n",
    "    input_file = './post_ocr_correction/data/en/data/test'\n",
    "    \n",
    "    # reverse is true, therefore pair[0] is ocr and pair[1] is gs\n",
    "    input_lang, output_lang, pairs = prepareData(input_file,'gs', 'ocr', True)\n",
    "    \n",
    "    output_sentences = []\n",
    "    input_sentences = []\n",
    "    corrected_sentences = []\n",
    "    for pair in pairs:\n",
    "        sentence = pair[0]  #ocr sentence\n",
    "        try:\n",
    "            output_words, attentions = evaluate(encoder1, attn_decoder1, sentence)\n",
    "            input_sentences.append(pair[1])  #gs sentence\n",
    "            output_sentences.append(pair[0]) #ocr sentence\n",
    "            corrected_sentences.append(' '.join(output_words))\n",
    " \n",
    "            #testing_df.append({'output':''.join(output_words),'input':x},ignore_index=True)\n",
    "            #showAttention(x, output_words, attentions)\n",
    "        except KeyError:\n",
    "            print('KeyError =', sentence)\n",
    "    testing_df = pd.DataFrame({\"gs\":input_sentences, \"ocr\":output_sentences, \"output\":corrected_sentences})\n",
    "    return testing_df\n",
    "testing_dataframe1 = readTest()\n",
    "testing_dataframe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# #dictionary = {'GS': \"\", 'OCR': \"\", 'index'=[0]} \n",
    "# #training_df = pd.DataFrame(dictionary)\n",
    "\n",
    "# ocr =[]\n",
    "# ocr_aligned=[]\n",
    "# gs_aligned=[]\n",
    "\n",
    "# for x in range(1008,1120):\n",
    "#     for y in range(len(PDF_OUT_SENT[x])):\n",
    "#         for z in range(len(PDF_OUT_SENT[x][y])):\n",
    "#             OCR = OCR_OUT_SENT[x][y][z]\n",
    "#             OCR_aligned_train = OCR_aligned_SENT[x][y][z]\n",
    "#             GS_aligned_train = PDF_OUT_SENT[x][y][z]\n",
    "#             #dictionary = {'GS': GS_train, 'OCR': OCR_train} \n",
    "#             #training_df = pd.DataFrame(dictionary)\n",
    "#             #gs_train = training_df.append(OCR_OUT_SENT)\n",
    "#             ocr.append(OCR)\n",
    "#             ocr_aligned.append(OCR_aligned_train)\n",
    "#             gs_aligned.append(GS_aligned_train)\n",
    "#             #training_df = pd.concat([training_df, pd.DataFrame(dictionary)], ignore_index = True)\n",
    "# data_ocr = pd.DataFrame({\"ocr_to_input\":ocr}) #\"ocr_aligned\":ocr_aligned, \"gs_aligned\":gs_aligned})\n",
    "# print(data_ocr.shape)\n",
    "# data_ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, probs = model.predict(dev_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tensor2text(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tensor2text(train_target[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tensor2text(train_source[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opence-v1.5.1]",
   "language": "python",
   "name": "conda-env-opence-v1.5.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
