{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This runs our model with mBART-50 specific corrections: https://huggingface.co/facebook/mbart-large-50"
      ],
      "metadata": {
        "id": "Ya2rjz5flA0f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnTDdWhrkhSL"
      },
      "outputs": [],
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_dir = 'gdrive/MyDrive/TPDL 2023 Colab Notebooks/'\n",
        "\n",
        "# where to output models\n",
        "output_dir = main_dir + 'mBART_models/ocrOnly_large/' # math/cite/refs -- just left in as raw\n",
        "\n",
        "# where is data stored?\n",
        "aligned_dataset_dir = main_dir + 'data/alignments/'\n",
        "\n",
        "# which model do we want to start from pre-trained?\n",
        "#model_pretrained = 'google/byt5-small' # orig\n",
        "#model_pretrained = 'yelpfeast/byt5-base-english-ocr-correction' # for OCR correction specifically\n",
        "###model_pretrained = 'facebook/mbart-large-50' # mBART-50"
      ],
      "metadata": {
        "id": "kCqaqXXMmqF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv(aligned_dataset_dir+'train_masked_n500000_20230503.csv')\n",
        "eval_df = pd.read_csv(aligned_dataset_dir+'val_masked_n10000_20230503.csv')\n",
        "test_df = pd.read_csv(aligned_dataset_dir+'test_masked_n10000_20230503.csv')\n",
        "\n",
        "only_words = True"
      ],
      "metadata": {
        "id": "oxxtYvsbncTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece]==4.28.0"
      ],
      "metadata": {
        "id": "wotefPaCoxuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers"
      ],
      "metadata": {
        "id": "2bfOyWk8lkUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Order here is important!"
      ],
      "metadata": {
        "id": "ir14BJRimVE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pybind11 \n",
        "# !pip install fastwer"
      ],
      "metadata": {
        "id": "6THPU5s7l5Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import HfArgumentParser, TensorFlowBenchmark, TensorFlowBenchmarkArguments\n",
        "#import pandas as pd\n",
        "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
        "from transformers import TrainingArguments\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import Trainer\n",
        "from transformers import EarlyStoppingCallback"
      ],
      "metadata": {
        "id": "g154RiKGlb_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##import fastwer\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "WX29dDbfls7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sys import path\n",
        "path.append(main_dir + 'libraries/')\n",
        "from utils_ocr_mini import get_fill_in_types"
      ],
      "metadata": {
        "id": "3Giern-Vlfsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "cuda.empty_cache()\n",
        "print(device)"
      ],
      "metadata": {
        "id": "oJUsyo_UlZko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_formatted_columns(datain):\n",
        "    source = []\n",
        "    target = []\n",
        "    source_aligned = []\n",
        "    target_aligned = []\n",
        "    for i in range(len(datain)):\n",
        "        d = datain.iloc[i]\n",
        "        s = np.array(list(d['aligned sentences source'])) # aligned source, with ^ symbols\n",
        "        t = np.array(list(d['aligned sentences target'])) # aligned target, with @ symbols\n",
        "        a = np.array(list(get_fill_in_types(d['aligned sentences target types'])))\n",
        "        if len(s) == len(t):\n",
        "            ss = \"\".join(s[np.where( (a == ' ') | (a == 'W') | (a == 'w'))[0]].tolist())\n",
        "            tt = \"\".join(t[np.where( (a == ' ') | (a == 'W') | (a == 'w'))[0]].tolist())\n",
        "        else:\n",
        "            print('have issue, testing')\n",
        "            if t[0] == ' ' and s[0] != ' ':\n",
        "                t = np.array(list(d['aligned sentences target']))[1:] # aligned target, with @ symbols\n",
        "                a = np.array(list(get_fill_in_types(d['aligned sentences target types'])))[1:]\n",
        "                if len(s) == len(t):\n",
        "                    ss = \"\".join(s[np.where( (a == ' ') | (a == 'W') | (a == 'w'))[0]].tolist())\n",
        "                    tt = \"\".join(t[np.where( (a == ' ') | (a == 'W') | (a == 'w'))[0]].tolist())\n",
        "                else:\n",
        "                    print('not aligned, best guess')\n",
        "                    import sys; sys.exit()\n",
        "\n",
        "        source_aligned.append(ss.replace('^','@')) # align with original \n",
        "        target_aligned.append(tt)\n",
        "        source.append(ss.replace('^',''))\n",
        "        target.append(tt.replace('@',''))\n",
        "\n",
        "    datain['words source aligned'] = source_aligned\n",
        "    datain['words target aligned'] = target_aligned\n",
        "    datain['words source'] = source\n",
        "    datain['words target'] = target\n",
        "    return datain"
      ],
      "metadata": {
        "id": "nOeops60nnoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "QItVt00_lZiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if only_words:\n",
        "    train_df = add_formatted_columns(train_df)\n",
        "    eval_df = add_formatted_columns(eval_df)\n",
        "    test_df = add_formatted_columns(test_df)\n",
        "    # rename sentences we want\n",
        "    train_df = train_df.rename(columns={\"words source\": \"input_text\", \n",
        "                        \"words target\": \"target_text\"})\n",
        "    eval_df = eval_df.rename(columns={\"words source\": \"input_text\", \n",
        "                        \"words target\": \"target_text\"})\n",
        "    test_df = test_df.rename(columns={\"words source\": \"input_text\", \n",
        "                        \"words target\": \"target_text\"})\n",
        "else:\n",
        "    # rename sentences we want\n",
        "    train_df = train_df.rename(columns={\"sentences source\": \"input_text\", \n",
        "                        \"sentences target\": \"target_text\"})\n",
        "    eval_df = eval_df.rename(columns={\"sentences source\": \"input_text\", \n",
        "                        \"sentences target\": \"target_text\"})\n",
        "    test_df = test_df.rename(columns={\"sentences source\": \"input_text\", \n",
        "                        \"sentences target\": \"target_text\"})"
      ],
      "metadata": {
        "id": "oQA5YZDdlZgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args_dict = {\n",
        "    #\"model_name_or_path\": 'google/byt5-small',\n",
        "    #\"max_len\": 4096,\n",
        "    #\"max_length\": 4096,\n",
        "    \"output_dir\": output_dir,\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"per_device_train_batch_size\": 4,\n",
        "    \"per_device_eval_batch_size\": 4,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"warmup_steps\": 250,\n",
        "    \"logging_steps\": 100,\n",
        "    \"evaluation_strategy\": \"steps\",\n",
        "    \"eval_steps\": 1000,\n",
        "    \"num_train_epochs\": 4,\n",
        "    \"do_train\": True,\n",
        "    \"do_eval\": True,\n",
        "    \"fp16\": False,\n",
        "    #\"use_cache\": False,\n",
        "    \"max_steps\": 100000,\n",
        "    'save_steps':1000,\n",
        "    'save_strategy':'steps',\n",
        "    'load_best_model_at_end': True#,\n",
        "    # 'metric_for_best_model':'eval_loss',\n",
        "    # 'greater_is_better':False\n",
        "}"
      ],
      "metadata": {
        "id": "Jd1PXsovlZd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade accelerate"
      ],
      "metadata": {
        "id": "FxQxNcMjoi8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = HfArgumentParser(\n",
        "        (TrainingArguments))\n",
        "training_args = parser.parse_dict(args_dict)\n",
        "# set_seed(training_args.seed)\n",
        "args = training_args[0]"
      ],
      "metadata": {
        "id": "MWfk71QslZby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast"
      ],
      "metadata": {
        "id": "GvNihf_CtUC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install sentencepiece"
      ],
      "metadata": {
        "id": "RCGKYeZctdea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "U2hDbx3BtmfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained model and tokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\n",
        "#     model_pretrained,\n",
        "#     cache_dir=output_dir, \n",
        "#     max_length=4096\n",
        "# )\n",
        "# mbart specific\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"en_XX\")"
      ],
      "metadata": {
        "id": "-UWnONbvlZZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = T5ForConditionalGeneration.from_pretrained(\n",
        "#     model_pretrained,\n",
        "#     cache_dir=output_dir,\n",
        "# )\n",
        "# specific for mBART\n",
        "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")"
      ],
      "metadata": {
        "id": "Vbky6fbWlZXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overwriting the default max_length of 20 \n",
        "tokenizer.model_max_length=4096\n",
        "model.config.max_length=4096"
      ],
      "metadata": {
        "id": "lVetcdNVlZVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPReviewDataset(Dataset):\n",
        "    def __init__(self, Text, Label):\n",
        "        self.Text = Text\n",
        "        self.Label = Label\n",
        "        # self.tokenizer = tokenizer\n",
        "        # self.max_len = max_len\n",
        "    def __len__(self):\n",
        "        return len(self.Text)\n",
        "    def __getitem__(self, item):\n",
        "        Text = str(self.Text[item])\n",
        "        Label = self.Label[item]\n",
        "        inputs = tokenizer(Text, padding=\"max_length\", truncation=True, max_length=512)\n",
        "        outputs = tokenizer(Label, padding=\"max_length\", truncation=True, max_length=512)\n",
        "        return {\n",
        "          \"input_ids\":inputs.input_ids,\n",
        "          \"attention_mask\" : inputs.attention_mask,\n",
        "          \"labels\" : outputs.input_ids,\n",
        "          \"decoder_attention_mask\" : outputs.attention_mask,\n",
        "          # \"labels\" : lbz\n",
        "        }"
      ],
      "metadata": {
        "id": "lVv94v7BlZS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = GPReviewDataset(\n",
        "  Text=train_df.input_text.to_numpy(),\n",
        "  Label=train_df.target_text.to_numpy()\n",
        "  # tokenizer=tokenizer,\n",
        "  # max_len=max_len\n",
        ")"
      ],
      "metadata": {
        "id": "mix1mmuWlZQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_test = GPReviewDataset(\n",
        "  Text=eval_df.input_text.to_numpy(),\n",
        "  Label=eval_df.target_text.to_numpy()\n",
        "  # tokenizer=tokenizer,\n",
        "  # max_len=max_len\n",
        ")"
      ],
      "metadata": {
        "id": "XxekcQahlZOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ds_train\n",
        "valid_dataset = ds_test"
      ],
      "metadata": {
        "id": "XFQ-LlwNlZMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    # callbacks = [EarlyStoppingCallback(early_stopping_patience=10)]\n",
        "    # compute_metrics=compute_metrics\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "vKJTtQS9lZKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.args.save_total_limit = 10\n",
        "trainer.args.logging_steps = 100 # down from 100\n",
        "trainer.args.save_steps=500 # down from 10000\n",
        "#trainer.train() # put in checkpoint if need be here to load \n",
        "trainer.train(output_dir + 'checkpoint-11000') # put in checkpoint if need be here to load "
      ],
      "metadata": {
        "id": "v0mnJg8HlZIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jwBjpXo-lY-r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}