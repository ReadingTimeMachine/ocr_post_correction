{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import sys\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mcosi153\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'pytorch_beam_search' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/jarobyte91/pytorch_beam_search.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/jarobyte91/pytorch_beam_search.git\n",
      "  Cloning https://github.com/jarobyte91/pytorch_beam_search.git to /tmp/pip-req-build-i_gy48u7\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/jarobyte91/pytorch_beam_search.git /tmp/pip-req-build-i_gy48u7\n",
      "  Resolved https://github.com/jarobyte91/pytorch_beam_search.git to commit 4f6c55d51556d731f3fff49d6032fe417de63c3f\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20.3 in /opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages (from pytorch-beam-search==1.2.2) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages (from pytorch-beam-search==1.2.2) (2.8.2)\n",
      "Requirement already satisfied: pandas>=1.2.4 in /opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages (from pytorch-beam-search==1.2.2) (1.4.2)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages (from pytorch-beam-search==1.2.2) (2021.10.8)\n",
      "Requirement already satisfied: six>=1.16.0 in ./.local/lib/python3.9/site-packages (from pytorch-beam-search==1.2.2) (1.16.0)\n",
      "Requirement already satisfied: tqdm>=4.61.0 in /opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages (from pytorch-beam-search==1.2.2) (4.64.0)\n",
      "Requirement already satisfied: torch>=1.8.1 in /opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages (from pytorch-beam-search==1.2.2) (1.10.1)\n",
      "Requirement already satisfied: nltk>=3.6.5 in ./.local/lib/python3.9/site-packages (from pytorch-beam-search==1.2.2) (3.7)\n",
      "Requirement already satisfied: pytz>=2021.1 in /opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages (from pytorch-beam-search==1.2.2) (2022.1)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in ./.local/lib/python3.9/site-packages (from pytorch-beam-search==1.2.2) (4.2.0)\n",
      "Requirement already satisfied: click in /opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages (from nltk>=3.6.5->pytorch-beam-search==1.2.2) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages (from nltk>=3.6.5->pytorch-beam-search==1.2.2) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/miniconda3/envs/opence-v1.5.1/lib/python3.9/site-packages (from nltk>=3.6.5->pytorch-beam-search==1.2.2) (2022.3.15)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/jarobyte91/pytorch_beam_search.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/mcosi153/guemes/lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pytorch-beam-search\n",
      "Version: 1.2.2\n",
      "Summary: A simple library that implements search algorithms for sequence models written in PyTorch.\n",
      "Home-page: https://github.com/jarobyte91/pytorch_beam_search\n",
      "Author: Juan Ramirez-Orta\n",
      "Author-email: jarobyte91@gmail.com\n",
      "License: \n",
      "Location: /home/mcosi153/.local/lib/python3.9/site-packages\n",
      "Requires: certifi, nltk, numpy, pandas, python-dateutil, pytz, six, torch, tqdm, typing-extensions\n",
      "Required-by: post-ocr-correction\n"
     ]
    }
   ],
   "source": [
    "!pip show pytorch_beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_decoding.seq2seq import Transformer\n",
    "from pytorch_beam_search.seq2seq import Transformer, beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scratch = \"/home/jarobyte/scratch/guemes/icdar/en/\"\n",
    "scratch = \"./post_ocr_correction/data/en/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2i = pickle.load(open(scratch + \"data/char2i_new_pages2.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2char = pickle.load(open(scratch + \"data/i2char_new_pages2.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1000000\n",
    "\n",
    "dev_size = 1000000\n",
    "\n",
    "# train_size = 1000\n",
    "\n",
    "# dev_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([234611, 80])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_source = torch.load(scratch + \"data/train_source_new_pages2.pt\")[:train_size].to(device)#add to custom data\n",
    "train_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([234611, 80])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target = torch.load(scratch + \"data/train_target_new_pages2.pt\")[:train_size].to(device)\n",
    "train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([389, 80])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_source = torch.load(scratch + \"data/dev_source_new_pages2.pt\")[:dev_size].to(device)\n",
    "dev_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([389, 80])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_target = torch.load(scratch + \"data/dev_target_new_pages2.pt\")[:dev_size].to(device)\n",
    "dev_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_source[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_files = glob.glob(scratch + \"data/train_source_new0*.pt\")\n",
    "# target_files = glob.glob(scratch + \"data/train_target_new0*.pt\")\n",
    "# source_files.sort()\n",
    "# target_files.sort()\n",
    "# source_files = source_files\n",
    "# target_files = target_files\n",
    "# file_number = 0\n",
    "# train_source = torch.load(source_files[file_number])[:train_size].to(device)\n",
    "# train_target = torch.load(target_files[file_number])[:train_size].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_source' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3895282/209658776.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_source' is not defined"
     ]
    }
   ],
   "source": [
    "#train_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_target = torch.load(scratch + \"data/train_target_new.pt\")[:train_size].to(device)#\n",
    "# train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([635, 102])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dev_source = torch.load(scratch + \"data/dev_source_new_full.pt\")[:dev_size].to(device)\n",
    "# dev_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([635, 102])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dev_target = torch.load(scratch + \"data/dev_target_new_full.pt\")[:dev_size].to(device)\n",
    "# dev_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from torchvision.io import read_image\n",
    "\n",
    "# class CustomImageDataset(Dataset):\n",
    "#     def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "#         self.img_labels = pd.read_csv(annotations_file)\n",
    "#         self.img_dir = img_dir\n",
    "#         self.transform = transform\n",
    "#         self.target_transform = target_transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.img_labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "#         image = read_image(img_path)\n",
    "#         label = self.img_labels.iloc[idx, 1]\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         if self.target_transform:\n",
    "#             label = self.target_transform(label)\n",
    "#         return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        #self.source = torch.load(scratch + \"data/train_source_new.pt\")[:train_size].to(device)\n",
    "       # self.target = torch.load(scratch + \"data/train_target_new.pt\")[:train_size].to(device)\n",
    "\n",
    "        source_files = glob.glob(scratch + \"data/train_source_new0*.pt\")\n",
    "        target_files = glob.glob(scratch + \"data/train_target_new0*.pt\")\n",
    "        source_files.sort()\n",
    "        target_files.sort()\n",
    "        self.source_files = source_files\n",
    "        self.target_files = target_files\n",
    "        #self.file_number = \n",
    "        self.file_tracker = []\n",
    "        self.file_lengths = []\n",
    "        for s in source_files:\n",
    "            #print(s)\n",
    "            y = torch.load(s).to(device)\n",
    "            self.file_lengths.append(len(y))\n",
    "        print(self.file_lengths)\n",
    "        \n",
    "        #self.source = torch.load(source_files[self.file_number])[:train_size].to(device)\n",
    "        #self.target = torch.load(target_files[self.file_number])[:train_size].to(device)\n",
    "        #self.file_length = len(self.source)\n",
    "        #print(self.file_length)\n",
    "#idx is the batch index\n",
    "    def __getitem__(self, idx) -> torch.Tensor:\n",
    "        while True:\n",
    "            index = np.random.randint(len(self.source_files))\n",
    "            item_index = np.random.randint(self.file_lengths[index])\n",
    "            if (index, item_index) not in self.file_tracker:\n",
    "                self.file_tracker.append((index, item_index))\n",
    "                break\n",
    "        source_sample = torch.load(self.source_files[index])[item_index].to(device)\n",
    "        target_sample = torch.load(self.target_files[index])[item_index].to(device)\n",
    "\n",
    "        return source_sample, target_sample\n",
    "#     def __getitem__(self, idx) -> torch.Tensor:\n",
    "#         index = np.random.randint(len(self.source_files))\n",
    "#         item_index = np.random.randint(self.file_lengths[index])\n",
    "#         #print(index)\n",
    "#         #print(item_index)\n",
    "#         index_save = index\n",
    "#         item_index_save = item_index\n",
    "#         while [index_save, item_index_save] not in self.file_tracker:\n",
    "#             index_save = index\n",
    "#             item_index_save = item_index\n",
    "#             #print(index)\n",
    "#             #print(item_index)\n",
    "#             self.file_tracker.append([index_save, item_index_save])\n",
    "#             print(time.ctime())\n",
    "#             source_sample = torch.load(self.source_files[index_save])[item_index_save].to(device)\n",
    "#             target_sample = torch.load(self.target_files[index_save])[item_index_save].to(device)\n",
    "#             print(time.ctime())\n",
    "#             index = np.random.randint(len(self.source_files))\n",
    "#             item_index = np.random.randint(self.file_lengths[index])\n",
    "            \n",
    "# #         if idx < self.file_length -1:\n",
    "# #             # load one sample by index, e.g like this:\n",
    "# #             source_sample = self.source[idx]\n",
    "# #             target_sample = self.target[idx]\n",
    "# #         else:\n",
    "# #             self.file_number += 1\n",
    "# #             self.source = torch.load(source_files[self.file_number])[:train_size].to(device)\n",
    "# #             self.target = torch.load(target_files[self.file_number])[:train_size].to(device)\n",
    "# #             self.file_length = len(self.source)\n",
    "# #             source_sample = self.source[idx]\n",
    "# #             target_sample = self.target[idx]\n",
    "# #             print(\"here\")\n",
    "\n",
    "#             #pd.len() \n",
    "\n",
    "#         # do some preprocessing, convert to tensor and what not\n",
    "\n",
    "#         return source_sample, target_sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.file_lengths)\n",
    "        #return train_size\n",
    "        #return len(self.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[999130, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999131, 999130, 999130, 999130, 999130, 999130, 999130, 999131, 999130, 999130, 999130, 999130, 999130, 999131, 999130, 999130, 999130, 999130, 999130, 999131, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999131, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999131, 999130, 999130, 999130, 999131, 999130, 999130, 999130, 999130, 999131, 999131, 999130, 999131, 999130, 999130, 999130, 999131, 999130, 999131, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999130, 999131, 999130, 999130, 999130, 999131, 999130, 999130, 999130, 999130, 999131, 999131, 999130, 999130, 999130, 999130]\n"
     ]
    }
   ],
   "source": [
    "yourDataset = YourDataset()\n",
    "train_loader = torch.utils.data.DataLoader(yourDataset, batch_size= 32, num_workers=0, shuffle=False) #pin_memory=True works if data is loaded on the cpu first, and then pushed to gpu\n",
    "#from pin_memory RuntimeError: cannot pin 'torch.cuda.LongTensor' only dense CPU tensors can be pinned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2872500"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(time.ctime())\n",
    "# for i, (target, source) in enumerate(train_loader):\n",
    "#     if i%10 == 0:\n",
    "#         print(i)\n",
    "#         print(target)\n",
    "#         print(time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as tud\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module): \n",
    "    \"\"\"\n",
    "    A generic sequence-to-sequence model. All other sequence-to-sequence models should extend this class \n",
    "    with a __init__ and forward methods, in the same way as in normal PyTorch.\n",
    "    \"\"\"\n",
    "    def print_architecture(self):\n",
    "        \"\"\"\n",
    "        Displays the information about the model in standard output. \n",
    "        \"\"\"\n",
    "        for k in self.architecture.keys():\n",
    "            print(f\"{k.replace('_', ' ').capitalize()}: {self.architecture[k]}\")\n",
    "        print(f\"Trainable parameters: {sum([p.numel() for p in self.parameters()]):,}\")\n",
    "        print()\n",
    "\n",
    "    def fit(self,#train_loader, \n",
    "            X_train, \n",
    "            Y_train, \n",
    "            X_dev = None, \n",
    "            Y_dev = None, \n",
    "            batch_size = 100, \n",
    "            epochs = 5, \n",
    "            learning_rate = 10**-4, \n",
    "            weight_decay = 0, \n",
    "            progress_bar = 0, \n",
    "            save_path = None):\n",
    "        print(\"fit begins\")\n",
    "        best_dev_loss=float('inf')\n",
    "        best_epoch=float('inf')\n",
    "        \"\"\"\n",
    "        A generic training method with Adam and Cross Entropy.\n",
    "\n",
    "        Parameters\n",
    "        ----------    \n",
    "        X_train: LongTensor of shape (train_examples, train_input_length)\n",
    "            The input sequences of the training set.\n",
    "            \n",
    "        Y_train: LongTensor of shape (train_examples, train_output_length)\n",
    "            The output sequences of the training set.\n",
    "            \n",
    "        X_dev: LongTensor of shape (dev_examples, dev_input_length), optional\n",
    "            The input sequences for the development set.\n",
    "            \n",
    "        Y_train: LongTensor of shape (dev_examples, dev_output_length), optional\n",
    "            The output sequences for the development set.\n",
    "            \n",
    "        batch_size: int\n",
    "            The number of examples to process in each batch.\n",
    "\n",
    "        epochs: int\n",
    "            The number of epochs of the training process.\n",
    "            \n",
    "        learning_rate: float\n",
    "            The learning rate to use with Adam in the training process. \n",
    "            \n",
    "        weight_decay: float\n",
    "            The weight_decay parameter of Adam (L2 penalty), useful for regularizing models. For a deeper \n",
    "            documentation, go to https://pytorch.org/docs/stable/_modules/torch/optim/adam.html#Adam            \n",
    "\n",
    "        progress_bar: int\n",
    "            Shows a tqdm progress bar, useful for tracking progress with large tensors.\n",
    "            If equal to 0, no progress bar is shown. \n",
    "            If equal to 1, shows a bar with one step for every epoch.\n",
    "            If equal to 2, shows the bar when equal to 1 and also shows a bar with one step per batch for every epoch.\n",
    "            If equal to 3, shows the bars when equal to 2 and also shows a bar to track the progress of the evaluation\n",
    "            in the development set.\n",
    "            \n",
    "        save_path: string, optional\n",
    "            Path to save the .pt file containing the model parameters when the training ends.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        performance: Pandas DataFrame\n",
    "            DataFrame with the following columns: epoch, train_loss, train_error_rate, (optionally dev_loss and \n",
    "            dev_error_rate), minutes, learning_rate, weight_decay, model, encoder_embedding_dimension, \n",
    "            decoder_embedding_dimension, encoder_hidden_units, encoder_layers, decoder_hidden_units, decoder_layers, \n",
    "            dropout, parameters and one row for each of the epochs, containing information about the training process.\n",
    "        \"\"\"\n",
    "        assert X_train.shape[0] == Y_train.shape[0]\n",
    "        assert (X_dev is None and Y_dev is None) or (X_dev is not None and Y_dev is not None) \n",
    "        if (X_dev is not None and Y_dev is not None):\n",
    "            assert X_dev.shape[0] == Y_dev.shape[0]\n",
    "            dev = True\n",
    "        else:\n",
    "            dev = False\n",
    "            \n",
    "\n",
    "        train_dataset = tud.TensorDataset(X_train, Y_train)\n",
    "        train_loader = tud.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)# make own class data loader to read in batches at a time\n",
    "#         class YourDataset(torch.utils.data.Dataset):\n",
    "#             def __init__(self) -> None:\n",
    "#                 #self.source = torch.load(scratch + \"data/train_source_new.pt\")[:train_size].to(device)\n",
    "#                # self.target = torch.load(scratch + \"data/train_target_new.pt\")[:train_size].to(device)\n",
    "                \n",
    "#                 source_files = glob.glob(scratch + \"data/train_source_new0*.pt\")\n",
    "#                 target_files = glob.glob(scratch + \"data/train_target_new0*.pt\")\n",
    "#                 source_files.sort()\n",
    "#                 target_files.sort()\n",
    "#                 self.source_files = source_files\n",
    "#                 self.target_files = target_files\n",
    "#                 self.file_number = 0\n",
    "#                 self.source = torch.load(source_files[self.file_number])[:train_size].to(device)\n",
    "#                 self.target = torch.load(target_files[self.file_number])[:train_size].to(device)\n",
    "#                 self.file_length = len(self.source)\n",
    "                \n",
    "#             def __getitem__(self, idx) -> torch.Tensor:\n",
    "#                 if idx < self.file_length:\n",
    "#                     # load one sample by index, e.g like this:\n",
    "#                     source_sample = self.source[idx]\n",
    "#                     target_sample = self.target[idx]\n",
    "#                 else:\n",
    "#                     self.file_number += 1\n",
    "#                     self.source = torch.load(source_files[self.file_number])[:train_size].to(device)\n",
    "#                     self.target = torch.load(target_files[self.file_number])[:train_size].to(device)\n",
    "#                     self.file_length = len(self.source)\n",
    "#                     source_sample = self.source[idx]\n",
    "#                     target_sample = self.target[idx]\n",
    "            \n",
    "                    \n",
    "#                     #pd.len() \n",
    "\n",
    "#                 # do some preprocessing, convert to tensor and what not\n",
    "\n",
    "#                 return source_sample, target_sample\n",
    "\n",
    "#             def __len__(self):\n",
    "#                 return len(self.source)\n",
    "        #creating dataloader\n",
    "        #yourDataset = YourDataset()\n",
    "        #train_loader = torch.utils.data.DataLoader(yourDataset, batch_size= batch_size, num_workers=0, shuffle=True)\n",
    "    \n",
    "        criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "        performance = []\n",
    "        start = timer()\n",
    "        epochs_iterator = range(1, epochs + 1)\n",
    "        if progress_bar > 0:\n",
    "            epochs_iterator = tqdm(epochs_iterator)\n",
    "            print(\"Training started\")\n",
    "        print(\"X_train.shape:\", X_train.shape)\n",
    "        print(\"Y_train.shape:\", Y_train.shape)\n",
    "        if dev:\n",
    "            print(\"X_dev.shape:\", X_dev.shape)\n",
    "            print(\"Y_dev.shape:\", Y_dev.shape)\n",
    "        print(f\"Epochs: {epochs:,}\\nLearning rate: {learning_rate}\\nWeight decay: {weight_decay}\")\n",
    "        header_1 = \"Epoch | Train                \"\n",
    "        header_2 = \"      | Loss     | Error Rate\"\n",
    "        rule = \"-\" * 29\n",
    "        if dev:\n",
    "            header_1 += \" | Development          \"\n",
    "            header_2 += \" | Loss     | Error Rate\"\n",
    "            rule += \"-\" * 24\n",
    "        header_1 += \" | Minutes\"\n",
    "        header_2 += \" |\"\n",
    "        rule += \"-\" * 10\n",
    "        print(header_1, header_2, rule, sep = \"\\n\")\n",
    "        for e in epochs_iterator:\n",
    "            self.train()\n",
    "            losses = []\n",
    "            errors = []\n",
    "            sizes = []\n",
    "            train_iterator = train_loader\n",
    "            if progress_bar > 1:\n",
    "                train_iterator = tqdm(train_iterator)\n",
    "            for x, y in train_iterator:\n",
    "                # compute loss and backpropagate\n",
    "                probabilities = self.forward(x, y).transpose(1, 2)[:, :, :-1]\n",
    "                y = y[:, 1:]\n",
    "                loss = criterion(probabilities, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                # compute accuracy\n",
    "                predictions = probabilities.argmax(1)\n",
    "                batch_errors = (predictions != y)\n",
    "                # append the results\n",
    "                losses.append(loss.item())\n",
    "                errors.append(batch_errors.sum().item())\n",
    "                sizes.append(batch_errors.numel())\n",
    "            train_loss = sum(losses) / len(losses)\n",
    "            train_error_rate = 100 * sum(errors) / sum(sizes)\n",
    "            t = (timer() - start) / 60\n",
    "            status_string = f\"{e:>5} | {train_loss:>8.4f} | {train_error_rate:>10.3f}\"\n",
    "            status = {\"epoch\":e,\n",
    "                      \"train_loss\": train_loss,\n",
    "                      \"train_error_rate\": train_error_rate}\n",
    "            if dev:\n",
    "                dev_loss, dev_error_rate = self.evaluate(X_dev, \n",
    "                                                         Y_dev, \n",
    "                                                         batch_size = batch_size, \n",
    "                                                         progress_bar = progress_bar > 2, \n",
    "                                                         criterion = criterion)\n",
    "                status_string += f\" | {dev_loss:>8.4f} | {dev_error_rate:>10.3f}\"\n",
    "                status.update({\"dev_loss\": dev_loss, \"dev_error_rate\": dev_error_rate})\n",
    "            status.update({\"training_minutes\": t,\n",
    "                           \"learning_rate\": learning_rate,\n",
    "                           \"weight_decay\": weight_decay})\n",
    "            performance.append(status)\n",
    "            if save_path is not None: \n",
    "                print(\"dev =\", dev)\n",
    "                print(\"e =\", e)\n",
    "                print(\"dev loss =\", dev_loss)\n",
    "                print(\"best dev loss =\", best_dev_loss)\n",
    "                #if (not dev) or (e < 2) or (dev_loss < min([p[\"dev_loss\"] for p in performance[:-1]])):\n",
    "                if (not dev) or (e < 2) or (dev_loss < best_dev_loss):\n",
    "                    torch.save(self.state_dict(), save_path)\n",
    "                    print(status)\n",
    "                    best_dev_loss = dev_loss\n",
    "                    print(\"save path =\", save_path)\n",
    "            status_string += f\" | {t:>7.1f}\"\n",
    "            print(status_string)\n",
    "        print()\n",
    "        return pd.concat((pd.DataFrame(performance), \n",
    "                          pd.DataFrame([self.architecture for i in performance])), axis = 1)\\\n",
    "               .drop(columns = [\"source_index\", \"target_index\"])\n",
    "    \n",
    "            \n",
    "    def evaluate(self, \n",
    "                 X, \n",
    "                 Y, \n",
    "                 criterion = nn.CrossEntropyLoss(), \n",
    "                 batch_size = 128, \n",
    "                 progress_bar = False):\n",
    "        \"\"\"\n",
    "        Evaluates the model on a dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: LongTensor of shape (examples, input_length)\n",
    "            The input sequences of the dataset.\n",
    "            \n",
    "        Y: LongTensor of shape (examples, output_length)\n",
    "            The output sequences of the dataset.\n",
    "            \n",
    "        criterion: PyTorch module\n",
    "            The loss function to evalue the model on the dataset, has to be able to compare self.forward(X, Y) and Y\n",
    "            to produce a real number.\n",
    "            \n",
    "        batch_size: int\n",
    "            The batch size of the evaluation loop.\n",
    "            \n",
    "        progress_bar: bool\n",
    "            Shows a tqdm progress bar, useful for tracking progress with large tensors.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        loss: float\n",
    "            The average of criterion across the whole dataset.\n",
    "            \n",
    "        error_rate: float\n",
    "            The step-by-step accuracy of the model across the whole dataset. Useful as a sanity check, as it should\n",
    "            go to zero as the loss goes to zero.\n",
    "            \n",
    "        \"\"\"\n",
    "        dataset = tud.TensorDataset(X, Y)\n",
    "        loader = tud.DataLoader(dataset, batch_size = batch_size)\n",
    "        self.eval()\n",
    "        losses = []\n",
    "        errors = []\n",
    "        sizes = []\n",
    "        with torch.no_grad():\n",
    "            iterator = iter(loader)\n",
    "            if progress_bar:\n",
    "                iterator = tqdm(iterator)\n",
    "            for batch in iterator:\n",
    "                x, y = batch\n",
    "                # compute loss\n",
    "                probabilities = self.forward(x, y).transpose(1, 2)[:, :, :-1]\n",
    "                y = y[:, 1:]\n",
    "                loss = criterion(probabilities, y)\n",
    "                # compute accuracy\n",
    "                predictions = probabilities.argmax(1)\n",
    "                batch_errors = (predictions != y)\n",
    "                # append the results\n",
    "                losses.append(loss.item())\n",
    "                errors.append(batch_errors.sum().item())\n",
    "                sizes.append(batch_errors.numel())\n",
    "            loss = sum(losses) / len(losses)\n",
    "            error_rate = 100 * sum(errors) / sum(sizes)\n",
    "        return loss, error_rate \n",
    "    \n",
    "class LSTM(Seq2Seq):\n",
    "    def __init__(self, \n",
    "                 source_index, \n",
    "                 target_index, \n",
    "                 encoder_embedding_dimension = 32,\n",
    "                 decoder_embedding_dimension = 32,\n",
    "                 encoder_hidden_units = 128, \n",
    "                 encoder_layers = 2,\n",
    "                 decoder_hidden_units = 128,\n",
    "                 decoder_layers = 2,\n",
    "                 dropout = 0.0):\n",
    "        \"\"\"\n",
    "        A standard Seq2Seq LSTM model as in 'Learning Phrase Representations using RNN Encoder-Decoder \n",
    "        for Statistical Machine Translation' by Cho et al. (2014). \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        in_vocabulary: dictionary\n",
    "            Vocabulary with the index:token pairs for the inputs of the model.\n",
    "            \n",
    "        out_vocabulary: dictionary\n",
    "            Vocabulary with the token:index pairs for the outputs of the model.\n",
    "            \n",
    "        encoder_embedding_dimension: int\n",
    "            Dimension of the embeddings to feed into the encoder.\n",
    "            \n",
    "        decoder_embedding_dimension: int\n",
    "            Dimension of the embeddings to feed into the decoder.\n",
    "            \n",
    "        encoder_hidden_units: int\n",
    "            Hidden size of the encoder.\n",
    "            \n",
    "        encoder_layers: int\n",
    "            Hidden layers of the encoder.\n",
    "            \n",
    "        decoder_hidden_units: int\n",
    "            Hidden units of the decoder.\n",
    "            \n",
    "        decoder_layers: int\n",
    "            Hidden layers of the decoder.\n",
    "            \n",
    "        dropout: float between 0.0 and 1.0\n",
    "            Dropout rate to apply to whole model.\n",
    "        \"\"\"\n",
    "        self.source_index = source_index\n",
    "        self.target_index = target_index\n",
    "        super().__init__()\n",
    "        self.source_embeddings = nn.Embedding(len(source_index), encoder_embedding_dimension)\n",
    "        self.target_embeddings = nn.Embedding(len(target_index), decoder_embedding_dimension)\n",
    "        self.encoder_rnn = nn.LSTM(input_size = encoder_embedding_dimension, \n",
    "                                   hidden_size = encoder_hidden_units, \n",
    "                                   num_layers = encoder_layers,\n",
    "                                   dropout = dropout)\n",
    "        self.decoder_rnn = nn.LSTM(input_size = encoder_layers * encoder_hidden_units + decoder_embedding_dimension, \n",
    "                                   hidden_size = decoder_hidden_units, \n",
    "                                   num_layers = decoder_layers,\n",
    "                                   dropout = dropout)\n",
    "        self.output_layer = nn.Linear(decoder_hidden_units, len(target_index))\n",
    "        self.architecture = dict(model = \"Seq2Seq LSTM\",\n",
    "                                 source_index = source_index, \n",
    "                                 target_index = target_index, \n",
    "                                 encoder_embedding_dimension = encoder_embedding_dimension,\n",
    "                                 decoder_embedding_dimension = decoder_embedding_dimension,\n",
    "                                 encoder_hidden_units = encoder_hidden_units, \n",
    "                                 encoder_layers = encoder_layers,\n",
    "                                 decoder_hidden_units = decoder_hidden_units,\n",
    "                                 decoder_layers = decoder_layers,\n",
    "                                 dropout = dropout)\n",
    "        self.print_architecture()\n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"\n",
    "        Forward method of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: LongTensor of shape (batch_size, input_length)\n",
    "            Tensor of integers containing the inputs for the model.\n",
    "            \n",
    "        Y: LongTensor of shape (batch_size, output_length)\n",
    "            Tensor of integers containing the output produced so far.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        output: FloatTensor of shape (batch_size, output_length, len(out_vocabulary))\n",
    "            Tensor of floats containing the inputs for the final Softmax layer (usually integrated into the loss function).\n",
    "        \"\"\"\n",
    "        X = self.source_embeddings(X.T)\n",
    "        encoder, (encoder_last_hidden, encoder_last_memory) = self.encoder_rnn(X)\n",
    "        encoder_last_hidden = encoder_last_hidden.transpose(0, 1).flatten(start_dim = 1)\n",
    "        encoder_last_hidden = encoder_last_hidden.repeat((Y.shape[1], 1, 1))\n",
    "        Y = self.target_embeddings(Y.T)\n",
    "        Y = torch.cat((Y, encoder_last_hidden), axis = -1)\n",
    "        decoder, (decoder_last_hidden, decoder_last_memory) = self.decoder_rnn(Y)\n",
    "        output = self.output_layer(decoder.transpose(0, 1))\n",
    "        return output        \n",
    "    \n",
    "    \n",
    "class ReversingLSTM(Seq2Seq):\n",
    "    def __init__(self, \n",
    "                 source_index, \n",
    "                 target_index, \n",
    "                 encoder_embedding_dimension = 32,\n",
    "                 decoder_embedding_dimension = 32,\n",
    "                 encoder_hidden_units = 128, \n",
    "                 encoder_layers = 2,\n",
    "                 decoder_hidden_units = 128,\n",
    "                 decoder_layers = 2,\n",
    "                 dropout = 0.0):\n",
    "        \"\"\"\n",
    "        A standard Seq2Seq LSTM model that reverses the order of the input as in \n",
    "        'Sequence to sequence learning with Neural Networks' by Sutskever et al. (2014). \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        in_vocabulary: dictionary\n",
    "            Vocabulary with the index:token pairs for the inputs of the model.\n",
    "            \n",
    "        out_vocabulary: dictionary\n",
    "            Vocabulary with the token:index pairs for the outputs of the model.\n",
    "            \n",
    "        encoder_embedding_dimension: int\n",
    "            Dimension of the embeddings to feed into the encoder.\n",
    "            \n",
    "        decoder_embedding_dimension: int\n",
    "            Dimension of the embeddings to feed into the decoder.\n",
    "            \n",
    "        encoder_hidden_units: int\n",
    "            Hidden size of the encoder.\n",
    "            \n",
    "        encoder_layers: int\n",
    "            Hidden layers of the encoder.\n",
    "            \n",
    "        decoder_hidden_units: int\n",
    "            Hidden units of the decoder.\n",
    "            \n",
    "        decoder_layers: int\n",
    "            Hidden layers of the decoder.\n",
    "            \n",
    "        dropout: float between 0.0 and 1.0\n",
    "            Dropout rate to apply to whole model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.source_index = source_index\n",
    "        self.target_index = target_index\n",
    "        self.source_embeddings = nn.Embedding(len(source_index), encoder_embedding_dimension)\n",
    "        self.target_embeddings = nn.Embedding(len(target_index), decoder_embedding_dimension)\n",
    "        self.encoder_rnn = nn.LSTM(input_size = encoder_embedding_dimension, \n",
    "                                   hidden_size = encoder_hidden_units, \n",
    "                                   num_layers = encoder_layers,\n",
    "                                   dropout = dropout)\n",
    "        self.decoder_rnn = nn.LSTM(input_size = decoder_embedding_dimension, \n",
    "                                   hidden_size = decoder_hidden_units, \n",
    "                                   num_layers = decoder_layers,\n",
    "                                   dropout = dropout)\n",
    "        self.output_layer = nn.Linear(decoder_hidden_units, len(target_index))\n",
    "        self.enc2dec = nn.Linear(encoder_hidden_units * encoder_layers, decoder_hidden_units * decoder_layers)\n",
    "        self.architecture = dict(model = \"Seq2Seq Reversing LSTM\",\n",
    "                                 source_index = source_index, \n",
    "                                 target_index = target_index, \n",
    "                                 encoder_embedding_dimension = encoder_embedding_dimension,\n",
    "                                 decoder_embedding_dimension = decoder_embedding_dimension,\n",
    "                                 encoder_hidden_units = encoder_hidden_units, \n",
    "                                 encoder_layers = encoder_layers,\n",
    "                                 decoder_hidden_units = decoder_hidden_units,\n",
    "                                 decoder_layers = decoder_layers,\n",
    "                                 dropout = dropout)\n",
    "        self.print_architecture()\n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"\n",
    "        Forward method of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: LongTensor of shape (batch_size, input_length)\n",
    "            Tensor of integers containing the inputs for the model.\n",
    "            \n",
    "        Y: LongTensor of shape (batch_size, output_length)\n",
    "            Tensor of integers containing the output produced so far.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        output: FloatTensor of shape (batch_size, output_length, len(out_vocabulary))\n",
    "            Tensor of floats containing the inputs for the final Softmax layer (usually integrated into the loss function).\n",
    "        \"\"\"\n",
    "        X = self.source_embeddings(torch.flip(X.T, dims = (1, )))\n",
    "        encoder, (encoder_last_hidden, encoder_last_memory) = self.encoder_rnn(X)\n",
    "        encoder_last_hidden = encoder_last_hidden.transpose(0, 1).flatten(start_dim = 1)\n",
    "        enc2dec = self.enc2dec(encoder_last_hidden)\\\n",
    "        .reshape(-1, self.decoder_rnn.num_layers, self.decoder_rnn.hidden_size)\\\n",
    "        .transpose(0, 1)\\\n",
    "        .contiguous()\n",
    "        Y = self.target_embeddings(Y.T)\n",
    "        decoder, (decoder_last_hidden, decoder_last_memory) = self.decoder_rnn(Y, (enc2dec, torch.zeros_like(enc2dec)))\n",
    "        output = self.output_layer(decoder.transpose(0, 1))\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class Transformer(Seq2Seq):\n",
    "    def __init__(self, \n",
    "                 source_index, \n",
    "                 target_index,\n",
    "                 max_sequence_length = 32,\n",
    "                 embedding_dimension = 32,\n",
    "                 feedforward_dimension = 128,\n",
    "                 encoder_layers = 2,\n",
    "                 decoder_layers = 2,\n",
    "                 attention_heads = 2,\n",
    "                 activation = \"relu\",\n",
    "                 dropout = 0.0):\n",
    "        \"\"\"\n",
    "        The standard PyTorch implementation of a Transformer model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        in_vocabulary: dictionary\n",
    "            Vocabulary with the index:token pairs for the inputs of the model.\n",
    "            \n",
    "        out_vocabulary: dictionary\n",
    "            Vocabulary with the token:index pairs for the outputs of the model.\n",
    "            \n",
    "        max_sequence_length: int\n",
    "            Maximum sequence length accepted by the model, both for the encoder and the decoder.\n",
    "            \n",
    "        embedding_dimension: int\n",
    "            Dimension of the embeddings of the model.\n",
    "            \n",
    "        feedforward_dimension: int\n",
    "            Dimension of the feedforward network inside the self-attention layers of the model.\n",
    "            \n",
    "        encoder_layers: int\n",
    "            Hidden layers of the encoder.\n",
    "            \n",
    "        decoder_layers: int\n",
    "            Hidden layers of the decoder.\n",
    "            \n",
    "        attention_heads: int\n",
    "            Attention heads inside every self-attention layer of the model.\n",
    "            \n",
    "        activation: string\n",
    "            Activation function of the feedforward network inside the self-attention layers of the model. Can\n",
    "            be either 'relu' or 'gelu'.\n",
    "            \n",
    "        dropout: float between 0.0 and 1.0\n",
    "            Dropout rate to apply to whole model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.source_index = source_index\n",
    "        self.target_index = target_index\n",
    "        self.source_embeddings = nn.Embedding(len(source_index), embedding_dimension)\n",
    "        self.target_embeddings = nn.Embedding(len(target_index), embedding_dimension)\n",
    "        self.positional_embeddings = nn.Embedding(max_sequence_length, embedding_dimension)\n",
    "        self.transformer = nn.Transformer(d_model = embedding_dimension, \n",
    "                                          dim_feedforward = feedforward_dimension,\n",
    "                                          nhead = attention_heads, \n",
    "                                          num_encoder_layers = encoder_layers, \n",
    "                                          num_decoder_layers = decoder_layers,\n",
    "                                          activation = activation,\n",
    "                                          dropout = dropout)\n",
    "        self.output_layer = nn.Linear(embedding_dimension, len(target_index))\n",
    "        self.architecture = dict(model = \"Seq2Seq Transformer\",\n",
    "                                 source_index = source_index,\n",
    "                                 target_index = target_index,\n",
    "                                 max_sequence_length = max_sequence_length,\n",
    "                                 embedding_dimension = embedding_dimension,\n",
    "                                 feedforward_dimension = feedforward_dimension,\n",
    "                                 encoder_layers = encoder_layers,\n",
    "                                 decoder_layers = decoder_layers,\n",
    "                                 attention_heads = attention_heads,\n",
    "                                 activation = activation,\n",
    "                                 dropout = dropout)\n",
    "        self.print_architecture()\n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"\n",
    "        Forward method of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: LongTensor of shape (batch_size, input_length)\n",
    "            Tensor of integers containing the inputs for the model.\n",
    "            \n",
    "        Y: LongTensor of shape (batch_size, output_length)\n",
    "            Tensor of integers containing the output produced so far.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        output: FloatTensor of shape (batch_size, output_length, len(out_vocabulary))\n",
    "            Tensor of floats containing the inputs for the final Softmax layer (usually integrated in the loss function).\n",
    "        \"\"\"\n",
    "        assert X.shape[1] <= self.architecture[\"max_sequence_length\"]\n",
    "        assert Y.shape[1] <= self.architecture[\"max_sequence_length\"]\n",
    "        X = self.source_embeddings(X)\n",
    "        X_positional = torch.arange(X.shape[1], device = X.device).repeat((X.shape[0], 1))\n",
    "        X_positional = self.positional_embeddings(X_positional)\n",
    "        X = (X + X_positional).transpose(0, 1)\n",
    "        Y = self.target_embeddings(Y)\n",
    "        Y_positional = torch.arange(Y.shape[1], device = Y.device).repeat((Y.shape[0], 1))\n",
    "        Y_positional = self.positional_embeddings(Y_positional)\n",
    "        Y = (Y + Y_positional).transpose(0, 1)\n",
    "        mask = self.transformer.generate_square_subsequent_mask(Y.shape[0]).to(Y.device)\n",
    "        transformer_output = self.transformer.forward(src = X,\n",
    "                                                      tgt = Y, \n",
    "                                                      tgt_mask = mask)\n",
    "        transformer_output = transformer_output.transpose(0, 1)\n",
    "        return self.output_layer(transformer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Seq2Seq Transformer\n",
      "Source index: {' ': 3, ',': 4, '-': 5, '.': 6, '<UNK>': 7, '@': 8, 'A': 9, 'M': 10, 'P': 11, 'S': 12, 'a': 13, 'b': 14, 'c': 15, 'd': 16, 'e': 17, 'f': 18, 'g': 19, 'h': 20, 'i': 21, 'l': 22, 'm': 23, 'n': 24, 'o': 25, 'p': 26, 'r': 27, 's': 28, 't': 29, 'u': 30, 'v': 31, 'w': 32, 'x': 33, 'y': 34, 'z': 35, 'À': 36, '<PAD>': 0, '<START>': 1, '<END>': 2}\n",
      "Target index: {3: ' ', 4: ',', 5: '-', 6: '.', 7: '<UNK>', 8: '@', 9: 'A', 10: 'M', 11: 'P', 12: 'S', 13: 'a', 14: 'b', 15: 'c', 16: 'd', 17: 'e', 18: 'f', 19: 'g', 20: 'h', 21: 'i', 22: 'l', 23: 'm', 24: 'n', 25: 'o', 26: 'p', 27: 'r', 28: 's', 29: 't', 30: 'u', 31: 'v', 32: 'w', 33: 'x', 34: 'y', 35: 'z', 36: 'À', 0: '<PAD>', 1: '<START>', 2: '<END>'}\n",
      "Max sequence length: 110\n",
      "Embedding dimension: 512\n",
      "Feedforward dimension: 2048\n",
      "Encoder layers: 4\n",
      "Decoder layers: 4\n",
      "Attention heads: 8\n",
      "Activation: relu\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 29,540,901\n",
      "\n",
      "model created\n",
      "fit begins\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb456f5e3b3049049ad2ca4986dafc2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "X_train.shape: torch.Size([234611, 80])\n",
      "Y_train.shape: torch.Size([234611, 80])\n",
      "X_dev.shape: torch.Size([389, 80])\n",
      "Y_dev.shape: torch.Size([389, 80])\n",
      "Epochs: 40\n",
      "Learning rate: 0.0001\n",
      "Weight decay: 0\n",
      "Epoch | Train                 | Development           | Minutes\n",
      "      | Loss     | Error Rate | Loss     | Error Rate |\n",
      "---------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5cfc8c56f684edca9b0f59913c79396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 1\n",
      "dev loss = 0.00011280814760539215\n",
      "best dev loss = inf\n",
      "{'epoch': 1, 'train_loss': 0.07292129002921845, 'train_error_rate': 50.82453481170474, 'dev_loss': 0.00011280814760539215, 'dev_error_rate': 50.11877257492434, 'training_minutes': 6.649431679650054, 'learning_rate': 0.0001, 'weight_decay': 0}\n",
      "save path = new_torch_file_new_pages10.pt\n",
      "    1 |   0.0729 |     50.825 |   0.0001 |     50.119 |     6.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75526360bf9044648e06ab6b1c522866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 2\n",
      "dev loss = 2.488751624696306e-05\n",
      "best dev loss = 0.00011280814760539215\n",
      "{'epoch': 2, 'train_loss': 5.272582491428035e-05, 'train_error_rate': 49.781288919460486, 'dev_loss': 2.488751624696306e-05, 'dev_error_rate': 50.11877257492434, 'training_minutes': 13.343578684485207, 'learning_rate': 0.0001, 'weight_decay': 0}\n",
      "save path = new_torch_file_new_pages10.pt\n",
      "    2 |   0.0001 |     49.781 |   0.0000 |     50.119 |    13.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349c3fc0eff14677be5ccd74f48805c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 3\n",
      "dev loss = 7.821887720638188e-06\n",
      "best dev loss = 2.488751624696306e-05\n",
      "{'epoch': 3, 'train_loss': 1.388599285003886e-05, 'train_error_rate': 49.781288919460486, 'dev_loss': 7.821887720638188e-06, 'dev_error_rate': 50.11877257492434, 'training_minutes': 20.03031948446684, 'learning_rate': 0.0001, 'weight_decay': 0}\n",
      "save path = new_torch_file_new_pages10.pt\n",
      "    3 |   0.0000 |     49.781 |   0.0000 |     50.119 |    20.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0cca8a5c0844f49b647c155e50a1037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 4\n",
      "dev loss = 2.5188855943270028e-06\n",
      "best dev loss = 7.821887720638188e-06\n",
      "{'epoch': 4, 'train_loss': 4.544835540117255e-06, 'train_error_rate': 49.781288919460486, 'dev_loss': 2.5188855943270028e-06, 'dev_error_rate': 50.11877257492434, 'training_minutes': 26.727525079836294, 'learning_rate': 0.0001, 'weight_decay': 0}\n",
      "save path = new_torch_file_new_pages10.pt\n",
      "    4 |   0.0000 |     49.781 |   0.0000 |     50.119 |    26.7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58eca1aa2a142d7903ce6b8505825a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 5\n",
      "dev loss = 7.378948509995098e-07\n",
      "best dev loss = 2.5188855943270028e-06\n",
      "{'epoch': 5, 'train_loss': 1.368194302962401e-06, 'train_error_rate': 49.781288919460486, 'dev_loss': 7.378948509995098e-07, 'dev_error_rate': 50.11877257492434, 'training_minutes': 33.43373896055079, 'learning_rate': 0.0001, 'weight_decay': 0}\n",
      "save path = new_torch_file_new_pages10.pt\n",
      "    5 |   0.0000 |     49.781 |   0.0000 |     50.119 |    33.4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51aed0a70c5d4b28a4e1a5afde71d193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 6\n",
      "dev loss = 2.403916887772084e-07\n",
      "best dev loss = 7.378948509995098e-07\n",
      "{'epoch': 6, 'train_loss': 4.2133226586944765e-07, 'train_error_rate': 49.781288919460486, 'dev_loss': 2.403916887772084e-07, 'dev_error_rate': 50.11877257492434, 'training_minutes': 40.15041704730053, 'learning_rate': 0.0001, 'weight_decay': 0}\n",
      "save path = new_torch_file_new_pages10.pt\n",
      "    6 |   0.0000 |     49.781 |   0.0000 |     50.119 |    40.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b63b83ec2949228fe48494d479b8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 7\n",
      "dev loss = 2.9550098180770874\n",
      "best dev loss = 2.403916887772084e-07\n",
      "    7 |   1.9599 |     78.021 |   2.9550 |     92.093 |    46.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8d03053c2a45d0ac2bc70151996f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 8\n",
      "dev loss = 0.14205220714211464\n",
      "best dev loss = 2.403916887772084e-07\n",
      "    8 |   0.3073 |     54.683 |   0.1421 |     52.524 |    53.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681e6032d6024407a22419247fa9a503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 9\n",
      "dev loss = 4.140516193729127e-05\n",
      "best dev loss = 2.403916887772084e-07\n",
      "    9 |   0.0442 |     50.538 |   0.0000 |     50.119 |    60.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27be6df838b4584bc584587e807db5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 10\n",
      "dev loss = 8.74797979122377e-06\n",
      "best dev loss = 2.403916887772084e-07\n",
      "   10 |   0.0000 |     49.781 |   0.0000 |     50.119 |    66.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101807832ff645cfa8615be0a6886bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 11\n",
      "dev loss = 2.7515846454662096e-06\n",
      "best dev loss = 2.403916887772084e-07\n",
      "   11 |   0.0000 |     49.781 |   0.0000 |     50.119 |    73.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15917e01a7dc4da18f5dad175b155d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 12\n",
      "dev loss = 9.471660291637818e-07\n",
      "best dev loss = 2.403916887772084e-07\n",
      "   12 |   0.0000 |     49.781 |   0.0000 |     50.119 |    80.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd34ec8b2e84601a30a3915d789eb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 13\n",
      "dev loss = 3.283863279079924e-07\n",
      "best dev loss = 2.403916887772084e-07\n",
      "   13 |   0.0000 |     49.781 |   0.0000 |     50.119 |    86.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6d0ef6d02e42c58a4f5310b889d136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 14\n",
      "dev loss = 1.1540192268455485e-07\n",
      "best dev loss = 2.403916887772084e-07\n",
      "{'epoch': 14, 'train_loss': 1.9920172023439284e-07, 'train_error_rate': 49.781288919460486, 'dev_loss': 1.1540192268455485e-07, 'dev_error_rate': 50.11877257492434, 'training_minutes': 93.61799415278558, 'learning_rate': 0.0001, 'weight_decay': 0}\n",
      "save path = new_torch_file_new_pages10.pt\n",
      "   14 |   0.0000 |     49.781 |   0.0000 |     50.119 |    93.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5853f42525654564aa97c2b8ac01ef80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 15\n",
      "dev loss = 4.407500231451422e-08\n",
      "best dev loss = 1.1540192268455485e-07\n",
      "{'epoch': 15, 'train_loss': 6.907831884577405e-08, 'train_error_rate': 49.781288919460486, 'dev_loss': 4.407500231451422e-08, 'dev_error_rate': 50.11877257492434, 'training_minutes': 100.32238940151873, 'learning_rate': 0.0001, 'weight_decay': 0}\n",
      "save path = new_torch_file_new_pages10.pt\n",
      "   15 |   0.0000 |     49.781 |   0.0000 |     50.119 |   100.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e92d7bb9aaf43f1bf3c3ef05ef99006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 16\n",
      "dev loss = 2.9611060270440248e-08\n",
      "best dev loss = 4.407500231451422e-08\n",
      "{'epoch': 16, 'train_loss': 3.2030423991496e-08, 'train_error_rate': 49.781288919460486, 'dev_loss': 2.9611060270440248e-08, 'dev_error_rate': 50.11877257492434, 'training_minutes': 107.0250778087182, 'learning_rate': 0.0001, 'weight_decay': 0}\n",
      "save path = new_torch_file_new_pages10.pt\n",
      "   16 |   0.0000 |     49.781 |   0.0000 |     50.119 |   107.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53869edb842d4a4f876a151c28c69aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 17\n",
      "dev loss = 3.019646666047038e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   17 |   0.0000 |     49.781 |   0.0000 |     50.119 |   113.7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387119dd8b5942d9b8fad73ec11a89df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 18\n",
      "dev loss = 6.008923758571427e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   18 |   0.0000 |     49.781 |   0.0000 |     50.119 |   120.4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9188ba42b8344cc6a07469aa09d77107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 19\n",
      "dev loss = 6.882601777391528e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   19 |   0.0000 |     49.781 |   0.0000 |     50.119 |   127.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b694e56f054e67a3015e621493c936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 20\n",
      "dev loss = 7.058389073222315e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   20 |   0.0000 |     49.781 |   0.0000 |     50.119 |   133.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2bcc1a88c984c53865738d27b2a2d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 21\n",
      "dev loss = 4.888917004564064e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   21 |   0.0000 |     49.781 |   0.0000 |     50.119 |   140.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d16e3a4f8f4029be93d68a12488749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 22\n",
      "dev loss = 1.2338778176967935e-07\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   22 |   0.0000 |     49.781 |   0.0000 |     50.119 |   147.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ec04732fca45aa89fd5198cc502363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 23\n",
      "dev loss = 7.984447858433441e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   23 |   0.0000 |     49.781 |   0.0000 |     50.119 |   153.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c77b29f127f4daab8894b61152fbda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 24\n",
      "dev loss = 4.825802601970963e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   24 |   0.0000 |     49.781 |   0.0000 |     50.119 |   160.7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6fd819d3d7847c1b9ca3538da22d27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 25\n",
      "dev loss = 7.230555176818143e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   25 |   0.0000 |     49.781 |   0.0000 |     50.119 |   167.4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009cb40578624d918a2e13098b51be0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 26\n",
      "dev loss = 6.510746519694521e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   26 |   0.0000 |     49.781 |   0.0000 |     50.119 |   174.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff3afe5050a457c8cb74ebb7951cad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 27\n",
      "dev loss = 1.1083634632313988e-07\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   27 |   0.0000 |     49.781 |   0.0000 |     50.119 |   180.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ee8ede97704f07b90102ddf0d73079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 28\n",
      "dev loss = 7.083097841587005e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   28 |   0.0000 |     49.781 |   0.0000 |     50.119 |   187.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e630caba3c4b34aab3158e671f03ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 29\n",
      "dev loss = 1.0986292231507377e-07\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   29 |   0.0000 |     49.781 |   0.0000 |     50.119 |   194.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03282b8256674a80a4278bee922a489f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 30\n",
      "dev loss = 7.671561785826952e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   30 |   0.0000 |     49.781 |   0.0000 |     50.119 |   201.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0c0090b7514c6ea65611e8128974b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 31\n",
      "dev loss = 3.896167299899389e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   31 |   0.0000 |     49.781 |   0.0000 |     50.119 |   207.7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3b8dc400e04eb49c090b5fc41c815e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 32\n",
      "dev loss = 7.453438577442739e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   32 |   0.0000 |     49.781 |   0.0000 |     50.119 |   214.4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75eeb72cf959406b90ac9535ed9f9114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 33\n",
      "dev loss = 6.147334286055184e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   33 |   0.0000 |     49.781 |   0.0000 |     50.119 |   221.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96decfae10744bf9ad93fcfeb357a61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 34\n",
      "dev loss = 5.505759315127534e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   34 |   0.0000 |     49.781 |   0.0000 |     50.119 |   227.7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f646aba287d74597b5e2e216a6e88bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 35\n",
      "dev loss = 7.665584433880213e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   35 |   0.0000 |     49.781 |   0.0000 |     50.119 |   234.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6674dfb5256d4a4f8d4f30dfea8f659e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 36\n",
      "dev loss = 8.883574942331052e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   36 |   0.0000 |     49.781 |   0.0000 |     50.119 |   241.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7827e2c0d3e451cabc9dabd21e3eb4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 37\n",
      "dev loss = 4.738592762620897e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   37 |   0.0000 |     49.781 |   0.0000 |     50.119 |   247.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099183cb70984c40aea3dda149a2c4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 38\n",
      "dev loss = 7.06719394116817e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   38 |   0.0000 |     49.781 |   0.0000 |     50.119 |   254.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f8df79e6c9438e8b4221733240916c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 39\n",
      "dev loss = 5.4120442793959e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   39 |   0.0000 |     49.781 |   0.0000 |     50.119 |   261.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e323090d28ef42ea9b93c39e3c23f462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev = True\n",
      "e = 40\n",
      "dev loss = 5.749160969514833e-08\n",
      "best dev loss = 2.9611060270440248e-08\n",
      "   40 |   0.0000 |     49.781 |   0.0000 |     50.119 |   268.1\n",
      "\n",
      "model.fit completed\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(char2i, \n",
    "                    i2char, \n",
    "                    max_sequence_length = 110,\n",
    "                    embedding_dimension = 512, #256,\n",
    "                    feedforward_dimension = 2048, #1024,\n",
    "                    attention_heads = 8,\n",
    "                    encoder_layers = 4,\n",
    "                    decoder_layers = 4)\n",
    "                   #dropout = .5)\n",
    "print(\"model created\")\n",
    "model.to(device)\n",
    "\n",
    "log = model.fit(train_source, \n",
    "                train_target,\n",
    "                #train_loader,\n",
    "                dev_source, \n",
    "                dev_target, \n",
    "                epochs = 40, \n",
    "                progress_bar = 2, \n",
    "                learning_rate = 10**-4,\n",
    "                save_path = \"new_torch_file_new_pages10.pt\")\n",
    "print(\"model.fit completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import matplotlib.pyplot as plt\n",
    "# # plt.style.use('ggplot')\n",
    "# class SaveBestModel:\n",
    "#     \"\"\"\n",
    "#     Class to save the best model while training. If the current epoch's \n",
    "#     validation loss is less than the previous least less, then save the\n",
    "#     model state.\n",
    "#     \"\"\"\n",
    "#     def __init__(\n",
    "#         self, best_dev_loss=float('inf')\n",
    "#     ):\n",
    "#         self.best_valid_loss = best_dev_loss\n",
    "        \n",
    "#     def __call__(\n",
    "#         self, current_dev_loss, \n",
    "#         epoch, model, optimizer, criterion\n",
    "#     ):\n",
    "#         if current_dev_loss < self.best_dev_loss:\n",
    "#             self.best_dev_loss = current_dev_loss\n",
    "#             print(f\"\\nBest validation loss: {self.best_dev_loss}\")\n",
    "#             print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch+1,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'loss': criterion,\n",
    "#                 }, 'outputs/best_model.pth')\n",
    "\n",
    "# # initialize SaveBestModel class\n",
    "# save_best_model = SaveBestModel()\n",
    "\n",
    "\n",
    "# save_best_model(\n",
    "#         valid_epoch_loss, epoch, model, optimizer, criterion\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(log['train_loss'])\n",
    "plt.plot(log['dev_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source = [list(\"abcdefghijkl\"), list(\"mnopqrstwxyz\")]\n",
    "#target = [list(\"ABCDEFGHIJKL\"), list(\"MNOPQRSTWXYZ\")] #update to astronomy vocab\n",
    "source = []yy\n",
    "for s in model.source_index:\n",
    "    sentence = \"\"\n",
    "    for c in s:\n",
    "        #nu = source_index[c]\n",
    "        #sentence.append(c)\n",
    "        #sentence.append(num)\n",
    "        sentence += c\n",
    "    source.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from nltk.lm import Vocabulary\n",
    "import pickle\n",
    "import sys\n",
    "import torch\n",
    "import importlib\n",
    "from timeit import default_timer as t\n",
    "sys.path.append(\"../../lib\")\n",
    "#from metrics import levenshtein\n",
    "import post_ocr_correction\n",
    "from pytorch_beam_search import seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = Transformer(char2i, \n",
    "                    i2char, \n",
    "                    max_sequence_length = 110,\n",
    "                    embedding_dimension = 256,\n",
    "                    feedforward_dimension = 1024,\n",
    "                    attention_heads = 8,\n",
    "                    encoder_layers = 2,\n",
    "                    decoder_layers = 2,\n",
    "                   dropout = .5)\n",
    "\n",
    "model.load_state_dict(torch.load(\"./new_torch_file60_7.pt\", map_location=torch.device('cpu')))\n",
    "#model = torch.load(\"../../data/torch_file60.pt\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./source_dataframe.csv')\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "!pip install Levenshtein\n",
    "from Levenshtein import distance\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "       \n",
    "def levenshtein(reference, hypothesis, progress_bar = False):\n",
    "    #assert len(reference) == len(hypothesis)\n",
    "    text = zip(reference, hypothesis)\n",
    "    if progress_bar:\n",
    "        text = tqdm(text, total = len(reference))\n",
    "    d = [distance(r, h) for r, h in text]\n",
    "    output = pd.DataFrame({\"reference\":reference, \"hypothesis\":hypothesis})\\\n",
    "    .assign(distance = lambda df: d)\\\n",
    "    .assign(\n",
    "        cer = lambda df: df.apply(\n",
    "            lambda r: 100 * r[\"distance\"] / max(len(r[\"reference\"]), 1), \n",
    "            axis = 1\n",
    "        )\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arr = test_data[\"ocr_to_input\"].to_numpy()\n",
    "# gs_arr = test_data[\"gs_aligned\"].to_numpy()\n",
    "ocr_list = test_data[\"ocr_aligned\"].tolist()\n",
    "gs_list = test_data[\"gs_aligned\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def uniform(j, window_size):\n",
    "    return 1.0\n",
    "\n",
    "def triangle(j, window_size):\n",
    "    m = window_size//2\n",
    "    return m - 0.5 * abs(m - j)\n",
    "\n",
    "def bell(j, window_size):\n",
    "    m = window_size // 2\n",
    "    s = window_size // 2\n",
    "    return exp(-((m-j)/s)**2)\n",
    "\n",
    "def disjoint(\n",
    "    string,\n",
    "    model, \n",
    "    source_index,\n",
    "    target_index,\n",
    "    window_size = 50,\n",
    "    decoding_method = \"greedy_search\", \n",
    "    document_progress_bar = False, \n",
    "    document_batch_progress_bar = 0, \n",
    "    *arcorrect\n",
    "):\n",
    "    model.eval()\n",
    "    windows = [string[i:i+window_size] \n",
    "        for i in range(0, len(string), window_size)]\n",
    "    windows = [\"\".join([source_index.vocabulary.lookup(c) for c in s])\\\n",
    "        .replace(\"<UNK>\", \" \") for s in windows]\n",
    "    X = source_index.text2tensor(windows, progress_bar = False).cuda()\n",
    "    if decoding_method == \"greedy_search\":\n",
    "        predictions, probs = seq2seq.greedy_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )\n",
    "    elif decoding_method == \"beam_search\":\n",
    "        predictions, probs = seq2seq.beam_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )   \n",
    "        predictions = predictions[:, 0, :]\n",
    "    output = target_index.tensor2text(predictions)\n",
    "    output = [re.sub(r\"<START>|<PAD>|<UNK>|<END>.*\", \"\", s) for s in output]\n",
    "    return \"\".join(output)\n",
    "\n",
    "def n_grams(\n",
    "    string,\n",
    "    model,\n",
    "    source_index,\n",
    "    target_index,\n",
    "    window_size = 50, \n",
    "    decoding_method = \"greedy_search\", \n",
    "    weighting = \"uniform\",\n",
    "    document_progress_bar = False, \n",
    "    document_batch_progress_bar = 0,      \n",
    "    main_batch_size = 1024,\n",
    "    *arcorrect\n",
    "):\n",
    "    model.eval()\n",
    "    if len(string) <= window_size:\n",
    "        windows = [string]\n",
    "    else:\n",
    "        windows = [string[i:i + window_size] \n",
    "            for i in range(len(string) - window_size + 1)]\n",
    "    windows = [\"\".join([source_index.vocabulary.lookup(c) for c in s])\\\n",
    "    .replace(\"<UNK>\", \" \") for s in windows]\n",
    "    X = source_index.text2tensor(windows, progress_bar = False).cuda()\n",
    "    if decoding_method == \"greedy_search\":\n",
    "        predictions, probs = seq2seq.greedy_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = False, \n",
    "            *arcorrect\n",
    "        )\n",
    "    if decoding_method == \"beam_search\":\n",
    "        predictions, probs = seq2seq.beam_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )   \n",
    "        predictions = predictions[:, 0, :]   \n",
    "    output = target_index.tensor2text(predictions)\n",
    "    output = [re.sub(r\"<START>|<PAD>|<UNK>|<END>.*\", \"\", s) for s in output]\n",
    "    if weighting == \"uniform\":\n",
    "        weighting = uniform\n",
    "    elif weighting == \"triangle\":\n",
    "        weighting = triangle\n",
    "    elif weighting == \"bell\":\n",
    "        weighting = bell\n",
    "    votes = [\n",
    "        {k:0.0 for k in target_index.vocabulary} \n",
    "        for c in string\n",
    "    ]\n",
    "    for i, s in enumerate(output):\n",
    "        for j, (counter, char)\\\n",
    "        in enumerate(zip(votes[i:i + window_size], s)):\n",
    "            counter[char] += weighting(j, window_size)\n",
    "    output = [max(c.keys(), key = lambda x: c[x]) for c in votes]\n",
    "    output = \"\".join(output)\n",
    "    return votes, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_source = []\n",
    "n = 20 # chunk length  \n",
    "#for s in arr:\n",
    "for x in range(48):#only process n paragraphs?\n",
    "    s = ocr_list[x]\n",
    "    #sentence_array = list(s) #convert string to an array of characters\n",
    "    chunks = [s[i:i+n] for i in range(0, len(s), n)]\n",
    "    for sentence_chunk in chunks:\n",
    "        new_source.append(list(sentence_chunk))#append array to new_source array\n",
    "\n",
    "    #new_source.append(list(\"stars cataclysmic  \"))#mark end of sentences with stars cataclysmic\n",
    "    new_source.append( list(\"advice charm touch  \"))#new end of sentence with 20 chars\n",
    "    #because the model only allows 20 characters at a time, each row is 20 chars chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_beam_search import seq2seq\n",
    "source_index = seq2seq.Index(source)\n",
    "target_index =  source_index\n",
    "X_new = source_index.text2tensor(new_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, log_probabilities = beam_search(\n",
    "    model,\n",
    "    X_new.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [target_index.tensor2text(p) for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output\n",
    "#output2 = [source_index.tensor2text(p) for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "predictions.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "predictions.to(device)\n",
    "device\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def uniform(j, window_size):\n",
    "    return 1.0\n",
    "\n",
    "def triangle(j, window_size):\n",
    "    m = window_size//2\n",
    "    return m - 0.5 * abs(m - j)\n",
    "\n",
    "def bell(j, window_size):\n",
    "    m = window_size // 2\n",
    "    s = window_size // 2\n",
    "    return exp(-((m-j)/s)**2)\n",
    "\n",
    "def disjoint(\n",
    "    string,\n",
    "    model, \n",
    "    source_index,\n",
    "    target_index,\n",
    "    window_size = 50,\n",
    "    decoding_method = \"greedy_search\", \n",
    "    document_progress_bar = False, \n",
    "    document_batch_progress_bar = 0, \n",
    "    *arcorrect\n",
    "):\n",
    "    model.eval()\n",
    "    windows = [string[i:i+window_size] \n",
    "        for i in range(0, len(string), window_size)]\n",
    "    windows = [\"\".join([source_index.vocabulary.lookup(c) for c in s])\\\n",
    "        .replace(\"<UNK>\", \" \") for s in windows]\n",
    "    X = source_index.text2tensor(windows, progress_bar = False).cuda()\n",
    "    if decoding_method == \"greedy_search\":\n",
    "        predictions, probs = seq2seq.greedy_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )\n",
    "    elif decoding_method == \"beam_search\":\n",
    "        predictions, probs = seq2seq.beam_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )   \n",
    "        predictions = predictions[:, 0, :]\n",
    "    output = target_index.tensor2text(predictions)\n",
    "    output = [re.sub(r\"<START>|<PAD>|<UNK>|<END>.*\", \"\", s) for s in output]\n",
    "    return \"\".join(output)\n",
    "\n",
    "def n_grams(\n",
    "    string,\n",
    "    model,\n",
    "    source_index,\n",
    "    target_index,\n",
    "    window_size = 50, \n",
    "    decoding_method = \"greedy_search\", \n",
    "    weighting = \"uniform\",\n",
    "    document_progress_bar = False, \n",
    "    document_batch_progress_bar = 0,      \n",
    "    main_batch_size = 1024,\n",
    "    *arcorrect\n",
    "):\n",
    "    model.eval()\n",
    "    if len(string) <= window_size:\n",
    "        windows = [string]\n",
    "    else:\n",
    "        windows = [string[i:i + window_size] \n",
    "            for i in range(len(string) - window_size + 1)]\n",
    "    windows = [\"\".join([source_index.vocabulary.lookup(c) for c in s])\\\n",
    "    .replace(\"<UNK>\", \" \") for s in windows]\n",
    "    X = source_index.text2tensor(windows, progress_bar = False).cuda()\n",
    "    if decoding_method == \"greedy_search\":\n",
    "        predictions, probs = seq2seq.greedy_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = False, \n",
    "            *arcorrect\n",
    "        )\n",
    "    if decoding_method == \"beam_search\":\n",
    "        predictions, probs = seq2seq.beam_search(\n",
    "            model,\n",
    "            X, \n",
    "            predictions = window_size, \n",
    "            progress_bar = document_batch_progress_bar, \n",
    "            *arcorrect\n",
    "        )   \n",
    "        predictions = predictions[:, 0, :]   \n",
    "    output = target_index.tensor2text(predictions)\n",
    "    output = [re.sub(r\"<START>|<PAD>|<UNK>|<END>.*\", \"\", s) for s in output]\n",
    "    if weighting == \"uniform\":\n",
    "        weighting = uniform\n",
    "    elif weighting == \"triangle\":\n",
    "        weighting = triangle\n",
    "    elif weighting == \"bell\":\n",
    "        weighting = bell\n",
    "    votes = [\n",
    "        {k:0.0 for k in target_index.vocabulary} \n",
    "        for c in string\n",
    "    ]\n",
    "    for i, s in enumerate(output):\n",
    "        for j, (counter, char)\\\n",
    "        in enumerate(zip(votes[i:i + window_size], s)):\n",
    "            counter[char] += weighting(j, window_size)\n",
    "    output = [max(c.keys(), key = lambda x: c[x]) for c in votes]\n",
    "    output = \"\".join(output)\n",
    "    return votes, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_evaluation(\n",
    "    raw, \n",
    "    gs, \n",
    "    model, \n",
    "    source_index,\n",
    "    target_index,\n",
    "    save_path = None, \n",
    "    window_size = 40, \n",
    "    document_progress_bar = False\n",
    "):\n",
    "    print(\"evaluating all methods...\")\n",
    "    metrics = []\n",
    "    old = levenshtein(reference = gs, hypothesis = raw).cer.mean()\n",
    "    # disjoint\n",
    "    print(\"  disjoint window...\")\n",
    "    print(\"    greedy_search...\")\n",
    "    start = t()\n",
    "    corrections = [\n",
    "        disjoint(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            document_progress_bar = document_progress_bar, \n",
    "            window_size = window_size\n",
    "        ) for s in raw]\n",
    "    print(\"hello\")\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"disjoint\", \n",
    "            \"decoding\":\"greedy\",\n",
    "            \"window_size\":window_size * 2,\n",
    "            \"weighting\":pd.NA,\n",
    "            \"inference_seconds\":t() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "    start = t()\n",
    "    corrections = [\n",
    "        disjoint(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            document_progress_bar = document_progress_bar, \n",
    "            window_size = window_size * 2\n",
    "        ) \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"disjoint\", \n",
    "            \"decoding\":\"greedy\",\n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":pd.NA,\n",
    "            \"inference_seconds\":t() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "    print(\"    beam_search...\")\n",
    "    start = t()\n",
    "    corrections = [\n",
    "        disjoint(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            decoding_method = \"beam_search\", \n",
    "            document_progress_bar = document_progress_bar, \n",
    "            window_size = window_size\n",
    "        ) \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"disjoint\", \n",
    "            \"decoding\":\"beam\", \n",
    "            \"window_size\":window_size * 2,\n",
    "            \"weighting\":pd.NA,\n",
    "            \"inference_seconds\":t() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "    start = t()\n",
    "    corrections = [\n",
    "        disjoint(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            decoding_method = \"beam_search\", \n",
    "            document_progress_bar = document_progress_bar, \n",
    "            window_size = window_size * 2\n",
    "        ) \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"disjoint\", \n",
    "            \"decoding\":\"beam\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":pd.NA,\n",
    "            \"inference_seconds\":t() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "    # sliding\n",
    "    print(\"  sliding\")\n",
    "    print(\"    greedy...\")\n",
    "    ## greedy search\n",
    "    print(\"      uniform...\")\n",
    "    start = t()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            weighting = uniform,\n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"greedy\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"uniform\",\n",
    "            \"inference_seconds\":t() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "\n",
    "    print(\"      triangle...\")\n",
    "    start = t()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            weighting = triangle, \n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"greedy\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"triangle\",\n",
    "            \"inference_seconds\":t() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "\n",
    "    print(\"      bell...\")\n",
    "    start = t()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            weighting = bell, \n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"greedy\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"bell\",\n",
    "            \"inference_seconds\":t() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "\n",
    "    ## beam search\n",
    "    print(\"    beam...\")\n",
    "    print(\"      uniform...\")\n",
    "    start = t()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            decoding_method = \"beam_search\", \n",
    "            weighting = uniform, \n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"beam\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"uniform\",\n",
    "            \"inference_seconds\":t() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "\n",
    "    print(\"      triangle...\")\n",
    "    start = t()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model, \n",
    "            source_index,\n",
    "            target_index,\n",
    "            decoding_method = \"beam_search\", \n",
    "            weighting = triangle, \n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"beam\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"triangle\",\n",
    "            \"inference_seconds\":t() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "\n",
    "    print(\"      bell...\")\n",
    "    start = t()\n",
    "    corrections = [\n",
    "        n_grams(\n",
    "            s, \n",
    "            model.cuda(), \n",
    "            source_index,\n",
    "            target_index,\n",
    "            decoding_method = \"beam_search\", \n",
    "            weighting = bell, \n",
    "            document_progress_bar = document_progress_bar,\n",
    "            window_size = window_size\n",
    "        )[1] \n",
    "        for s in raw\n",
    "    ]\n",
    "    metrics.append(\n",
    "        {\n",
    "            \"window\":\"sliding\", \n",
    "            \"decoding\":\"beam\", \n",
    "            \"window_size\":window_size,\n",
    "            \"weighting\":\"bell\",\n",
    "            \"inference_seconds\":t() - start,\n",
    "            \"cer_before\":old,\n",
    "            \"cer_after\":levenshtein(gs, corrections).cer.mean()\n",
    "        }\n",
    "    )\n",
    "    if save_path:\n",
    "        pd.DataFrame(metrics).assign(\n",
    "            improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "        ).to_csv(save_path, index = False)\n",
    "    print()\n",
    "    return pd.DataFrame(metrics).assign(\n",
    "        improvement = lambda df: 100 * (1 - df.cer_after / df.cer_before)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nevaluating all correction methods...\")\n",
    "evaluation = full_evaluation(\n",
    "    ocr_list,\n",
    "    gs_list,\n",
    "    model,\n",
    "    source_index,\n",
    "    target_index,\n",
    ")\n",
    "\n",
    "print(\"\\n--results--\")\n",
    "print(\"test data:\", test)\n",
    "print(\"plain beam search:\", just_beam)\n",
    "print(\"disjoint\")\n",
    "print(\"  greedy search:\", disjoint_greedy)\n",
    "print(\"  beam search:\", disjoint_beam)\n",
    "print(\"n_grams\")\n",
    "print(\"  greedy search:\", n_grams_greedy)\n",
    "print(\"  beam search:\", n_grams_beam)\n",
    "\n",
    "print(\"\\n\", evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_list #take out @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.to_csv(\"evaluation60new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_output = []\n",
    "sentence = \"\"\n",
    "for chunks in output:\n",
    "    #if chunks[0] == 'stars cataclysmic  ':#everytime it finds stars cataclysmic it outputs a new sentence #theres only 19?\n",
    "    if chunks[0] == '<START>advice charm touch  ':#everytime it finds advice charm touch   it outputs a new sentence\n",
    "        new_output.append(sentence)\n",
    "        sentence = \"\"# this empties the sentence variable whenever a sentence is added to the array\n",
    "    else:\n",
    "       new_sentence = chunks[0].replace(\"<START>\", \"\")\n",
    "       new_sentence = new_sentence.replace(\"<END>\", \"\")\n",
    "       sentence = sentence + new_sentence #combining the chunks of 20 characters together without the start and end\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "unnamed = list(range(1,49))\n",
    "#print(unnamed)\n",
    "test_data['new_output'] = new_output\n",
    "test_data['unnamed'] = unnamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data\n",
    "#test_data.to_csv('./github_output_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pybind11\n",
    "!pip install fastwer\n",
    "#!pip install pytesseract\n",
    "#!sudo apt install tesseract-ocr\n",
    "\n",
    "import cv2\n",
    "#import pytesseract\n",
    "import fastwer\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test_data.iterrows():\n",
    "  filename = row['unnamed']\n",
    "  ref = row['gs_aligned']\n",
    "  ocr = row['ocr_aligned']\n",
    "  output = row['new_output']\n",
    "  cer = fastwer.score_sent(ocr, ref, char_level=True)\n",
    "  wer = fastwer.score_sent(ocr, ref, char_level=False)\n",
    "  test_data.loc[test_data['unnamed'] == filename, 'before_cer'] = round(cer,2) # Round value to 2 decimal places\n",
    "  test_data.loc[test_data['unnamed'] == filename, 'before_wer'] = round(wer,2)\n",
    "\n",
    "#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('./cer_wer_new50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test_data.iterrows():\n",
    "  filename = row['unnamed']\n",
    "  ref = row['gs_aligned']\n",
    "  ocr = row['ocr_aligned']\n",
    "  output = row['new_output']\n",
    "  cer = fastwer.score_sent(output, ref, char_level=True)\n",
    "  wer = fastwer.score_sent(output, ref, char_level=False)\n",
    "  test_data.loc[test_data['unnamed'] == filename, 'after_cer'] = round(cer,2) # Round value to 2 decimal places\n",
    "  test_data.loc[test_data['unnamed'] == filename, 'after_wer'] = round(wer,2)\n",
    "\n",
    "#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('./cer_wer_new50epochs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "new_output = tokenize.sent_tokenize(sentence)\n",
    "#new_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs =test_data['gs_aligned']\n",
    "gs_list = ' '.join(test_data['gs_aligned'].tolist())\n",
    "#gs_array = gs.to_numpy()\n",
    "\n",
    "#print(gs_array)\n",
    "new_gs = tokenize.sent_tokenize(gs_list) # trying to take gs column to match the length of the new_output.\n",
    "len(new_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTest():   \n",
    "    input_file = './post_ocr_correction/data/en/data/test'\n",
    "    \n",
    "    # reverse is true, therefore pair[0] is ocr and pair[1] is gs\n",
    "    input_lang, output_lang, pairs = prepareData(input_file,'gs', 'ocr', True)\n",
    "    \n",
    "    output_sentences = []\n",
    "    input_sentences = []\n",
    "    corrected_sentences = []\n",
    "    for pair in pairs:\n",
    "        sentence = pair[0]  #ocr sentence\n",
    "        try:\n",
    "            output_words, attentions = evaluate(encoder1, attn_decoder1, sentence)\n",
    "            input_sentences.append(pair[1])  #gs sentence\n",
    "            output_sentences.append(pair[0]) #ocr sentence\n",
    "            corrected_sentences.append(' '.join(output_words))\n",
    " \n",
    "            #testing_df.append({'output':''.join(output_words),'input':x},ignore_index=True)\n",
    "            #showAttention(x, output_words, attentions)\n",
    "        except KeyError:\n",
    "            print('KeyError =', sentence)\n",
    "    testing_df = pd.DataFrame({\"gs\":input_sentences, \"ocr\":output_sentences, \"output\":corrected_sentences})\n",
    "    return testing_df\n",
    "testing_dataframe1 = readTest()\n",
    "testing_dataframe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_df1 = pd.DataFrame({\"output\":new_output})\n",
    "output_df1.to_csv('./output_dataframe_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTest():   \n",
    "    input_file = './post_ocr_correction/data/en/data/test'\n",
    "    \n",
    "    # reverse is true, therefore pair[0] is ocr and pair[1] is gs\n",
    "    input_lang, output_lang, pairs = prepareData(input_file,'gs', 'ocr', True)\n",
    "    \n",
    "    output_sentences = []\n",
    "    input_sentences = []\n",
    "    corrected_sentences = []\n",
    "    for pair in pairs:\n",
    "        sentence = pair[0]  #ocr sentence\n",
    "        try:\n",
    "            output_words, attentions = evaluate(encoder1, attn_decoder1, sentence)\n",
    "            input_sentences.append(pair[1])  #gs sentence\n",
    "            output_sentences.append(pair[0]) #ocr sentence\n",
    "            corrected_sentences.append(' '.join(output_words))\n",
    " \n",
    "            #testing_df.append({'output':''.join(output_words),'input':x},ignore_index=True)\n",
    "            #showAttention(x, output_words, attentions)\n",
    "        except KeyError:\n",
    "            print('KeyError =', sentence)\n",
    "    testing_df = pd.DataFrame({\"gs\":input_sentences, \"ocr\":output_sentences, \"output\":corrected_sentences})\n",
    "    return testing_df\n",
    "testing_dataframe1 = readTest()\n",
    "testing_dataframe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# #dictionary = {'GS': \"\", 'OCR': \"\", 'index'=[0]} \n",
    "# #training_df = pd.DataFrame(dictionary)\n",
    "\n",
    "# ocr =[]\n",
    "# ocr_aligned=[]\n",
    "# gs_aligned=[]\n",
    "\n",
    "# for x in range(1008,1120):\n",
    "#     for y in range(len(PDF_OUT_SENT[x])):\n",
    "#         for z in range(len(PDF_OUT_SENT[x][y])):\n",
    "#             OCR = OCR_OUT_SENT[x][y][z]\n",
    "#             OCR_aligned_train = OCR_aligned_SENT[x][y][z]\n",
    "#             GS_aligned_train = PDF_OUT_SENT[x][y][z]\n",
    "#             #dictionary = {'GS': GS_train, 'OCR': OCR_train} \n",
    "#             #training_df = pd.DataFrame(dictionary)\n",
    "#             #gs_train = training_df.append(OCR_OUT_SENT)\n",
    "#             ocr.append(OCR)\n",
    "#             ocr_aligned.append(OCR_aligned_train)\n",
    "#             gs_aligned.append(GS_aligned_train)\n",
    "#             #training_df = pd.concat([training_df, pd.DataFrame(dictionary)], ignore_index = True)\n",
    "# data_ocr = pd.DataFrame({\"ocr_to_input\":ocr}) #\"ocr_aligned\":ocr_aligned, \"gs_aligned\":gs_aligned})\n",
    "# print(data_ocr.shape)\n",
    "# data_ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, probs = model.predict(dev_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tensor2text(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tensor2text(train_target[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tensor2text(train_source[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opence-v1.5.1]",
   "language": "python",
   "name": "conda-env-opence-v1.5.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
